<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content>
  <meta name="author" content="Rock">
  <!-- Open Graph Data -->
  <meta property="og:title" content="达观杯比赛记录">
  <meta property="og:description" content>
  <meta property="og:site_name" content="Rock-Blog">
  <meta property="og:type" content="article">
  <meta property="og:image" content="http://rock-blog.top">
  
    <link rel="alternate" href="/atom.xml" title="Rock-Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>Rock-Blog</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/龙珠3.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">达观杯比赛记录</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/guobaoyo">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:shi_chenggong@163.com">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Rock</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2019-07-29</span>
            <span class="time">11:34:21</span>
          </span>
          
          <!--  Categories  -->
            <span class="categories info">Under 

<a href="/categories/NLP/">NLP</a>
</span>
          
        </div>
        <!-- Tags -->
        
          <div class="post-tags text-muted">
            Tags: 

<a class="tag" href="/tags/深度学习/">#深度学习</a> <a class="tag" href="/tags/人工智能/">#人工智能</a> <a class="tag" href="/tags/NLP/">#NLP</a>


          </div>
        
        <!-- Post Main Content -->
        <div class="post-content">
          <p>达观杯备赛记录</p>
<a id="more"></a>
<p>1.对比赛有个基本了解—是个NER的任务：</p>
<p><a href="https://biendata.com/competition/datagrand/?source=guanwang" target="_blank" rel="noopener">比赛官网</a></p>
<p>重要知识点记录：</p>
<p>​    1.不允许使用主办方提供的数据集之外的任何外部标注数据，也不允许使用预训练词向量，<strong>参赛选手不得在未经主办方授权情况下将比赛数据作为其他用途使用</strong>；</p>
<p>​    2.参赛队伍可在参赛期间随时上传验证集的预测结果，一天不能超过<strong>2次</strong>；</p>
<p>​    3.训练集有17000条，我们在17000条数据上标注了3个字段，共有字段a 9281处，字段b 14704处，字段c 9097处。预测集有3000条。</p>
<p>​    4.比赛方提供一个大规模的未标注语料供参赛选手预训练语言模型</p>
<p>​    5.train_set.txt </p>
<p>此数据集用于训练模型，每一行对应一条文本数据。每一个数字对应一个“字”或“标点符号”。字和字之间用‘_’连接，在对应字段后面标注/a、/b、/c，非字段文本标注/o。比如：“欢迎来到达观数据。”是形如“1_2_3_4_5_6_7_8_9”的字符串，如果“达观数据”是字段c，就会被标成“1_2_3_4/o  5_6_7_8/c 9/o”的形式。</p>
<p>​    6.评分标准：</p>
<p>信息抽取的评估指标是F1值，是正确率和召回率的调和平均值。</p>
<p> 正确率 = 抽取出的正确字段数 / 抽取出的字段数<br> 召回率 = 抽取出的正确字段数 / 样本的字段数<br> F1值 = （2 <em> 正确率 </em> 召回率）/（正确率 + 召回率）</p>
<p>2.学习王老板给的三篇网上大佬学习资料，先对NER有个基本了解：</p>
<p><a href="https://www.lookfor404.com/%e7%94%a8%e8%a7%84%e5%88%99%e5%81%9a%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab-ner%e7%b3%bb%e5%88%97%ef%bc%88%e4%b8%80%ef%bc%89/" target="_blank" rel="noopener">NER-1</a></p>
<p><a href="https://www.lookfor404.com/%e7%94%a8%e9%9a%90%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e6%a8%a1%e5%9e%8bhmm%e5%81%9a%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab-ner%e7%b3%bb%e5%88%97%e4%ba%8c/" target="_blank" rel="noopener">NER-2</a></p>
<p><a href="https://www.lookfor404.com/%e7%94%a8%e8%a7%84%e5%88%99%e5%81%9a%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab-ner%e7%b3%bb%e5%88%97%ef%bc%88%e4%b8%80%ef%bc%89/" target="_blank" rel="noopener">NER-3</a></p>
<p>3.再看的<a href="https://segmentfault.com/a/1190000016778761#articleHeader6" target="_blank" rel="noopener">官方给的解答</a>和<a href="https://www.kaggle.com/huskylovers/fork-of-starter-datagrand" target="_blank" rel="noopener">王老板分享的代码</a></p>
<p>条件随机场（ConditionalRandom<br>Field，CRF）是NER目前的主流模型。它的目标函数不仅考虑输入的状态特征函数，而且还包含了标签转移特征函数。在训练时可以使用SGD学习模型参数。在已知模型时，给输入序列求预测输出序列即求使目标函数最大化的最优序列，是一个动态规划问题，可以使用Viterbi算法解码来得到最优标签序列。<strong>CRF的优点在于其为一个位置进行标注的过程中可以利用丰富的内部及上下文特征信息。</strong>—出自官方给的资料</p>
<p><strong>看完官方给的视频后总结如下</strong><br>需要学的东西：<br>1.基础知识：专业术语听不太懂，看看embedding是啥？</p>
<p>（答：embedding的意思应该是将一个词转换为向量，但是之前听说过one hot encoding,只不过one hot encoding会产生大量的稀疏矩阵，且维度之间的关系也没有很好的体现，所以在比赛中能用别的就尽量别用one hot encoding，而在<a href="https://blog.csdn.net/k284213498/article/details/83474972" target="_blank" rel="noopener">YJango老师的解释中</a>Word embedding就是要从数据中自动学习到输入空间到Distributed representation空间的 映射f ，且训练词向量的过程中是无监督的。）</p>
<p>NLP里面的数据增强，最好是在预训练模型加入一些同数据源的特征。</p>
<p>（通过数据增强可以明显提升模型性能。具体地，我们对原语料进行分句，然后随机地对各个句子进行bigram、trigram拼接，最后与原始句子一起作为训练语料。）<br>2.维特比算法比较重要，对后面一系列算法的学习都有帮助。<br>3.得理解算法本身才能调参，效果才好，别瞎调。<br>4.学一下LSTM的原版论文，可以试一下这个模型。<br>5.看看能不能利用attention模型。<br>6.深度学习的调参尽量用经典的套路，例如优化函数：adam可以试试的<br>7.过拟合怎么处理：dropout可以设置大一点，再一个就是可以尽量的使用corpus这个东西，从而让它的泛化能力更强。</p>
<p>注意点：<br>1.一开始先看数据，做清洗，可以一开始用机器学习的模型去做知道机器学习模型的底线之后，再搞深度学习。<br>2.做文本处理神经网络的层数不要太多：否则第一容易过拟合，第二训练的慢。<br>3.一个实体有多个关系，联合标注会出问题，一个模型出一个关系就可以一定程度上解决。<br>4.CRF++有一定的局限性，如果想要扩展需要修改源码，但是改完之后不一定会有明显的效果。<br>5.只用train data的话肯定过拟合，所以数据很重要，官方给的数据尽量全用吧，corpus的利用有待提高！<br>6.观察算法本身，所以别想着再添加训练集。<br>7.在做词向量的时候，ELMO会比word2vec效果好很多。</p>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        </p><p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
//<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

