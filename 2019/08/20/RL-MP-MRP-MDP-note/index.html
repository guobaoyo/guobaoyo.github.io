<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <title>RL_MDP | Rock-Blog</title>
  <meta name="keywords" content=" AI , 深度学习 , 强化学习 ">
  <meta name="description" content="RL_MDP | Rock-Blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="主要为TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game学习笔记 论文链接：https://arxiv.org/pdf/1809.07193.pdf 代码连接：https://github.com/Tencent/TStarBot1">
<meta name="keywords" content="AI,深度学习,强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Tstarbots_note">
<meta property="og:url" content="http://rock-blog.top/2020/10/23/tstar/index.html">
<meta property="og:site_name" content="Rock-Blog">
<meta property="og:description" content="主要为TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game学习笔记 论文链接：https://arxiv.org/pdf/1809.07193.pdf 代码连接：https://github.com/Tencent/TStarBot1">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://rock-blog.top/2020/10/23/tstar/D:/rockblog/source/rockblog/source/_posts/tstar/image-20201023141451747.png">
<meta property="og:image" content="http://rock-blog.top/2020/10/23/tstar/D:/rockblog/source/rockblog/source/_posts/tstar/image-20201023143727477.png">
<meta property="og:image" content="http://rock-blog.top/2020/10/23/tstar/D:/rockblog/source/rockblog/source/_posts/tstar/image-20201023150239912.png">
<meta property="og:updated_time" content="2020-11-03T13:47:53.508Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tstarbots_note">
<meta name="twitter:description" content="主要为TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game学习笔记 论文链接：https://arxiv.org/pdf/1809.07193.pdf 代码连接：https://github.com/Tencent/TStarBot1">
<meta name="twitter:image" content="http://rock-blog.top/2020/10/23/tstar/D:/rockblog/source/rockblog/source/_posts/tstar/image-20201023141451747.png">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>

<script src="/js/iconfont.js?v=1.0.1"></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="true">
  <input class="theme_blog_path" value>
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg">
</a>
<div class="author">
    <span>Rock</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/guobaoyo" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"/>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:1415500736@qq.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"/>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=1415500736&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"/>
                </svg>
            
        </a>
        
    
        
        <a title="neteasemusic" href="https://music.163.com/#/user/home?id=280020740" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-neteasemusic"/>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(49)</small></div></li>
    
        
            
            <li><div data-rel="三省吾身">三省吾身<small>(5)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="强化学习">强化学习<small>(16)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="AI">AI<small>(7)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="编程">编程<small>(17)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数学">数学<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="组会报告">组会报告<small>(1)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    
    
    </div>
    <div><a class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="49">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
            <li><a target="_blank" href="http://zivblog.top">王金锋</a></li>
            
            <li><a target="_blank" href="http://yearing1017.cn/">进哥</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off" id="local-search-input">
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none">
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">三省吾身</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">AI</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">数学</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">深度学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">编程</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">CV</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">python</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">强化学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">技术小节</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">go</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">leetcode</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">技术小结</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">组会报告</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">考研</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">NLP</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a class="三省吾身 " href="/2019/07/08/20190708日记/" data-tag="三省吾身,AI,数学" data-author>
            <span class="post-title" title="20190708日记">20190708日记</span>
            <span class="post-date" title="2019-07-08 19:43:26">2019/07/08</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/29/DLwords/" data-tag="AI,深度学习" data-author>
            <span class="post-title" title="DLwords">DLwords</span>
            <span class="post-date" title="2019-04-29 19:17:39">2019/04/29</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/08/A-algorithm/" data-tag="AI,数学,编程" data-author>
            <span class="post-title" title="A*_algorithm">A*_algorithm</span>
            <span class="post-date" title="2020-06-08 21:25:01">2020/06/08</span>
        </a>
        
        <a class="AI " href="/2019/05/08/DLwithPython/" data-tag="深度学习,CV,python" data-author>
            <span class="post-title" title="DeepLearning with Python">DeepLearning with Python</span>
            <span class="post-date" title="2019-05-08 17:54:01">2019/05/08</span>
        </a>
        
        <a class="强化学习 " href="/2019/08/05/RL-DDPG-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A2c&amp;ppo">Aatri_A2c&amp;ppo</span>
            <span class="post-date" title="2019-08-05 21:25:01">2019/08/05</span>
        </a>
        
        <a class="AI " href="/20120/09/25/DRL-report/" data-tag="数学,深度学习,强化学习" data-author>
            <span class="post-title" title="DRL_report">DRL_report</span>
            <span class="post-date" title="20120-09-25 19:17:39">20120/09/25</span>
        </a>
        
        <a class="强化学习 " href="/2019/08/20/RL-MP-MRP-MDP-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_MDP">RL_MDP</span>
            <span class="post-date" title="2019-08-20 21:25:01">2019/08/20</span>
        </a>
        
        <a class="强化学习 " href="/2019/09/10/RL-DP-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_DP">RL_DP</span>
            <span class="post-date" title="2019-09-10 21:25:01">2019/09/10</span>
        </a>
        
        <a class="强化学习 " href="/2020/08/02/RL-PPO-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="pg_note">pg_note</span>
            <span class="post-date" title="2020-08-02 21:25:01">2020/08/02</span>
        </a>
        
        <a class="强化学习 " href="/2020/01/26/RLTF/" data-tag="AI,数学,强化学习" data-author>
            <span class="post-title" title="RLTF">RLTF</span>
            <span class="post-date" title="2020-01-26 19:21:06">2020/01/26</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/31/RL_A3C_A2C_note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="A3C_note">A3C_note</span>
            <span class="post-date" title="2020-07-31 21:25:01">2020/07/31</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/26/RL_AC-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="ac_note">ac_note</span>
            <span class="post-date" title="2019-07-26 21:25:01">2019/07/26</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/12/atari-env-note/" data-tag="深度学习,编程,强化学习" data-author>
            <span class="post-title" title="Atari游戏环境笔记">Atari游戏环境笔记</span>
            <span class="post-date" title="2020-07-12 10:00:12">2020/07/12</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/21/RL-basic-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="rl_basic_note">rl_basic_note</span>
            <span class="post-date" title="2019-07-21 21:25:01">2019/07/21</span>
        </a>
        
        <a class="编程 " href="/2020/10/08/docker-base/" data-tag="编程,python,技术小节" data-author>
            <span class="post-title" title="docker-base">docker-base</span>
            <span class="post-date" title="2020-10-08 17:54:01">2020/10/08</span>
        </a>
        
        <a class="编程 " href="/2020/09/25/effective-python/" data-tag="python,技术小节" data-author>
            <span class="post-title" title="effective_python_note">effective_python_note</span>
            <span class="post-date" title="2020-09-25 19:17:39">2020/09/25</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/21/RL_pg-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="pg_note">pg_note</span>
            <span class="post-date" title="2019-07-21 21:25:01">2019/07/21</span>
        </a>
        
        <a class="AI " href="/2019/09/14/deep-reinforcement-learning/" data-tag="数学,深度学习,强化学习" data-author>
            <span class="post-title" title="deep-reinforcement-learning">deep-reinforcement-learning</span>
            <span class="post-date" title="2019-09-14 10:00:12">2019/09/14</span>
        </a>
        
        <a class="编程 " href="/2020/01/31/leetcode/" data-tag="数学,编程,python" data-author>
            <span class="post-title" title="leetcode">leetcode</span>
            <span class="post-date" title="2020-01-31 10:14:45">2020/01/31</span>
        </a>
        
        <a class="编程 " href="/2019/07/23/go-http搭建服务器知识点儿/" data-tag="编程,go" data-author>
            <span class="post-title" title="go-http搭建服务器知识点儿">go-http搭建服务器知识点儿</span>
            <span class="post-date" title="2019-07-23 11:29:08">2019/07/23</span>
        </a>
        
        <a class="数学 " href="/2019/04/25/math/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="线性代数补充笔记">线性代数补充笔记</span>
            <span class="post-date" title="2019-04-25 21:25:01">2019/04/25</span>
        </a>
        
        <a class href="/2020/06/05/lgb-note/" data-tag="AI,编程,python" data-author>
            <span class="post-title" title="lgb_note">lgb_note</span>
            <span class="post-date" title="2020-06-05 21:25:01">2020/06/05</span>
        </a>
        
        <a class="编程 " href="/2020/07/06/sword-offer05/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer05">剑指offer05</span>
            <span class="post-date" title="2020-07-06 20:19:39">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/04/sword-offer03/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer03">剑指offer03</span>
            <span class="post-date" title="2020-07-04 20:19:39">2020/07/04</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/06/rl-questions/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_questions">RL_questions</span>
            <span class="post-date" title="2020-07-06 21:25:01">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/06/sword-offer04/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer04">剑指offer04</span>
            <span class="post-date" title="2020-07-06 20:19:39">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/07/sword-offer06/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer06">剑指offer06</span>
            <span class="post-date" title="2020-07-07 20:19:39">2020/07/07</span>
        </a>
        
        <a class="AI " href="/2020/06/02/pytorch-note/" data-tag="AI,python,技术小结" data-author>
            <span class="post-title" title="pytorch_note">pytorch_note</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="编程 " href="/2020/07/10/sword-offer10/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-10 20:19:39">2020/07/10</span>
        </a>
        
        <a class="编程 " href="/2020/07/08/sword-offer07/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer07">剑指offer07</span>
            <span class="post-date" title="2020-07-08 20:19:39">2020/07/08</span>
        </a>
        
        <a class="编程 " href="/2020/07/09/sword-offer09/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer09">剑指offer09</span>
            <span class="post-date" title="2020-07-09 20:19:39">2020/07/09</span>
        </a>
        
        <a class="编程 " href="/2020/07/10/sword-offer11/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-10 20:19:39">2020/07/10</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/11/tianshou-a2c-note/" data-tag="AI,编程,强化学习" data-author>
            <span class="post-title" title="tianshou平台源码阅读笔记">tianshou平台源码阅读笔记</span>
            <span class="post-date" title="2020-06-11 21:25:01">2020/06/11</span>
        </a>
        
        <a class="编程 " href="/2019/04/29/使用Python实现excel中固定数据排序且改名至另一个文件夹/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="基于python实现excel中读取文件目录+相应数据排序+将文件重命名">基于python实现excel中读取文件目录+相应数据排序+将文件重命名</span>
            <span class="post-date" title="2019-04-29 19:30:06">2019/04/29</span>
        </a>
        
        <a class="三省吾身 " href="/2019/11/03/智源大会听报告笔记/" data-tag="三省吾身,AI" data-author>
            <span class="post-title" title="智源大会听学术报告笔记">智源大会听学术报告笔记</span>
            <span class="post-date" title="2019-11-03 13:24:12">2019/11/03</span>
        </a>
        
        <a class="数学 " href="/2019/10/24/数值计算与凸优化补充笔记/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="数值计算与凸优化补充笔记">数值计算与凸优化补充笔记</span>
            <span class="post-date" title="2019-10-24 16:14:54">2019/10/24</span>
        </a>
        
        <a class="强化学习 " href="/2020/10/23/tstar/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Tstarbots_note">Tstarbots_note</span>
            <span class="post-date" title="2020-10-23 21:25:01">2020/10/23</span>
        </a>
        
        <a class="编程 " href="/2020/07/19/动态规划/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-19 20:19:39">2020/07/19</span>
        </a>
        
        <a class="三省吾身 " href="/2021/02/05/不正经/" data-tag="三省吾身" data-author>
            <span class="post-title" title="老不正经">老不正经</span>
            <span class="post-date" title="2021-02-05 21:12:21">2021/02/05</span>
        </a>
        
        <a class="组会报告 " href="/2019/09/26/组会报告-0930-QLearning/" data-tag="深度学习,强化学习,组会报告" data-author>
            <span class="post-title" title="组会报告-0930-QLearning">组会报告-0930-QLearning</span>
            <span class="post-date" title="2019-09-26 15:15:04">2019/09/26</span>
        </a>
        
        <a class="编程 " href="/2019/09/05/物体检测打标小脚本-获取当前图片中鼠标位置/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="物体检测打标小脚本-获取当前图片中鼠标位置">物体检测打标小脚本-获取当前图片中鼠标位置</span>
            <span class="post-date" title="2019-09-05 15:30:36">2019/09/05</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/15/考研历程反思总结/" data-tag="三省吾身,考研" data-author>
            <span class="post-title" title="考研历程反思总结">考研历程反思总结</span>
            <span class="post-date" title="2019-04-15 21:25:01">2019/04/15</span>
        </a>
        
        <a class="AI " href="/2019/07/21/计算机视觉存疑解答记录/" data-tag="AI,数学,CV" data-author>
            <span class="post-title" title="计算机视觉存疑解答记录">计算机视觉存疑解答记录</span>
            <span class="post-date" title="2019-07-21 13:04:25">2019/07/21</span>
        </a>
        
        <a class="AI " href="/2019/07/29/达观杯比赛记录/" data-tag="三省吾身,AI,NLP" data-author>
            <span class="post-title" title="达观杯比赛记录">达观杯比赛记录</span>
            <span class="post-date" title="2019-07-29 20:19:39">2019/07/29</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/31/智能博弈挑战赛-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="智能博弈挑战赛">智能博弈挑战赛</span>
            <span class="post-date" title="2020-07-31 21:25:01">2020/07/31</span>
        </a>
        
        <a class="AI " href="/2020/06/02/强化学习在滴滴网约车的应用记录/" data-tag="AI,强化学习,组会报告" data-author>
            <span class="post-title" title="强化学习在滴滴网约车的应用笔记">强化学习在滴滴网约车的应用笔记</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/06/Atari-a2c/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A3C_A2c">Aatri_A3C_A2c</span>
            <span class="post-date" title="2019-07-06 21:25:01">2019/07/06</span>
        </a>
        
        <a class="编程 " href="/2021/02/01/hot100/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="leetcodehot">leetcodehot</span>
            <span class="post-date" title="2021-02-01 20:19:39">2021/02/01</span>
        </a>
        
        <a class="编程 " href="/2020/08/05/lucifer-91/" data-tag="数学,编程,python" data-author>
            <span class="post-title" title="leetcode">leetcode</span>
            <span class="post-date" title="2020-08-05 10:14:45">2020/08/05</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-RL-MP-MRP-MDP-note" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">RL_MDP</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a href="javascript:" data-rel="强化学习">强化学习</a>
            
        </span>
        
        
        <span class="tag">
            
            <a href="javascript:" class="color3">AI</a>
            
            <a href="javascript:" class="color5">深度学习</a>
            
            <a href="javascript:" class="color5">强化学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title="更新时间: 2020-08-22 21:48:51">2019-08-20 21:25</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程"><span class="toc-text">马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫过程与强化学习问题的关系"><span class="toc-text">马尔可夫过程与强化学习问题的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫过程分类"><span class="toc-text">马尔可夫过程分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫奖励过程"><span class="toc-text">马尔可夫奖励过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝尔曼方程"><span class="toc-text">贝尔曼方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫决策过程"><span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#例子-学生MDP"><span class="toc-text">例子-学生MDP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#例子-蛇棋游戏"><span class="toc-text">例子-蛇棋游戏</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#策略（policy）"><span class="toc-text">策略（policy）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#定义-1"><span class="toc-text">定义</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MDP和马尔可夫过程和MRP的关系"><span class="toc-text">MDP和马尔可夫过程和MRP的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#马尔可夫性"><span class="toc-text">马尔可夫性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#环境"><span class="toc-text">环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#马尔可夫决策过程（MDP）三层含义"><span class="toc-text">马尔可夫决策过程（MDP）三层含义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#价值函数（value-function）"><span class="toc-text">价值函数（value function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#贝尔曼期望方程（Bellman-Expectation-Equation）"><span class="toc-text">贝尔曼期望方程（Bellman Expectation Equation）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Expectation-Equation-for-V-π"><span class="toc-text">Bellman Expectation Equation for $V_π$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Expectation-Equation-for-q-π"><span class="toc-text">Bellman Expectation Equation for $q_π$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Expectation-Equation-for-V-π-1"><span class="toc-text">Bellman Expectation Equation for $V_π$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Expectation-Equation-for-q-π-1"><span class="toc-text">Bellman Expectation Equation for $q_π$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Example-Bellman-Expectation-Equation-in-Student-MDP"><span class="toc-text">Example: Bellman Expectation Equation in Student MDP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#贝尔曼期望方程（矩阵形式）"><span class="toc-text">贝尔曼期望方程（矩阵形式）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优价值函数（Optimal-Value-Function）"><span class="toc-text">最优价值函数（Optimal Value Function）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义："><span class="toc-text">定义：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#例子："><span class="toc-text">例子：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#学生MDP的最优价值函数"><span class="toc-text">学生MDP的最优价值函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#例子：学生MDP的最优action-value函数"><span class="toc-text">例子：学生MDP的最优action-value函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#最优策略（Optimal-Policy）"><span class="toc-text">最优策略（Optimal Policy）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#寻找一个最优策略"><span class="toc-text">寻找一个最优策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#学生MDP的最优策略"><span class="toc-text">学生MDP的最优策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#贝尔曼最优方程（Bellman-Optimality-Equation）"><span class="toc-text">贝尔曼最优方程（Bellman Optimality Equation）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Optimality-Equation-for-v-∗"><span class="toc-text">Bellman Optimality Equation for $v_∗$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Optimality-Equation-for-q-∗"><span class="toc-text">Bellman Optimality Equation for $q_∗$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Optimality-Equation-for-v-∗-1"><span class="toc-text">Bellman Optimality Equation for $v_∗$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bellman-Optimality-Equation-for-q-∗-1"><span class="toc-text">Bellman Optimality Equation for $q_∗$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#例子：学生MDP的贝尔曼最优方程"><span class="toc-text">例子：学生MDP的贝尔曼最优方程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#求解贝尔曼最优方程"><span class="toc-text">求解贝尔曼最优方程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MDPs拓展"><span class="toc-text">MDPs拓展</span></a></li></ol></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>主要为马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程的算法总结</p>
<a id="more"></a>
<h2 id="马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程"><a href="#马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程" class="headerlink" title="马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程"></a>马尔可夫过程，马尔可夫奖励过程，马尔可夫决策过程</h2><h3 id="马尔可夫过程与强化学习问题的关系"><a href="#马尔可夫过程与强化学习问题的关系" class="headerlink" title="马尔可夫过程与强化学习问题的关系"></a>马尔可夫过程与强化学习问题的关系</h3><p>一般情况下，强化学习问题都可以形式化为马尔可夫决策过程，也就是说马尔可夫过程正式描述了强化学习的环境，大部分问题中的环境是完全可观测的，当前状态状态过程完全可以被特征化。</p>
<p>而马尔可夫过程都具有的马尔可夫性质，马尔可夫性质是概率论中的一个概念。当一个随机过程在给定现在状态以及过去所有状态的情况下,其未来状态的条件概率分布仅依赖于当前的状态，用公式表示为</p>
<script type="math/tex; mode=display">
\mathbb{P}\left[S_{t+1} \mid S_{t}\right]=\mathbb{P}\left[S_{t+1} \mid S_{1}, \ldots, S_{t}\right]</script><p>也就是说掌握当前状态$S_t$后，历史状态即可被丢弃，在马尔可夫过程中的各个状态相互之间是可以以一定概率进行转移的，这个概率叫做状态转移概率，所有的概率组成的矩阵叫做状态转移矩阵，用数学公式表达为</p>
<script type="math/tex; mode=display">
\mathcal{P}=\text { }\left[\begin{array}{ccc}
\mathcal{P}_{11} & \ldots & \mathcal{P}_{1 n} \\
\vdots & & \\
\mathcal{P}_{n 1} & \ldots & \mathcal{P}_{n n}
\end{array}\right]</script><p>其中矩阵的每一行之和均为1</p>
<h3 id="马尔可夫过程分类"><a href="#马尔可夫过程分类" class="headerlink" title="马尔可夫过程分类"></a>马尔可夫过程分类</h3><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822111740340.png" alt></p>
<p>马尔可夫过程依据时间共可分为三大类：</p>
<ul>
<li>时间和状态都离散的马尔科夫过程称为马尔科夫链</li>
<li>时间连续、状态离散的马尔可夫过程，称为连续时间的马尔科夫链</li>
<li>时间、状态都连续的马尔可夫过程</li>
</ul>
<p>以学生上课，睡觉，刷faceboook为例</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822111101122.png" alt></p>
<p>当学生处在第一节课（Class1）时，他/她有50%的几率会参加第2节课（Class2）；同时在也有50%的几率不在认真听课，进入到浏览facebook这个状态中。在浏览facebook这个状态时，他/她有90%的几率在下一时刻继续浏览，也有10%的几率返回到课堂内容上来。当学生进入到第二节课（Class2）时，会有80%的几率继续参加第三节课（Class3），也有20%的几率觉得课程较难而退出（Sleep）。当学生处于第三节课这个状态时，他有60%的几率通过考试，继而100%的退出该课程，也有40%的可能性需要到去图书馆之类寻找参考文献，此后根据其对课堂内容的理解程度，又分别有20%、40%、40%的几率返回值第一、二、三节课重新继续学习。一个可能的学生马尔科夫链从状态Class1开始，最终结束于Sleep，其间的过程根据状态转化图可以有很多种可能性。</p>
<p>利用概率论与随机过程课程中的知识，也可以其转化为概率转移矩阵</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822111341534.png" alt></p>
<h3 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h3><p>马尔科夫奖励过程在马尔科夫过程的基础上增加了奖励R和衰减系数𝛾，带有value判断的Markov Process，value会告诉我们这个状态有多好。即对于一些从某个Markov Process取样得到的特定的序列，他们已经累计的多少reward。R是当前时刻从状态S得到多少即时奖励,我们关心的是最大化累计的rewards。</p>
<p>很多人会纠结为什么奖励是t+1时刻的。照此理解起来相当于离开这个状态才能获得奖励而不是进入这个状态即获得奖励。这仅是一个约定，为了在描述RL问题中涉及到的观测O、行为A、和奖励R时比较方便。他同时指出如果把奖励改为$R_t$，而不是$R_{t+1}$ ，只要规定好，本质上意义是相同的，在表述上可以把奖励描述为“当进入某个状态会获得相应的奖励”。</p>
<p>$\gamma$∈ [0, 1]，它的引入有很多理由，其中有数学表达的方便，避免陷入无限循环，远期利益具有一定的不确定性，符合人类对于眼前利益的追求，符合金融学上获得的利益能够产生新的利益因而更有价值等等。</p>
<p>$R_s$是在状态s下，奖励的期望。因为从状态s可能会到达多个状态，到每个状态都有一个reward，我们需要把每个状态的reward求均值。</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822111755466.png" alt></p>
<p>下图是一个“马尔科夫奖励过程”图示的例子，在“马尔科夫过程”基础上增加了针对每一个状态的奖励，由于不涉及衰减系数相关的计算，这张图并没有特殊交代衰减系数值的大小。</p>
<p>以Class2状态为例，我们假定不管到达哪个后继状态，我们的reward都是-2，所以Class3的$R_s$为-2*0.6+-2*0.4=-2，这里是我们为了方便举例设定的。在真实应用中不一定到达每个后继状态reward都是-2</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822111854526.png" alt></p>
<p>添加上即时奖励的概念后，会自然产生一个相关的回报值的概念，回报值$G_t$ 是在一个马尔科夫奖励链上从t时刻开始往后所有的奖励的有衰减的总和。</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822112151775.png" alt></p>
<p>其中，折扣值𝛾∈[0,1]。而经过k +1个时间步长后获得奖励R的值为$𝛾^𝑘 𝑅_(𝑡+𝑘+1)$ ,这将即时奖励（ immediate reward ）视为高于延迟奖励（ delayed reward）</p>
<ul>
<li>𝛾接近于0会导致“近视”评估</li>
<li>𝛾接近于1会导致“远视”评估</li>
</ul>
<p>一般情况下计算长期累计回报值时都会将未来的奖励值进行打折，其原因是什么呢？</p>
<ul>
<li>数学上方便计算</li>
<li>避免循环马尔可夫过程中的无限收益（有的游戏或者实例是无限马尔可夫过程，如果不带折扣因子，表示某一状态的价值函数就是无穷大，这样是无法通过价值函数的比较产生决策，所以添加折扣因子使得价值函数存在上界）</li>
<li>如果奖励是财务奖励，则即时奖励比延迟奖励可能会获得更多的利息（对于同样数值大小的奖励，我们可能更希望在下一时刻就得到，而不是很多时间步之后再得到，所以添加折扣因子可以从一定程度上减小来未来奖励值的权重）</li>
<li>有时可能会使用未折现的马尔可夫奖励流程（即𝛾=1）, 例如：一个有终止状态的马尔可夫过程</li>
</ul>
<p>而价值函数是未来累计奖励的期望值。<img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822112919928.png" alt></p>
<p>价值函数v(s)给出状态 s 的长期值（long-term value ），特点为从状态s开始的预期收益。<br>v(s) 是从状态 s 开始的一个MRP的期望回报。</p>
<p>下图为当$\gamma = 0.5$时的学生上课的例子</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822113213420.png" alt></p>
<p>按照此种方法计算，当$\gamma=0$的时候，计算结果如下<img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822113309222.png" alt></p>
<p>当$\gamma=0.9$计算结果如下</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822113347732.png" alt></p>
<p>当$\gamma=1$计算结果如下</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822113415002.png" alt></p>
<h3 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h3><p>贝尔曼方程的基本思想是对value function进行递归分解。一个是即时奖励$R_{t+1}$</p>
<p>另一个是 后继状态的折扣值$γV_{S_{t+1}}$</p>
<p>这个推导过程相对简单，仅在导出最后一行时，将 $G_{t+1}$变成了$ v_{St+1}$。其理由是收获的期望等于收获的期望的期望</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822113544342.png" alt></p>
<blockquote>
<p>备注：为什么即时奖励索引为t+1？<br> 这只是一种表示方式，也可以不这么表示。这种表示方式的理由是：考虑了environment和agent之间的界限，它的思想是，我们采取action进入环境后，然后一个时间步就产生了，环境发生改变后，因为下标是在控制环境传递回来的时间步，所以说是新时间步。环境发生改变之后，任何东西的下标都会变成t+1。</p>
</blockquote>
<p>如果用s’表示s状态下一时刻任一可能的状态</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822114401571.png" alt></p>
<p>那么贝尔曼方程可以写为</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822114446595.png" alt></p>
<p>下图已经给出了γ=1时各状态的价值（该图没有文字说明γ=1，根据视频讲解和前面图示以及状态方程的要求，γ必须要确定才能计算）</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822114530061.png" alt></p>
<p>同时也可以将贝尔曼方程可使用矩阵简明表示</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822114617597.png" alt></p>
<p>而其中v是一个列向量，每个状态一个条目</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822114642511.png" alt></p>
<p>可以看出贝尔曼方程是一个线性方程，可以直接求解</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200822114741766.png" alt></p>
<p>对于n个状态，计算复杂度为$O(n^3)$,对于大型MRP，有很多迭代方法，例如：</p>
<ul>
<li>Dynamic programming（动态规划）</li>
<li>Monte-Carlo evaluation（蒙特卡罗评估）</li>
<li><p>Temporal-Difference learning（时序差分学习）直接求解仅适用于小型MRP</p>
<h3 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h3></li>
</ul>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>马尔可夫决策过程（MDP）是具有决策的马尔可夫奖励过程（MRP）， 它在所有状态具有马尔可夫性质的环境中</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\image-20200821185922248.png" alt></p>
<p>相较于马尔科夫奖励过程，马尔科夫决定过程多了一个行为集合A，它是这样的一个元组: <s, a, p, r, γ>。看起来很类似马尔科夫奖励过程，但这里的P和R都与具体的行为a对应，而不像马尔科夫奖励过程那样仅对应于某个状态，A表示的是有限的行为的集合。具体的数学表达式如图。</s,></p>
<h4 id="例子-学生MDP"><a href="#例子-学生MDP" class="headerlink" title="例子-学生MDP"></a>例子-学生MDP</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821190017593.png" alt></p>
<p>上图给出了一个可能的MDP的状态转化图。图中红色的文字表示的是采取的行为，而不是先前的状态名。</p>
<p>对比之前的学生MRP示例可以发现，即时奖励与行为对应了，同一个状态下采取不同的行为得到的即时奖励是不一样的。由于引入了Action，容易与状态名混淆，因此此图没有给出各状态的名称；</p>
<p>此图还把Pass和Sleep状态合并成一个终止状态；另外当选择”去查阅文献”这个动作时，主动进入了一个临时状态（图中用黑色小实点表示），随后被动的被环境按照其动力学分配到另外三个状态，也就是说此时Agent没有选择权决定去哪一个状态。</p>
<h4 id="例子-蛇棋游戏"><a href="#例子-蛇棋游戏" class="headerlink" title="例子-蛇棋游戏"></a>例子-蛇棋游戏</h4><ul>
<li><p>蛇棋游戏介绍</p>
<p>蛇棋是一个棋盘游戏， 棋盘上有许多小格子，每个格子代表一个位置</p>
<p>除此之外，游戏还需要几个棋子和骰子，游戏最终的目标是到达“ 100”处</p>
<p>假设玩家的手法只包含两种： 一种可以均匀投掷出 1～6 这 6 个数字，另一种可以均匀投掷出1～3 这 3 个数字</p>
</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821190138456.png" alt></p>
<p>哪些因素决定了蛇棋的最终获得的分数？</p>
<p>选择什么样的手法投掷（可由玩家决定）</p>
<p>投掷出的数目大小（玩家无法确定的，受环境的随机性控制）</p>
<p>若用S_t表示 t 时刻游戏状态的观测值， 也就是行走到的位置</p>
<p>用a_t表示 t 时刻选择的手法，那么游戏过程就可以用一条状态-行动链表示：</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821190240751.png" alt></p>
<ul>
<li><p>蛇棋的玩法</p>
<p>玩家每人拥有一个棋子，出发点在图中标为“1”的格子处。</p>
<p>玩家依次掷骰子，根据骰子的点数向前行进相应的步数。 假设玩家的棋子在“1”处，并且投掷出“4”，则笔者的棋子就可以到达 5”的位置。</p>
<p>棋盘上有一些梯子，它的两边与棋盘上的两个格子相连。 如果棋子落在其中 一个格子上，就会自动走到梯子对应的另一个格子中。 上图所示的棋盘为例，如果玩家的棋子在“1 ”处，并且投掷出“2”，棋子将到达“3”处，而此处有梯子， 棋子将直接前进到梯子的另一端“20”的位置。</p>
<p>如果在到达时投掷的数字加上当前的位置超过了 100，那么棋子将首先到达 100，剩余的步数将反向前进。</p>
</li>
</ul>
<h4 id="策略（policy）"><a href="#策略（policy）" class="headerlink" title="策略（policy）"></a>策略（policy）</h4><h5 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h5><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821190637320.png" alt></p>
<ul>
<li><p>策略完全定义了agent的行为</p>
</li>
<li><p>玩家根据当前的状态选择“自认为”最好的行动方式 </p>
</li>
<li><p>对于蛇棋这个问题，我们可以认为玩家会在内心对每一个行动进行衡量，并最终选择评价最高的行动</p>
</li>
<li><p>MDP策略取决于当前状态（而不是历史记录）</p>
</li>
<li><p>即策略是静态的（与时间无关）</p>
</li>
</ul>
<h4 id="MDP和马尔可夫过程和MRP的关系"><a href="#MDP和马尔可夫过程和MRP的关系" class="headerlink" title="MDP和马尔可夫过程和MRP的关系"></a>MDP和马尔可夫过程和MRP的关系</h4><p>给定一个MDP：M=<s,a,p,r,γ>和一个策略 π</s,a,p,r,γ></p>
<p>状态序列$S_1,S_2,…$是马尔可夫过程$⟨S,P^π ⟩$</p>
<p>状态和奖励序列$S_1,R_1,S_2,…$是一个马尔可夫奖励过程$⟨S,P^π,R^π,γ⟩$</p>
<p>满足以下两个方程</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821190541067.png" alt></p>
<p>用文字表述是这样的：当前状态s下执行某一指定策略得到的即时奖励是该策略下所有可能行为得到的奖励与该行为发生的概率的乘积的和。</p>
<p>策略在MDP中的作用相当于agent可以在某一个状态时做出选择，进而有形成各种马尔科夫过程的可能，而且基于策略产生的每一个马尔科夫过程是一个马尔科夫奖励过程，各过程之间的差别是不同的选择产生了不同的后续状态以及对应的不同的奖励。</p>
<h4 id="马尔可夫性"><a href="#马尔可夫性" class="headerlink" title="马尔可夫性"></a>马尔可夫性</h4><ul>
<li><p>使用蛇棋中状态之间无依赖的性质进行化筒，即当前时刻选择什么行动只和当前的状态有关，和前面的状态与行动无关。</p>
</li>
<li><p>行动的评价越高，行动产生的概率越大</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821191115753.png" alt></p>
</li>
<li><p>进一步化简，由于行动的集合是离散有限的，可以把选择行动的问题变 成一个多分类问题</p>
</li>
</ul>
<h4 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h4><ul>
<li><p>环境决定：从行动到状态的转换（第二种转换）</p>
</li>
<li><p>玩家完成行动后，环境会受到影响并完成状态的转换</p>
</li>
<li><p>下一步的状态只受前一步状态影响，不受更前面的状态影响，于是这里的状态转换就能以概率的形式表现为</p>
</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821191154589.png" alt></p>
<h4 id="马尔可夫决策过程（MDP）三层含义"><a href="#马尔可夫决策过程（MDP）三层含义" class="headerlink" title="马尔可夫决策过程（MDP）三层含义"></a>马尔可夫决策过程（MDP）三层含义</h4><ul>
<li><p>“<strong>马尔可夫</strong>”表示了状态间的依赖性。 </p>
<p>当前状态的取值只和前一个状态产生依 赖，不和更早的状态产生联系。</p>
</li>
<li><p>“<strong>决策</strong>”表示了其中的策略部分将由Agent决定。 </p>
<ul>
<li><p>Agent 可以通过自己的行动 改变状态序列</p>
</li>
<li><p>和环境中存在的随机性共同决定未来的状态。</p>
</li>
</ul>
</li>
<li><p>“<strong>过程</strong>”表示了时间的属性。 </p>
<ul>
<li><p>Agent 行动后 ，环境的状态将发生改变</p>
</li>
<li><p>同时时间向前推进，新的状态产生</p>
</li>
<li><p>Agent 将获得观测值，于是新的行动产生，然后状态再更新</p>
</li>
</ul>
</li>
</ul>
<h4 id="价值函数（value-function）"><a href="#价值函数（value-function）" class="headerlink" title="价值函数（value function）"></a>价值函数（value function）</h4><ul>
<li>定义：一个MDP的state-value函数$V_π (s)$是从状态s开始，遵循策略π得到的期望回报(expected return)</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192358891.png" alt></p>
<ul>
<li>定义： action-value函数$q_π (s,a)$从状态s开始，采取行为a，遵循策略π得到的期望回报(expected return)</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192418505.png" alt></p>
<p><strong>例子：学生MDP的state-value函数</strong></p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192457145.png" alt></p>
<h4 id="贝尔曼期望方程（Bellman-Expectation-Equation）"><a href="#贝尔曼期望方程（Bellman-Expectation-Equation）" class="headerlink" title="贝尔曼期望方程（Bellman Expectation Equation）"></a>贝尔曼期望方程（Bellman Expectation Equation）</h4><ul>
<li>定义：一个MDP的state-value函数可以再次分解为即时奖励加上后继状态的discounted value</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192619148.png" alt></p>
<ul>
<li><p>定义： action-value函数可以类似地分解:</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192640030.png" alt>-</p>
</li>
</ul>
<h4 id="Bellman-Expectation-Equation-for-V-π"><a href="#Bellman-Expectation-Equation-for-V-π" class="headerlink" title="Bellman Expectation Equation for $V_π$"></a>Bellman Expectation Equation for $V_π$</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192855287.png" alt></p>
<p>上图中，空心较大圆圈表示状态，黑色实心小圆表示的是动作本身，连接状态和动作的线条仅仅把该状态以及该状态下可以采取的行为关联起来。</p>
<p>可以看出，在遵循策略π时，状态s的价值体现为在该状态下遵循某一策略而采取所有可能行为的价值按行为发生概率的乘积求和。</p>
<h4 id="Bellman-Expectation-Equation-for-q-π"><a href="#Bellman-Expectation-Equation-for-q-π" class="headerlink" title="Bellman Expectation Equation for $q_π$"></a>Bellman Expectation Equation for $q_π$</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821192956795.png" alt></p>
<p>它表明，一个某一个状态下采取一个行为的价值，可以分为两部分：其一是离开这个状态的价值，其二是所有进入新的状态的价值于其转移概率乘积的和。</p>
<h4 id="Bellman-Expectation-Equation-for-V-π-1"><a href="#Bellman-Expectation-Equation-for-V-π-1" class="headerlink" title="Bellman Expectation Equation for $V_π$"></a>Bellman Expectation Equation for $V_π$</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193025020.png" alt></p>
<p>一个状态下可能执行多个不同的action，执行一个action也可能导致不同的环境变化，进而引起状态的变化。</p>
<h4 id="Bellman-Expectation-Equation-for-q-π-1"><a href="#Bellman-Expectation-Equation-for-q-π-1" class="headerlink" title="Bellman Expectation Equation for $q_π$"></a>Bellman Expectation Equation for $q_π$</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193059624.png" alt></p>
<p>从某个状态执行一个确定的action可以达到多个后继状态，每个后继状态又可能执行不同的action</p>
<h4 id="Example-Bellman-Expectation-Equation-in-Student-MDP"><a href="#Example-Bellman-Expectation-Equation-in-Student-MDP" class="headerlink" title="Example: Bellman Expectation Equation in Student MDP"></a><strong>Example</strong>: Bellman Expectation Equation in Student MDP</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193243399.png" alt></p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193232740.png" alt></p>
<p>上图中 7.4 这个值可以用这个公式算出：</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193335779.png" alt></p>
<h4 id="贝尔曼期望方程（矩阵形式）"><a href="#贝尔曼期望方程（矩阵形式）" class="headerlink" title="贝尔曼期望方程（矩阵形式）"></a><strong>贝尔曼期望方程（矩阵形式）</strong></h4><ul>
<li><p>Bellman期望方程可以使用归纳MRP简洁表达：</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193435174.png" alt></p>
</li>
<li><p>直接求解</p>
</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193446467.png" alt></p>
<h3 id="最优价值函数（Optimal-Value-Function）"><a href="#最优价值函数（Optimal-Value-Function）" class="headerlink" title="最优价值函数（Optimal Value Function）"></a>最优价值函数（Optimal Value Function）</h3><h4 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h4><ul>
<li>最优state-value函数$V_∗ (s)$是所有策略下最大价值函数</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193613944.png" alt></p>
<ul>
<li>最优action-value函数$q_∗ (s,a)$是所有策略下最大action-value函数</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193626220.png" alt></p>
<ul>
<li><p>最优价值函数明确了MDP的最优可能表现</p>
</li>
<li><p>当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。</p>
</li>
</ul>
<h4 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h4><h4 id="学生MDP的最优价值函数"><a href="#学生MDP的最优价值函数" class="headerlink" title="学生MDP的最优价值函数"></a>学生MDP的最优价值函数</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193726076.png" alt></p>
<h4 id="例子：学生MDP的最优action-value函数"><a href="#例子：学生MDP的最优action-value函数" class="headerlink" title="例子：学生MDP的最优action-value函数"></a>例子：学生MDP的最优action-value函数</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193813028.png" alt></p>
<h4 id="最优策略（Optimal-Policy）"><a href="#最优策略（Optimal-Policy）" class="headerlink" title="最优策略（Optimal Policy）"></a>最优策略（Optimal Policy）</h4><p>定义一个基于policy的部分排序</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193928250.png" alt></p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821193932158.png" alt></p>
<ul>
<li><p>如果一个policy的value function大于另一个，则这个policy比另一个好。</p>
</li>
<li><p>至少存在一个最好的policy</p>
</li>
<li><p>可以有不止一个最优policy，多个最优policy必须达到相同数量的reward</p>
</li>
<li><p>例子：在你的MDP里有两个不同的action，可以带你到相同的state，对于你采取那个action其实无所谓，他们都可以是最优。</p>
</li>
</ul>
<p><strong>定理</strong> </p>
<p>对于任何MDP，下面几点成立：<br> 1.存在一个最优策略，比任何其他策略更好或至少相等；<br> 2.所有的最优策略有相同的最优价值函数；<br> 3.所有的最优策略具有相同的行为价值函数。</p>
<h4 id="寻找一个最优策略"><a href="#寻找一个最优策略" class="headerlink" title="寻找一个最优策略"></a>寻找一个最优策略</h4><p>可以通过最大化$q_∗ (s,a)$来找到最佳策略</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194111055.png" alt></p>
<ul>
<li><p>任何MDP始终都有确定性的最佳策略</p>
</li>
<li><p>如果我们知道$q_∗ (s,a)$，我们将立即获得最优策略</p>
</li>
</ul>
<h4 id="学生MDP的最优策略"><a href="#学生MDP的最优策略" class="headerlink" title="学生MDP的最优策略"></a>学生MDP的最优策略</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194143935.png" alt></p>
<h4 id="贝尔曼最优方程（Bellman-Optimality-Equation）"><a href="#贝尔曼最优方程（Bellman-Optimality-Equation）" class="headerlink" title="贝尔曼最优方程（Bellman Optimality Equation）"></a>贝尔曼最优方程（Bellman Optimality Equation）</h4><h4 id="Bellman-Optimality-Equation-for-v-∗"><a href="#Bellman-Optimality-Equation-for-v-∗" class="headerlink" title="Bellman Optimality Equation for $v_∗$"></a><strong>Bellman</strong> <strong>Optimality Equation for</strong> $v_∗$</h4><ul>
<li>一个状态的最优价值等于从该状态出发采取的所有行为产生的行为价值中最大的那个行为价值：</li>
</ul>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194332460.png" alt></p>
<h4 id="Bellman-Optimality-Equation-for-q-∗"><a href="#Bellman-Optimality-Equation-for-q-∗" class="headerlink" title="Bellman Optimality Equation for $q_∗$"></a><strong>Bellman</strong> <strong>Optimality Equation for</strong> $q_∗$</h4><p>n在某个状态s下，采取某个行为的最优价值由2部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有能到达的状态 s’ 的最优状态价值按出现概率求和：</p>
<p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194407159.png" alt></p>
<h4 id="Bellman-Optimality-Equation-for-v-∗-1"><a href="#Bellman-Optimality-Equation-for-v-∗-1" class="headerlink" title="Bellman Optimality Equation for $v_∗$"></a><strong>Bellman</strong> <strong>Optimality Equation for</strong> $v_∗$</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194437960.png" alt></p>
<h4 id="Bellman-Optimality-Equation-for-q-∗-1"><a href="#Bellman-Optimality-Equation-for-q-∗-1" class="headerlink" title="Bellman Optimality Equation for $q_∗$"></a>Bellman Optimality Equation for $q_∗$<img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194518901.png" alt></h4><h4 id="例子：学生MDP的贝尔曼最优方程"><a href="#例子：学生MDP的贝尔曼最优方程" class="headerlink" title="例子：学生MDP的贝尔曼最优方程"></a><strong>例子：学生</strong>MDP的贝尔曼最优方程</h4><p><img src="/2019/08/20/RL-MP-MRP-MDP-note/D:/rockblog\source\rockblog\source\_posts\RL-MP-MRP-MDP-note\\image-20200821194614933.png" alt></p>
<h4 id="求解贝尔曼最优方程"><a href="#求解贝尔曼最优方程" class="headerlink" title="求解贝尔曼最优方程"></a>求解贝尔曼最优方程</h4><ul>
<li><p>Bellman最优方程是非线性的</p>
</li>
<li><p>没有固定的解决方案</p>
</li>
<li><p>通过一些迭代方法来解决：</p>
<ul>
<li>价值迭代（Value Iteration）</li>
<li>策略迭代（Policy Iteration）</li>
<li>Q学习（Q-learning）</li>
<li>Sarsa</li>
</ul>
</li>
</ul>
<h4 id="MDPs拓展"><a href="#MDPs拓展" class="headerlink" title="MDPs拓展"></a>MDPs拓展</h4><ul>
<li><p>Infinite and continuous MDPs（无限和连续的MDPs）</p>
</li>
<li><p>Partially observable MDPs（部分可观察的MDPs）</p>
</li>
<li><p>Undiscounted, average reward MDPs（无折扣的平均奖励MDPs）</p>
</li>
</ul>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1415500736@qq.com </span>
    </div>
</article>


<p>
    <a href="javascript:void(0)" class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>RL_MDP</p>
    
    <p><span class="copy-title">本文作者:</span><a href="javascript:void(0)" title="Rock">Rock</a></p>
    <p><span class="copy-title">发布时间:</span>2019-08-20, 21:25:01</p>
    <p><span class="copy-title">最后更新:</span>2020-08-22, 21:48:51</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2019/08/20/RL-MP-MRP-MDP-note/" title="RL_MDP">http://rock-blog.top/2019/08/20/RL-MP-MRP-MDP-note/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: 'e78b4c19bc08850d88df',
            clientSecret: '308b55a6d580ee7a819af0f950b3188be697ae29',
            repo: 'guobaoyo.github.io',
            owner: 'guobaoyo',
            admin: ['guobaoyo'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js" value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">

    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2016-2019 Rock&#39;s  blog</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class id="rocket" href="javascript:void(0)"></a>
    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close" href="javascript:void(0)" onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1"></script>

<script src="/js/script.js?v=1.0.1"></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#三省吾身','#AI','#数学','#深度学习','#编程','#CV','#python','#强化学习','#技术小节','#go','#leetcode','#技术小结','#组会报告','#考研','#NLP',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #c1bfc1;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.5;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
