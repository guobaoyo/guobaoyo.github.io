<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content>
  <meta name="author" content="Rock">
  <!-- Open Graph Data -->
  <meta property="og:title" content="deep-reinforcement-learning">
  <meta property="og:description" content>
  <meta property="og:site_name" content="Rock-Blog">
  <meta property="og:type" content="article">
  <meta property="og:image" content="http://rock-blog.top">
  
    <link rel="alternate" href="/atom.xml" title="Rock-Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>Rock-Blog</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/龙珠3.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">deep-reinforcement-learning</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/guobaoyo">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:shi_chenggong@163.com">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Rock</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2019-09-14</span>
            <span class="time">12:19:39</span>
          </span>
          
          <!--  Categories  -->
            <span class="categories info">Under 

<a href="/categories/强化学习/">强化学习</a>
</span>
          
        </div>
        <!-- Tags -->
        
          <div class="post-tags text-muted">
            Tags: 

<a class="tag" href="/tags/人工智能/">#人工智能</a> <a class="tag" href="/tags/深度强化学习/">#深度强化学习</a> <a class="tag" href="/tags/数学/">#数学</a>


          </div>
        
        <!-- Post Main Content -->
        <div class="post-content">
          <p>李宏毅《深度强化学习》学习笔记-<a href="https://www.bilibili.com/video/av58458003/?p=1" target="_blank" rel="noopener">https://www.bilibili.com/video/av58458003/?p=1</a></p>
<a id="more"></a>
<h1 id="李宏毅《深度强化学习》学习笔记"><a href="#李宏毅《深度强化学习》学习笔记" class="headerlink" title="李宏毅《深度强化学习》学习笔记"></a>李宏毅《深度强化学习》学习笔记</h1><h2 id="P1-0导论："><a href="#P1-0导论：" class="headerlink" title="P1-0导论："></a>P1-0导论：</h2><p>1.注意在强化学习里面的state指的是环境的状态而不是智能体的状态，智能体观测到环境的状态，也叫observation,而智能体做的动作叫action或者是policy。</p>
<p>2.在下棋这种博弈对抗的游戏中不建议用监督学习的原因是当输入某个棋局状态的时候，无法像图像识别或检测位置一样给出准确的标答，所以需要用强化学习的思想去训练智能体，故在训练Alpha Go的训练过程中是搞两个智能体互相下棋，同理还可以应用在制作聊天机器人上。</p>
<p>3.</p>
<p><img src="/2019/09/14/deep-reinforcement-learning/应用方向1.png" alt="1强化学习应用方向"></p>
<p><img src="/2019/09/14/deep-reinforcement-learning/应用方向2.png" alt="1强化学习应用方向"></p>
<p><img src="/2019/09/14/deep-reinforcement-learning/more-reference.png" alt="更多"></p>
<p>4.李宏毅老师认为强化学习的难点：    </p>
<ul>
<li>reward的出现具有延迟性</li>
<li>智能体需要主动去<strong>探索</strong>，对环境做出改变进而得到正向或负向的反馈</li>
</ul>
<p>5.A3C：</p>
<p><img src="/2019/09/14/deep-reinforcement-learning/A3C.png" alt="A3C"></p>
<p>6.李宏毅老师讲因为电玩的未知性太大，model based的方法在电玩游戏方面应用比较困难。</p>
<p>7.深度学习的步骤:</p>
<p>​    1.使用神经网络作为智能体，当智能体是深度神经网络的时候，这套系统就是深度强化学习系统。</p>
<p>​    2.最大化total reward的多次平均值（因为游戏具有随机性，且对于同一个Actor，输入相同的observation，做出的反应也可能不同），并用total reward评判函数的好坏，注：</p>
<script type="math/tex; mode=display">
\tau=\left\{s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \cdots, s_{T}, a_{T}, r_{T}\right\}且
R(\tau)=\sum_{n=1}^{N} r_{n}</script><script type="math/tex; mode=display">
\overline{R}_{\theta}=\sum_{\tau} R(\tau) P(\tau | \theta) \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right)且P(\tau | \theta)是智能体参数为\theta 的时候发生事件\tau的概率</script><p>​    3.选择最好的Actor-&gt;使用Gradient Ascent的方法找到θ使下式最大：</p>
<script type="math/tex; mode=display">
\overline{R}_{\theta}=\sum_{\tau} R(\tau) P(\tau | \theta) \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right)</script><p>​            3.1随机初始化θ0和η0，再对 <script type="math/tex">\overline{R}_{\theta}</script> 求微分</p>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        </p><p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
//<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

