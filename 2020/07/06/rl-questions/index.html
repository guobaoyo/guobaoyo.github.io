<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <title>RL_questions | Rock-Blog</title>
  <meta name="keywords" content=" AI , 深度学习 , 强化学习 ">
  <meta name="description" content="RL_questions | Rock-Blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="2020-09-25智能对抗初赛后的停顿与反思，全文主要 https://mp.weixin.qq.com/s/FFb1HZOQn48V-L7IhFgRGQ 或者 https://zhuanlan.zhihu.com/p/39999667 DRL近三年的应用成果分类 棋牌类游戏（麻将、德克萨斯等游戏）、Atari游戏、星际争霸达到专业玩家水平甚至超人类水平-deepmind  控制机械臂-open">
<meta name="keywords" content="数学,深度学习,强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="DRL_report">
<meta property="og:url" content="http://rock-blog.top/20120/09/25/DRL-report/index.html">
<meta property="og:site_name" content="Rock-Blog">
<meta property="og:description" content="2020-09-25智能对抗初赛后的停顿与反思，全文主要 https://mp.weixin.qq.com/s/FFb1HZOQn48V-L7IhFgRGQ 或者 https://zhuanlan.zhihu.com/p/39999667 DRL近三年的应用成果分类 棋牌类游戏（麻将、德克萨斯等游戏）、Atari游戏、星际争霸达到专业玩家水平甚至超人类水平-deepmind  控制机械臂-open">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://rock-blog.top/20120/09/25/DRL-report/D:/rockblog/source/rockblog/source/_posts/DRL-report/1.jpg">
<meta property="og:updated_time" content="2020-09-26T13:30:39.729Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DRL_report">
<meta name="twitter:description" content="2020-09-25智能对抗初赛后的停顿与反思，全文主要 https://mp.weixin.qq.com/s/FFb1HZOQn48V-L7IhFgRGQ 或者 https://zhuanlan.zhihu.com/p/39999667 DRL近三年的应用成果分类 棋牌类游戏（麻将、德克萨斯等游戏）、Atari游戏、星际争霸达到专业玩家水平甚至超人类水平-deepmind  控制机械臂-open">
<meta name="twitter:image" content="http://rock-blog.top/20120/09/25/DRL-report/D:/rockblog/source/rockblog/source/_posts/DRL-report/1.jpg">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>

<script src="/js/iconfont.js?v=1.0.1"></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="true">
  <input class="theme_blog_path" value>
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg">
</a>
<div class="author">
    <span>Rock</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/guobaoyo" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"/>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:1415500736@qq.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"/>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=1415500736&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"/>
                </svg>
            
        </a>
        
    
        
        <a title="neteasemusic" href="https://music.163.com/#/user/home?id=280020740" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-neteasemusic"/>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(48)</small></div></li>
    
        
            
            <li><div data-rel="三省吾身">三省吾身<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="AI">AI<small>(7)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="强化学习">强化学习<small>(16)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="编程">编程<small>(17)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数学">数学<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="组会报告">组会报告<small>(1)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    
    
    </div>
    <div><a class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="48">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
            <li><a target="_blank" href="http://zivblog.top">王金锋</a></li>
            
            <li><a target="_blank" href="http://yearing1017.cn/">进哥</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off" id="local-search-input">
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none">
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">三省吾身</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">AI</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">数学</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">深度学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">CV</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">python</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">编程</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">强化学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">技术小节</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">leetcode</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">技术小结</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">go</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">组会报告</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">考研</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">NLP</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a class="三省吾身 " href="/2019/07/08/20190708日记/" data-tag="三省吾身,AI,数学" data-author>
            <span class="post-title" title="20190708日记">20190708日记</span>
            <span class="post-date" title="2019-07-08 19:43:26">2019/07/08</span>
        </a>
        
        <a class="AI " href="/2019/05/08/DLwithPython/" data-tag="深度学习,CV,python" data-author>
            <span class="post-title" title="DeepLearning with Python">DeepLearning with Python</span>
            <span class="post-date" title="2019-05-08 17:54:01">2019/05/08</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/29/DLwords/" data-tag="AI,深度学习" data-author>
            <span class="post-title" title="DLwords">DLwords</span>
            <span class="post-date" title="2019-04-29 19:17:39">2019/04/29</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/08/A-algorithm/" data-tag="AI,数学,编程" data-author>
            <span class="post-title" title="A*_algorithm">A*_algorithm</span>
            <span class="post-date" title="2020-06-08 21:25:01">2020/06/08</span>
        </a>
        
        <a class="AI " href="/20120/09/25/DRL-report/" data-tag="数学,深度学习,强化学习" data-author>
            <span class="post-title" title="DRL_report">DRL_report</span>
            <span class="post-date" title="20120-09-25 19:17:39">20120/09/25</span>
        </a>
        
        <a class="强化学习 " href="/2019/08/20/RL-MP-MRP-MDP-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_MDP">RL_MDP</span>
            <span class="post-date" title="2019-08-20 21:25:01">2019/08/20</span>
        </a>
        
        <a class="强化学习 " href="/2020/08/02/RL-PPO-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="pg_note">pg_note</span>
            <span class="post-date" title="2020-08-02 21:25:01">2020/08/02</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/21/RL-basic-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="rl_basic_note">rl_basic_note</span>
            <span class="post-date" title="2019-07-21 21:25:01">2019/07/21</span>
        </a>
        
        <a class="强化学习 " href="/2019/09/10/RL-DP-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_DP">RL_DP</span>
            <span class="post-date" title="2019-09-10 21:25:01">2019/09/10</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/31/RL_A3C_A2C_note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="A3C_note">A3C_note</span>
            <span class="post-date" title="2020-07-31 21:25:01">2020/07/31</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/26/RL_AC-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="ac_note">ac_note</span>
            <span class="post-date" title="2019-07-26 21:25:01">2019/07/26</span>
        </a>
        
        <a class="强化学习 " href="/2020/01/26/RLTF/" data-tag="AI,数学,强化学习" data-author>
            <span class="post-title" title="RLTF">RLTF</span>
            <span class="post-date" title="2020-01-26 19:21:06">2020/01/26</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/12/atari-env-note/" data-tag="深度学习,编程,强化学习" data-author>
            <span class="post-title" title="Atari游戏环境笔记">Atari游戏环境笔记</span>
            <span class="post-date" title="2020-07-12 10:00:12">2020/07/12</span>
        </a>
        
        <a class="强化学习 " href="/2019/08/05/RL-DDPG-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A2c&amp;ppo">Aatri_A2c&amp;ppo</span>
            <span class="post-date" title="2019-08-05 21:25:01">2019/08/05</span>
        </a>
        
        <a class="编程 " href="/2020/09/25/effective-python/" data-tag="python,技术小节" data-author>
            <span class="post-title" title="effective_python_note">effective_python_note</span>
            <span class="post-date" title="2020-09-25 19:17:39">2020/09/25</span>
        </a>
        
        <a class="编程 " href="/2020/10/08/docker-base/" data-tag="python,编程,技术小节" data-author>
            <span class="post-title" title="docker-base">docker-base</span>
            <span class="post-date" title="2020-10-08 17:54:01">2020/10/08</span>
        </a>
        
        <a class="编程 " href="/2021/02/01/hot100/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="leetcodehot100">leetcodehot100</span>
            <span class="post-date" title="2021-02-01 20:19:39">2021/02/01</span>
        </a>
        
        <a class="编程 " href="/2020/01/31/leetcode/" data-tag="数学,python,编程" data-author>
            <span class="post-title" title="leetcode">leetcode</span>
            <span class="post-date" title="2020-01-31 10:14:45">2020/01/31</span>
        </a>
        
        <a class href="/2020/06/05/lgb-note/" data-tag="AI,python,编程" data-author>
            <span class="post-title" title="lgb_note">lgb_note</span>
            <span class="post-date" title="2020-06-05 21:25:01">2020/06/05</span>
        </a>
        
        <a class="编程 " href="/2019/07/23/go-http搭建服务器知识点儿/" data-tag="编程,go" data-author>
            <span class="post-title" title="go-http搭建服务器知识点儿">go-http搭建服务器知识点儿</span>
            <span class="post-date" title="2019-07-23 11:29:08">2019/07/23</span>
        </a>
        
        <a class="数学 " href="/2019/04/25/math/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="线性代数补充笔记">线性代数补充笔记</span>
            <span class="post-date" title="2019-04-25 21:25:01">2019/04/25</span>
        </a>
        
        <a class="AI " href="/2019/09/14/deep-reinforcement-learning/" data-tag="数学,深度学习,强化学习" data-author>
            <span class="post-title" title="deep-reinforcement-learning">deep-reinforcement-learning</span>
            <span class="post-date" title="2019-09-14 10:00:12">2019/09/14</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/21/RL_pg-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="pg_note">pg_note</span>
            <span class="post-date" title="2019-07-21 21:25:01">2019/07/21</span>
        </a>
        
        <a class="AI " href="/2020/06/02/pytorch-note/" data-tag="AI,python,技术小结" data-author>
            <span class="post-title" title="pytorch_note">pytorch_note</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="编程 " href="/2020/07/04/sword-offer03/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer03">剑指offer03</span>
            <span class="post-date" title="2020-07-04 20:19:39">2020/07/04</span>
        </a>
        
        <a class="编程 " href="/2020/07/06/sword-offer04/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer04">剑指offer04</span>
            <span class="post-date" title="2020-07-06 20:19:39">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/06/sword-offer05/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer05">剑指offer05</span>
            <span class="post-date" title="2020-07-06 20:19:39">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/08/sword-offer07/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer07">剑指offer07</span>
            <span class="post-date" title="2020-07-08 20:19:39">2020/07/08</span>
        </a>
        
        <a class="编程 " href="/2020/07/09/sword-offer09/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer09">剑指offer09</span>
            <span class="post-date" title="2020-07-09 20:19:39">2020/07/09</span>
        </a>
        
        <a class="编程 " href="/2020/07/10/sword-offer10/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-10 20:19:39">2020/07/10</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/06/rl-questions/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_questions">RL_questions</span>
            <span class="post-date" title="2020-07-06 21:25:01">2020/07/06</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/11/tianshou-a2c-note/" data-tag="AI,编程,强化学习" data-author>
            <span class="post-title" title="tianshou平台源码阅读笔记">tianshou平台源码阅读笔记</span>
            <span class="post-date" title="2020-06-11 21:25:01">2020/06/11</span>
        </a>
        
        <a class="编程 " href="/2019/04/29/使用Python实现excel中固定数据排序且改名至另一个文件夹/" data-tag="python,编程,技术小结" data-author>
            <span class="post-title" title="基于python实现excel中读取文件目录+相应数据排序+将文件重命名">基于python实现excel中读取文件目录+相应数据排序+将文件重命名</span>
            <span class="post-date" title="2019-04-29 19:30:06">2019/04/29</span>
        </a>
        
        <a class="编程 " href="/2020/07/07/sword-offer06/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer06">剑指offer06</span>
            <span class="post-date" title="2020-07-07 20:19:39">2020/07/07</span>
        </a>
        
        <a class="编程 " href="/2020/07/10/sword-offer11/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-10 20:19:39">2020/07/10</span>
        </a>
        
        <a class="数学 " href="/2019/10/24/数值计算与凸优化补充笔记/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="数值计算与凸优化补充笔记">数值计算与凸优化补充笔记</span>
            <span class="post-date" title="2019-10-24 16:14:54">2019/10/24</span>
        </a>
        
        <a class="三省吾身 " href="/2019/11/03/智源大会听报告笔记/" data-tag="三省吾身,AI" data-author>
            <span class="post-title" title="智源大会听学术报告笔记">智源大会听学术报告笔记</span>
            <span class="post-date" title="2019-11-03 13:24:12">2019/11/03</span>
        </a>
        
        <a class="强化学习 " href="/2020/10/23/tstar/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Tstarbots_note">Tstarbots_note</span>
            <span class="post-date" title="2020-10-23 21:25:01">2020/10/23</span>
        </a>
        
        <a class="编程 " href="/2019/09/05/物体检测打标小脚本-获取当前图片中鼠标位置/" data-tag="python,编程,技术小结" data-author>
            <span class="post-title" title="物体检测打标小脚本-获取当前图片中鼠标位置">物体检测打标小脚本-获取当前图片中鼠标位置</span>
            <span class="post-date" title="2019-09-05 15:30:36">2019/09/05</span>
        </a>
        
        <a class="组会报告 " href="/2019/09/26/组会报告-0930-QLearning/" data-tag="深度学习,强化学习,组会报告" data-author>
            <span class="post-title" title="组会报告-0930-QLearning">组会报告-0930-QLearning</span>
            <span class="post-date" title="2019-09-26 15:15:04">2019/09/26</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/15/考研历程反思总结/" data-tag="三省吾身,考研" data-author>
            <span class="post-title" title="考研历程反思总结">考研历程反思总结</span>
            <span class="post-date" title="2019-04-15 21:25:01">2019/04/15</span>
        </a>
        
        <a class="AI " href="/2019/07/21/计算机视觉存疑解答记录/" data-tag="AI,数学,CV" data-author>
            <span class="post-title" title="计算机视觉存疑解答记录">计算机视觉存疑解答记录</span>
            <span class="post-date" title="2019-07-21 13:04:25">2019/07/21</span>
        </a>
        
        <a class="AI " href="/2020/06/02/强化学习在滴滴网约车的应用记录/" data-tag="AI,强化学习,组会报告" data-author>
            <span class="post-title" title="强化学习在滴滴网约车的应用笔记">强化学习在滴滴网约车的应用笔记</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="编程 " href="/2020/07/19/动态规划/" data-tag="python,leetcode,技术小结" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-19 20:19:39">2020/07/19</span>
        </a>
        
        <a class="强化学习 " href="/2019/07/06/Atari-a2c/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A3C_A2c">Aatri_A3C_A2c</span>
            <span class="post-date" title="2019-07-06 21:25:01">2019/07/06</span>
        </a>
        
        <a class="AI " href="/2019/07/29/达观杯比赛记录/" data-tag="三省吾身,AI,NLP" data-author>
            <span class="post-title" title="达观杯比赛记录">达观杯比赛记录</span>
            <span class="post-date" title="2019-07-29 20:19:39">2019/07/29</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/31/智能博弈挑战赛-note/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="智能博弈挑战赛">智能博弈挑战赛</span>
            <span class="post-date" title="2020-07-31 21:25:01">2020/07/31</span>
        </a>
        
        <a class="编程 " href="/2020/08/05/lucifer-91/" data-tag="数学,python,编程" data-author>
            <span class="post-title" title="leetcode">leetcode</span>
            <span class="post-date" title="2020-08-05 10:14:45">2020/08/05</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-rl-questions" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">RL_questions</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a href="javascript:" data-rel="强化学习">强化学习</a>
            
        </span>
        
        
        <span class="tag">
            
            <a href="javascript:" class="color3">AI</a>
            
            <a href="javascript:" class="color5">深度学习</a>
            
            <a href="javascript:" class="color5">强化学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title="更新时间: 2020-10-27 21:00:15">2020-07-06 21:25</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RL概括与基础"><span class="toc-text">RL概括与基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是强化学习？"><span class="toc-text">问：什么是强化学习？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：那你再详细说说强化学习和监督学习的区别点？"><span class="toc-text">问：那你再详细说说强化学习和监督学习的区别点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：为什么研究强化学习？"><span class="toc-text">问：为什么研究强化学习？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：强化学习的组成成分？"><span class="toc-text">问：强化学习的组成成分？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：强化学习分为哪些种类？"><span class="toc-text">问：强化学习分为哪些种类？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是马尔可夫过程（马尔可夫链）？"><span class="toc-text">问：什么是马尔可夫过程（马尔可夫链）？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是马尔可夫奖励过程？"><span class="toc-text">问：什么是马尔可夫奖励过程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：折扣因子的存在意义是什么？"><span class="toc-text">问：折扣因子的存在意义是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？"><span class="toc-text">问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？"><span class="toc-text">问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：简要说一下MDP里面的价值函数及其推导关系？"><span class="toc-text">问：简要说一下MDP里面的价值函数及其推导关系？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？"><span class="toc-text">问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？"><span class="toc-text">问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是动态规划？（动态规划适合解决什么样的问题？）"><span class="toc-text">问：什么是动态规划？（动态规划适合解决什么样的问题？）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么解决policy-evaluation问题？"><span class="toc-text">问：怎么解决policy evaluation问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？"><span class="toc-text">问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么通过policy-iteration来优化策略？为什么优化后的策略一定比之前的策略要好？"><span class="toc-text">问：怎么通过policy iteration来优化策略？为什么优化后的策略一定比之前的策略要好？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：value-iteration与policy-iteration的区别？"><span class="toc-text">问：value iteration与policy iteration的区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：MC和TD方法的好处是什么？现实情况下没有状态转移矩阵应该怎么办？"><span class="toc-text">问：MC和TD方法的好处是什么？现实情况下没有状态转移矩阵应该怎么办？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：DP和MC之间的区别是什么？"><span class="toc-text">问：DP和MC之间的区别是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：简要介绍一下TD-你觉得TD的优点是什么-（TD于MC的区别是什么？）"><span class="toc-text">问：简要介绍一下TD,你觉得TD的优点是什么?（TD于MC的区别是什么？）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：-epsilon-greedy-有什么作用？-为什么要用-epsilon-greedy-呢？"><span class="toc-text">问：$\epsilon-greedy $ 有什么作用？(为什么要用$\epsilon-greedy $ 呢？)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：Sarsa和Q-learning的异同点？"><span class="toc-text">问：Sarsa和Q-learning的异同点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：强化学习训练十分不稳定的原因有哪些？"><span class="toc-text">问：强化学习训练十分不稳定的原因有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：蒙特卡罗，sarsa，q-learning的收敛问题？"><span class="toc-text">问：蒙特卡罗，sarsa，q-learning的收敛问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：简要说一下deep-q-learning系列的创新点或者遇到什么问题，采用什么技术手法解决的？"><span class="toc-text">问：简要说一下deep-q-learning系列的创新点或者遇到什么问题，采用什么技术手法解决的？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：policy-based和value-based算法的区别是什么？"><span class="toc-text">问：policy-based和value-based算法的区别是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：policy-based的优缺点？"><span class="toc-text">问：policy-based的优缺点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：策略梯度的score-function是怎么来的？有什么作用和几何意义？"><span class="toc-text">问：策略梯度的score function是怎么来的？有什么作用和几何意义？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么推出来最基本的策略梯度算法？"><span class="toc-text">问：怎么推出来最基本的策略梯度算法？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：AC算法有哪些不同形式？为什么critic那里也需要计算TD-error？"><span class="toc-text">问：AC算法有哪些不同形式？为什么critic那里也需要计算TD-error？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：A3C算法流程？"><span class="toc-text">问：A3C算法流程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：A2C算法流程？"><span class="toc-text">问：A2C算法流程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：PPO算法有什么改进点？PPO算法流程？"><span class="toc-text">问：PPO算法有什么改进点？PPO算法流程？</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>主要是看周博磊老师的强化学习视频<a href="https://space.bilibili.com/511221970/channel/detail?cid=105354" target="_blank" rel="noopener">https://space.bilibili.com/511221970/channel/detail?cid=105354</a></p>
<p>再次总结的值得思考的问题（或许也是<strong>面试</strong>经常会问的问题）</p>
<a id="more"></a>
<h2 id="RL概括与基础"><a href="#RL概括与基础" class="headerlink" title="RL概括与基础"></a>RL概括与基础</h2><h4 id="问：什么是强化学习？"><a href="#问：什么是强化学习？" class="headerlink" title="问：什么是强化学习？"></a>问：什么是强化学习？</h4><p>答：这个问题需要与其他同级概念相互比较才会好理解，我理解的强化学习是一机器学习中的一部分，机器学习的学习范式有监督学习，无监督学习，强化学习，其中监督学习有明确的监督信号也就是所谓的标答，无监督学习没有标答主要是想从内部挖掘数据的关联性，而强化学习与监督学习与无监督学习的不同点是强化学习中<strong>没有所谓的标准答案</strong>，有的是智能体与环境这两大组成部分，智能体根据环境的观测状态值做出动作选择，动作输入到环境后环境会改变状态，随着时间往后推移，智能体与环境是在一直交互的，其核心目的是使得强化学习中的智能体在一个<strong>不确定环境</strong>下能够学习到<strong>最大化累计奖励</strong>的<strong>策略</strong>。</p>
<h4 id="问：那你再详细说说强化学习和监督学习的区别点？"><a href="#问：那你再详细说说强化学习和监督学习的区别点？" class="headerlink" title="问：那你再详细说说强化学习和监督学习的区别点？"></a>问：那你再详细说说强化学习和监督学习的区别点？</h4><p>答：在监督学习中，每个数据样本都有一个标准答案，且把这些样本输入到网络时有两个特点</p>
<ul>
<li>这些数据都是符合独立同分布条件</li>
<li>标答也给到网络去计算loss</li>
</ul>
<p>而训练强化学习智能体时，训练数据是一个交互过程（一个时间序列）</p>
<ul>
<li>智能体所观测到的状态不符合独立同分布条件，上一帧和下一帧是十分相关的</li>
<li>在训练的时候不会告诉智能体去做哪个动作，智能体需要自己去与环境进行交互，在最大化累计奖励的过程中去寻找最优策略</li>
<li>奖励（或者说反馈）具有延迟性，也就是智能体当前时刻做出了个动作并不会马上获得奖励，有可能在多步之后再获得奖励，因此强化学习的训练也会更加困难</li>
<li>基于第二点，在强化学习中也不能单凭奖励值的大小去判断这个动作决策的好坏，因为在强化学习中还有个比较重要的概念就是探索，智能体有可能会舍弃掉即时奖励比较大的动作，而去选择即时奖励较小的动作，其根本原因还是第二点虽然当前这个动作的即时奖励较小，但是可能在后面所带来的延迟奖励会很大</li>
</ul>
<h4 id="问：为什么研究强化学习？"><a href="#问：为什么研究强化学习？" class="headerlink" title="问：为什么研究强化学习？"></a>问：为什么研究强化学习？</h4><p>答：从相互比较的角度来看，监督学习中图像分类任务中的标答都是人给定的标准答案，所获得效果最好就是让训练出来的模型无限逼近于人类的图像分类能力。 而强化学习的这种学习范式是让智能体在环境中自己探索，所以在某些环境下有可能会达到超越人类的结果。</p>
<h4 id="问：强化学习的组成成分？"><a href="#问：强化学习的组成成分？" class="headerlink" title="问：强化学习的组成成分？"></a>问：强化学习的组成成分？</h4><p>答：感觉可以答分为两部分，从环境加智能体出发（也就是1+2为智能体，3为环境）</p>
<ul>
<li>决策函数-policy，也就是智能体<ul>
<li><strong>随机</strong>策略函数$\pi(a|s)=P[A_t = a|S_t=s]$ </li>
<li><strong>确定</strong>性策略函数$a^*=argmax_a\pi(a|s)$ </li>
</ul>
</li>
<li>价值函数，是对当前状态或者（状态+动作）的估值</li>
</ul>
<script type="math/tex; mode=display">
v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right] \  \ all \ of \ \ s∈S</script><script type="math/tex; mode=display">
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right]</script><ul>
<li>model-也就是模型，这里虽然叫做模型但是可以理解为环境，该部分主要包含两个方面，其一是状态转移概率是状态s到状态s’的概率<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{P}_{s s^{\prime}}^{a} &=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right] \\
\end{aligned}</script>以及奖励函数（当前状态采取某一动作后会得到多少奖励）<script type="math/tex; mode=display">
\mathcal{R}_{s}^{a} =\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]</script></li>
</ul>
<h4 id="问：强化学习分为哪些种类？"><a href="#问：强化学习分为哪些种类？" class="headerlink" title="问：强化学习分为哪些种类？"></a>问：强化学习分为哪些种类？</h4><p>答：可以从两个角度来分类，</p>
<ul>
<li>第一个角度：从智能体的学习情况分为三种，分别为<ul>
<li>value-based    <ul>
<li>学习值函数</li>
<li>通过比较值函数来生成策略</li>
</ul>
</li>
<li>policy-based<ul>
<li>不学习值函数</li>
<li>输出的是一个策略$\pi$，代表各个动作的概率值</li>
</ul>
</li>
<li>Actor-Critic<ul>
<li>输出两部分为值函数与策略</li>
</ul>
</li>
</ul>
</li>
<li>第二个角度：是否学习环境来分<ul>
<li>model-based：学习了环境的模型，知道状态转移概率</li>
<li>model-free：不知道状态转移概率，通过学习value-function或者policy-function来生成策略</li>
</ul>
</li>
</ul>
<h4 id="问：什么是马尔可夫过程（马尔可夫链）？"><a href="#问：什么是马尔可夫过程（马尔可夫链）？" class="headerlink" title="问：什么是马尔可夫过程（马尔可夫链）？"></a>问：什么是马尔可夫过程（马尔可夫链）？</h4><p>答：马尔可夫过程是一个具有马尔可夫性质的过程，主要是用于研究离散事件在动态系统状态空间的方法。而马尔可夫性(未来状态只与当前状态和当前动作有关而与之前状态无关)可以用如下公式表达</p>
<script type="math/tex; mode=display">
p(s_{t+1}|s_t) = p(s_{t+1}|h_t)其中h_t = {s_1,s_2,s_3,...,s_t}</script><p>而马尔可夫链的主要元素有两部分，分别为各个状态和状态转移矩阵。</p>
<p>用公式表达为</p>
<script type="math/tex; mode=display">
P=\left[\begin{array}{cccc}
P\left(s_{1} \mid s_{1}\right) & P\left(s_{2} \mid s_{1}\right) & \dots & P\left(s_{N} \mid s_{1}\right) \\
P\left(s_{1} \mid s_{2}\right) & P\left(s_{2} \mid s_{2}\right) & \dots & P\left(s_{N} \mid s_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
P\left(s_{1} \mid s_{N}\right) & P\left(s_{2} \mid s_{N}\right) & \dots & P\left(s_{N} \mid s_{N}\right)
\end{array}\right]</script><h4 id="问：什么是马尔可夫奖励过程？"><a href="#问：什么是马尔可夫奖励过程？" class="headerlink" title="问：什么是马尔可夫奖励过程？"></a>问：什么是马尔可夫奖励过程？</h4><p>答：马尔可夫奖励过程是在马尔可夫过程基础上加上奖励函数的过程，主要由三部分组成，分别为状态state，状态转移矩阵，<strong>奖励函数+折扣因子</strong>$(S,P,R,\gamma)$。其中奖励函数和折扣因子用来计算总奖励值Return，用公式表达为$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma ^3 R_{t+4}+…+\gamma^{T-t-1}R_T$</p>
<p>相比于马尔可夫过程，马尔可夫奖励过程中还要多出来的一个概念是价值函数，价值函数是总奖励值的期望值。</p>
<h4 id="问：折扣因子的存在意义是什么？"><a href="#问：折扣因子的存在意义是什么？" class="headerlink" title="问：折扣因子的存在意义是什么？"></a>问：折扣因子的存在意义是什么？</h4><p>答：可以从两个角度来描述其意义</p>
<ul>
<li>有的游戏或者实例是无限马尔可夫过程，如果不带折扣因子，表示某一状态的价值函数就是无穷大，这样是无法通过价值函数的比较产生决策，所以添加折扣因子使得价值函数存在上界</li>
<li>对于同样数值大小的奖励，我们可能更希望在下一时刻就得到，而不是很多时间步之后再得到，所以添加折扣因子可以从一定程度上减小来未来奖励值的权重</li>
</ul>
<h4 id="问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？"><a href="#问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？" class="headerlink" title="问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？"></a>问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？</h4><p>答：</p>
<blockquote>
<p>贝尔曼方程主要描述了当前状态和未来状态的迭代关系</p>
</blockquote>
<p>有条弹幕说的很贴切，可以把贝尔曼方程理解为套娃</p>
<script type="math/tex; mode=display">
V(s)=\underbrace{R(s)}_{\text {Immediate reward }}+\underbrace{\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {Discounted sum of future reward }}</script><p>$V(s)$是由s状态的奖励和其后续状态$s’$的价值函数决定的</p>
<p>那么$s’$也是这样，由s’的奖励值和$s’$的后续状态$s’’$的价值函数决定……</p>
<script type="math/tex; mode=display">
\begin{array}{c}
{\left[\begin{array}{c}
V\left(s_{1}\right) \\
V\left(s_{2}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right]=\left[\begin{array}{c}
R\left(s_{1}\right) \\
R\left(s_{2}\right) \\
\vdots \\
R\left(s_{N}\right)
\end{array}\right]+\gamma\left[\begin{array}{cccc}
P\left(s_{1} \mid s_{1}\right) & P\left(s_{2} \mid s_{1}\right) & \dots & P\left(s_{N} \mid s_{1}\right) \\
P\left(s_{1} \mid s_{2}\right) & P\left(s_{2} \mid s_{2}\right) & \dots & P\left(s_{N} \mid s_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
P\left(s_{1} \mid s_{N}\right) & P\left(s_{2} \mid s_{N}\right) & \dots & P\left(s_{N} \mid s_{N}\right)
\end{array}\right]\left[\begin{array}{c}
V\left(s_{1}\right) \\
V\left(s_{2}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right]}
\end{array}</script><p>对于上面的式子可以由如下方法计算$V$值</p>
<p>直接计算：（缺点为涉及矩阵求逆，计算量巨大）</p>
<script type="math/tex; mode=display">
V=R+\gamma PV\\
V=(I-\gamma P)^{-1}R</script><p>迭代计算：（后面会有具体的问答）</p>
<ul>
<li>DP</li>
<li>MC</li>
<li>TD</li>
</ul>
<h4 id="问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？"><a href="#问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？" class="headerlink" title="问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？"></a>问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？</h4><p>答：马尔可夫决策过程是在马尔科夫奖励过程的基础上加上动作决策的过程，主要由状态，动作，状态转移矩阵，奖励函数+折扣因子组成:$(S,A,P,R,\gamma)$</p>
<p>主要区别为相比于MRP,MDP增加动作这个概念。而MRP与MDP之间的联系是可以将MDP对动作进行”marginization”我更倾向于将”边缘化“理解为”积分”，推导出MRP</p>
<script type="math/tex; mode=display">
\begin{aligned}
P^{\pi}\left(s^{\prime} \mid s\right) &=\sum_{a \in A} \pi(a \mid s) P\left(s^{\prime} \mid s, a\right) \\
R^{\pi}(s) &=\sum_{a \in A} \pi(a \mid s) R(s, a)
\end{aligned}</script><p>从图的角度来看如下（从周博磊老师的PPT上拷过来的）</p>
<p><img src="/2020/07/06/rl-questions/1.png" alt></p>
<h4 id="问：简要说一下MDP里面的价值函数及其推导关系？"><a href="#问：简要说一下MDP里面的价值函数及其推导关系？" class="headerlink" title="问：简要说一下MDP里面的价值函数及其推导关系？"></a>问：简要说一下MDP里面的价值函数及其推导关系？</h4><p>MDP里面的价值函数一共有两种分别是$V$和$Q$</p>
<p>其中$V$函数与$MRP$中的价值函数相同，</p>
<script type="math/tex; mode=display">
\begin{array}{l}
v^{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s\right] \\
q^{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s, A_{t}=a\right] \\
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a)
\end{array}</script><p>而第三个公式为对状态动作价值q函数的动作进行“积分”会得到v价值函数</p>
<h4 id="问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？"><a href="#问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？" class="headerlink" title="问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？"></a>问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？</h4><script type="math/tex; mode=display">
\begin{array}{c}
v^{\pi}(s)=E_{\pi}\left[R_{t+1}+\gamma v^{\pi}\left(s_{t+1}\right) \mid s_{t}=s\right] \\
q^{\pi}(s, a)=E_{\pi}\left[R_{t+1}+\gamma q^{\pi}\left(s_{t+1}, A_{t+1}\right) \mid s_{t}=s, A_{t}=a\right]
\end{array}</script><p>与贝尔曼方程的区别是增加了“期望”，这个期望指的是对policy中的动作进行求期望</p>
<script type="math/tex; mode=display">
\begin{array}{c}
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a) \\
q^{\pi}(s, a)=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right) \\

\end{array}</script><p>$V$与$Q$的关系如上所示，将上面第一行代入第二行或者第二行代入第一行如下所示(可以看出自举的意思)</p>
<p><img src="/2020/07/06/rl-questions/2.png" alt></p>
<script type="math/tex; mode=display">
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\right) \\</script><p><img src="/2020/07/06/rl-questions/3.png" alt></p>
<script type="math/tex; mode=display">
q^{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q^{\pi}\left(s^{\prime}, a^{\prime}\right)</script><h4 id="问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？"><a href="#问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？" class="headerlink" title="问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？"></a>问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？</h4><p>预测和控制的输入和输出是不一样的，预测功能的输入是$MDP+\pi$或者是$MRP$，而其输出是价值函数，目的是判断某一状态的价值是多少</p>
<p>控制功能的输入是MDP的5元组，其输出是最优价值函数或者最优策略，这部分实现的是功能为输出最优策略，为智能体做出动作决策。</p>
<p>解决：</p>
<p>可以通过动态规划和蒙特卡罗方法或者时序差分的方法来解决问题</p>
<h4 id="问：什么是动态规划？（动态规划适合解决什么样的问题？）"><a href="#问：什么是动态规划？（动态规划适合解决什么样的问题？）" class="headerlink" title="问：什么是动态规划？（动态规划适合解决什么样的问题？）"></a>问：什么是动态规划？（动态规划适合解决什么样的问题？）</h4><p>答：动态规划是一种求解决策过程最优化的方法，该问题具有如下两种性质</p>
<ul>
<li>问题具有最优子结构<ul>
<li>问题可以被分为多个子问题</li>
<li>一个最优策略的子策略也是最优的</li>
</ul>
</li>
<li>子问题具有重叠性<ul>
<li>子问题都重复多遍<ul>
<li>以斐波那契数列为例，fib(5)和fib(6)里都需要计算fib(4)</li>
</ul>
</li>
<li>子问题的解可以被存储或者重用<ul>
<li>还是以斐波那契数列为例，通过一个列表存储子问题的解，属于用空间换时间的想法</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>而之前的马尔可夫决策过程正好满足上述条件，贝尔曼等式可以将问题分为多个子问题，而价值函数就相当于“列表”的作用存储并重用子问题的解，下面的策略评估，策略迭代，价值迭代都算是动态规划大类里面，且这三种方法都知道状态转移矩阵，所以是model-based方法</p>
<h4 id="问：怎么解决policy-evaluation问题？"><a href="#问：怎么解决policy-evaluation问题？" class="headerlink" title="问：怎么解决policy evaluation问题？"></a>问：怎么解决policy evaluation问题？</h4><p>答：policy evaluation问题可以通过贝尔曼期望方程来解决，贝尔曼期望方程是通过$V(s)$和$V(s’)$来表达当前时刻状态和下一时刻状态之间的关系,在奖励函数被完全告知的前提下，可以通过迭代计算使得该公式收敛，当公式收敛时会得到一个比较合适的价值函数，这个价值函数就是对策略$\pi$的估值，MDP或者MRP之间的关系如下所示</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{t+1}(s)=& \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{t}\left(s^{\prime}\right)\right) \\
& v_{t+1}(s)=R^{\pi}(s)+\gamma P^{\pi}\left(s^{\prime} \mid s\right) v_{t}\left(s^{\prime}\right)
\end{aligned}</script><h4 id="问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？"><a href="#问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？" class="headerlink" title="问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？"></a>问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？</h4><p>最优价值函数是对于某个状态s，在所有的策略所对应价值函数$v^{\pi}(s)$中取得最大的价值函数</p>
<p>而最优策略是使得价值函数最大所对应的策略</p>
<script type="math/tex; mode=display">
\begin{array}{c}
v^{*}(s)=\max _{\pi} v^{\pi}(s) \\
\pi^{*}(s)=\arg \max _{\pi} v^{\pi}(s)
\end{array}</script><p>对于价值函数为q的表达式，可以通过极大化$q$函数来得到最优策略</p>
<script type="math/tex; mode=display">
\pi^{*}(a \mid s)=\left\{\begin{array}{ll}
1, & \text { if } a=\arg \max _{a \in A} q^{*}(s, a) \\
0, & \text { otherwise }
\end{array}\right.</script><p>且一般情况下，在有限马尔可夫过程里最优策略具有如下性质</p>
<ul>
<li>确定性</li>
<li>平稳性（无论是在t时刻还是t+n时刻，最优策略不会随着时间而改变）</li>
<li>不具有唯一性（有可能有两个动作的价值函数相等，那么最优策略不唯一）</li>
</ul>
<h4 id="问：怎么通过policy-iteration来优化策略？为什么优化后的策略一定比之前的策略要好？"><a href="#问：怎么通过policy-iteration来优化策略？为什么优化后的策略一定比之前的策略要好？" class="headerlink" title="问：怎么通过policy iteration来优化策略？为什么优化后的策略一定比之前的策略要好？"></a>问：怎么通过policy iteration来优化策略？为什么优化后的策略一定比之前的策略要好？</h4><p>答：policy iteration = policy evaluation+policy improvement</p>
<ul>
<li>首先对于给定的策略$\pi$和价值函数使用贝尔曼期望方程使得价值函数收敛<ul>
<li>$v_{i}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{i-1}\left(s^{\prime}\right)\right)$</li>
</ul>
</li>
<li>通过贪婪的方式，对价值函数取极大化来改进其策略<ul>
<li>$q^{\pi_{i}}(s, a) =R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi_{i}}\left(s^{\prime}\right)$</li>
<li>$pi_{i+1}(s) =\underset{a}{\arg \max } q^{\pi_{i}}(s, a)$</li>
</ul>
</li>
<li>得到新的策略$\pi’$重复1，2两步</li>
</ul>
<p><img src="/2020/07/06/rl-questions/4.png" alt></p>
<p>为什么优化后的策略一定比之前的策略要好？可通过如下公式推导得出</p>
<script type="math/tex; mode=display">
q^{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in \mathcal{A}} q^{\pi}(s, a) \geq q^{\pi}(s, \pi(s))=v^{\pi}(s) \\</script><script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s) & \leq q_{\pi}\left(s, \pi^{\prime}(s)\right) \\
&=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=\pi^{\prime}(s)\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1}, A_{t+1}=\pi^{\prime}\left(S_{t+1}\right)\right] \mid S_{t}=s]\right.\\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} v_{\pi}\left(S_{t+2}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} v_{\pi}\left(S_{t+3}\right) \mid S_{t}=s\right] \\
& \vdots \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots \mid S_{t}=s\right] \\
&=v_{\pi^{\prime}}(s)
\end{aligned}</script><p>而当策略收敛不再改变时，就已经达到最优策略（第一行），推导可得出满足<strong>贝尔曼最优方程</strong>（最后一行）</p>
<script type="math/tex; mode=display">
\begin{array}{c}
v^{*}(s)=\max _{a} q^{*}(s, a) \\
q^{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{*}\left(s^{\prime}\right) \\
v^{*}(s)=\max _{a} R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{*}\left(s^{\prime}\right) \\
q^{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} q^{*}\left(s^{\prime}, a^{\prime}\right)
\end{array}</script><h4 id="问：value-iteration与policy-iteration的区别？"><a href="#问：value-iteration与policy-iteration的区别？" class="headerlink" title="问：value iteration与policy iteration的区别？"></a>问：value iteration与policy iteration的区别？</h4><ul>
<li>value iteration 是<strong>直接</strong>使用贝尔曼最优方程来进行迭代        <ul>
<li>$v_{i+1}(s) \leftarrow \max _{a \in \mathcal{A}} R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{i}\left(s^{\prime}\right)$</li>
</ul>
</li>
<li>当价值函数收敛时可以推导得到最佳策略<ul>
<li>$\pi^{*}(s) \leftarrow \underset{a}{\arg \max } R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v_{e n d}\left(s^{\prime}\right)$</li>
</ul>
</li>
</ul>
<p><img src="/2020/07/06/rl-questions/6.png" alt></p>
<p>可以看出价值迭代方法没有像底下策略迭代一样需要分为两步，只是一个两侧的循环即可，在David silver课程中讲过价值迭代也可以看作单步的策略迭代</p>
<p><img src="/2020/07/06/rl-questions/5.png" alt></p>
<p>这两种实现“控制”方法的区别点在于</p>
<ul>
<li><p>直观来看，策略迭代分为两步，价值迭代只有一步</p>
</li>
<li><p>策略迭代中的策略评估部分需要收敛后才能根据收敛的价值函数进行策略提升，而价值迭代不需要等价值函数收敛，马上对价值函数取极大值，没有policy的参与</p>
</li>
<li>策略迭代使用的更新公式为<strong>贝尔曼期望方程</strong>，价值迭代使用的更新公式为<strong>贝尔曼最优方程</strong></li>
</ul>
<h4 id="问：MC和TD方法的好处是什么？现实情况下没有状态转移矩阵应该怎么办？"><a href="#问：MC和TD方法的好处是什么？现实情况下没有状态转移矩阵应该怎么办？" class="headerlink" title="问：MC和TD方法的好处是什么？现实情况下没有状态转移矩阵应该怎么办？"></a>问：MC和TD方法的好处是什么？现实情况下没有状态转移矩阵应该怎么办？</h4><p>答：动态规划中的价值估计和MC,TD之间的一个比较重要的区别是动态规划中并没有让智能体与环境进行交互，而MC,TD是智能体与环境交互才会得到真实交互序列，才会进一步得到对状态的估值。现实情况中，没有状态转移矩阵和奖励函数R还要得到状态估计值（可能是因为环境过于复杂，不能得到状态转移矩阵和所有的准确的奖励值）就不能用model-based的动态规划方法求解，可以使用蒙特卡罗方法或者是时序差分方法来计算价值函数。</p>
<h4 id="问：DP和MC之间的区别是什么？"><a href="#问：DP和MC之间的区别是什么？" class="headerlink" title="问：DP和MC之间的区别是什么？"></a>问：DP和MC之间的区别是什么？</h4><ul>
<li>价值函数的求法不同<ul>
<li>动态规划方法是使用自举的方式估计价值函数（最优子结构的性质）</li>
<li>MC采用均值法估计价值函数（蒙特卡罗方法的核心思想是大数定理，让智能体与环境进行多轮交互，对每轮游戏的每种状态都取其return值的平均值。而与环境交互次数越多，得到return的平均值越接近于真实值。）</li>
</ul>
</li>
<li>所需条件不同<ul>
<li>DP为model-based方法，需要知道状态转移矩阵和奖励函数</li>
<li>优点为model-free，不用状态转移矩阵和奖励函数</li>
</ul>
</li>
<li><p>游戏类型也有区分</p>
<ul>
<li>DP类游戏对终止状态的有无没有明确的规定</li>
<li>MC规定该类游戏必须是有<strong>终止状态</strong>的游戏。</li>
</ul>
</li>
<li><p>更新状态不同</p>
<ul>
<li>DP是更新所有状态（如果状态数量过多会导致更新速度缓慢）</li>
<li>MC更新的只有实际产生序列中的状态（不需要考虑其他未出现的状态，有利有弊，利为更新速度快，弊为还存在一部分没有更新到的状态）</li>
</ul>
</li>
</ul>
<h4 id="问：简要介绍一下TD-你觉得TD的优点是什么-（TD于MC的区别是什么？）"><a href="#问：简要介绍一下TD-你觉得TD的优点是什么-（TD于MC的区别是什么？）" class="headerlink" title="问：简要介绍一下TD,你觉得TD的优点是什么?（TD于MC的区别是什么？）"></a>问：简要介绍一下TD,你觉得TD的优点是什么?（TD于MC的区别是什么？）</h4><p>TD可以看作是蒙特卡洛法和动态规划方法的一个折中，其优点为</p>
<ul>
<li>计算价值函数的时间点不同<ul>
<li>MC需要等序列结束才能计算价值函数（偏差小，方差大）</li>
<li>TD不需要等序列结束只是向后走一步，得到一个target的估计值（偏差大，方差小）</li>
</ul>
</li>
<li>TD与MC对于终止状态的要求不一样<ul>
<li>TD不要求序列有终止状态</li>
<li>MC要求序列有终止状态</li>
</ul>
</li>
<li>对马尔可夫性的要求程度也不同<ul>
<li>MC对马尔可夫性质要求不高，即使该序列不符合MDP，也可以使用MC方法求价值函数</li>
<li>TD方法在马尔可夫环境中计算效率更高</li>
</ul>
</li>
<li>TD与MC一样，不需要知道状态转移矩阵和奖励函数就可以计算出价值函数</li>
<li>相比于MC中的target值为序列的奖励总return值，TD的target值不一样（是向后走一步的即时奖励+$\gamma*$后继状态的估计值），TD也是直接于环境进行交互，吸取DP的自举思想，通过下一时刻状态的价值函数求得当前状态的价值函数<ul>
<li>TD:$\begin{array}{c}<br>v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma v\left(S_{t+1}\right)-v\left(S_{t}\right)\right) \\<br>\end{array}$</li>
<li>MC:$v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{i, t}-v\left(S_{t}\right)\right)$</li>
</ul>
</li>
</ul>
<p>图示区别如下</p>
<p><img src="/2020/07/06/rl-questions/7.png" alt></p>
<p><img src="/2020/07/06/rl-questions/8.png" alt></p>
<p><img src="/2020/07/06/rl-questions/9.png" alt></p>
<h4 id="问：-epsilon-greedy-有什么作用？-为什么要用-epsilon-greedy-呢？"><a href="#问：-epsilon-greedy-有什么作用？-为什么要用-epsilon-greedy-呢？" class="headerlink" title="问：$\epsilon-greedy $ 有什么作用？(为什么要用$\epsilon-greedy $ 呢？)"></a>问：$\epsilon-greedy $ 有什么作用？(为什么要用$\epsilon-greedy $ 呢？)</h4><ul>
<li>$\epsilon-greedy $ 相比于直接greedy更好的体现了”探索“的思想，以$\epsilon$的概率随机选择动作，以$1-\epsilon$的概率选择贪婪策略，且探索因子$\epsilon $随着迭代次数的增加而减小</li>
</ul>
<script type="math/tex; mode=display">
\pi(a \mid s)=\left\{\begin{array}{ll}
\epsilon /|\mathcal{A}|+1-\epsilon & \text { if } a^{*}=\arg \max _{a \in \mathcal{A}} Q(s, a) \\
\epsilon /|\mathcal{A}| & \text { otherwise }
\end{array}\right.</script><p>如下推导确定，以$\epsilon -greedy$的方法进行探索出来的策略$\pi’$一定优于迭代之前的$\pi$</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a \in \mathcal{A}} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \\
&=\frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a \mid s)-\frac{\epsilon}{|\mathcal{A}|}}{1-\epsilon} q_{\pi}(s, a) \\
&=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a)=v_{\pi}(s)
\end{aligned}</script><p>我更倾向于如下理解（这样就可以与目标对应上）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi’}(s)=q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a \in \mathcal{A}} \pi^{\prime}(a | s) q_{\pi}(s, a) \\
&=\epsilon / m \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \max _{a \in \mathcal{A}} q_{\pi}(s, a) \\
& \geq \epsilon / m \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a | s)-\epsilon / m}{1-\epsilon} q_{\pi}(s, a) \\
&=\sum_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a)=q_{\pi}\left(s, \pi(s)\right)=v_{\pi}(s)
\end{aligned}</script><h4 id="问：Sarsa和Q-learning的异同点？"><a href="#问：Sarsa和Q-learning的异同点？" class="headerlink" title="问：Sarsa和Q-learning的异同点？"></a>问：Sarsa和Q-learning的异同点？</h4><p>答：</p>
<p><img src="/2020/07/06/rl-questions/10.png" alt></p>
<p><img src="/2020/07/06/rl-questions/11.png" alt>sarsa和q-learning的算法流程图如上所示</p>
<p>两算法的主要区别点是一个为<strong>on-policy</strong>算法，一个为<strong>off-policy</strong>算法。</p>
<ul>
<li>关于on-policy还是off-policy最重要的区别是算法<strong>所更新的策略和跟环境交互的策略是否相同</strong>，同为on，异为off。</li>
<li>且网上还有部分资料说区分on还是off是看是否使用了<strong>重要性采样</strong>（我在这里并不是十分认同这种说法，个人理解为增加重要性采样的权重值只是为了使得通过两个不同的策略计算的值差别缩小，换句话说，重要性采样是方法而非原因）</li>
<li>周老师的讲解是sarsa采取动作A和A’用的都是$\epsilon -greedy$所以是on-policy而q-learning算法分别采用$\epsilon-greedy$和$max$操作，所以q-learning是off-policy（没有十分理解这种说法，在此提出的疑问是如果将q-learning的选择动作$A$换成$greedy$的算法，难道q-learning就变成on-policy的了？或者将sarsa算法的第一步改为$greedy$就变成off-policy了？-&gt;这种说法跟别说不通啊，或者说暂时存疑~）</li>
<li>我比较认同的说法是，对于sarsa算法，在更新Q值需要计算的TD-target值的动作A’是从$\epsilon-greedy$方式得来的，而在q-learning算法中计算TD-target值是直接对所有的Q值取最大值。该层loop结束后，sarsa算法说到做到一定会执行<strong>刚才用于计算TD-target值</strong>的动作，而q-learning算法只是将”$a$”用于计算TD-target值，在新一轮loop会根据新的状态重新选择动作。</li>
</ul>
<h4 id="问：强化学习训练十分不稳定的原因有哪些？"><a href="#问：强化学习训练十分不稳定的原因有哪些？" class="headerlink" title="问：强化学习训练十分不稳定的原因有哪些？"></a>问：强化学习训练十分不稳定的原因有哪些？</h4><ul>
<li>首先问题出在函数拟合上，我们将状态输入到神经网络，输出状态的价值函数会产生很多误差</li>
<li>自举的模式中的TD-target值包含$v(s’)$，是含有一部分偏差的，会导致训练不稳定</li>
<li>在off-policy模式下训练智能体，可能会导致两个策略差异过大导致效果变差</li>
</ul>
<h4 id="问：蒙特卡罗，sarsa，q-learning的收敛问题？"><a href="#问：蒙特卡罗，sarsa，q-learning的收敛问题？" class="headerlink" title="问：蒙特卡罗，sarsa，q-learning的收敛问题？"></a>问：蒙特卡罗，sarsa，q-learning的收敛问题？</h4><p><img src="/2020/07/06/rl-questions/12.png" alt></p>
<p>备注：对号为可以收敛到全局最优解，（对号）是接近于全局最优解，叉号为不保证</p>
<h4 id="问：简要说一下deep-q-learning系列的创新点或者遇到什么问题，采用什么技术手法解决的？"><a href="#问：简要说一下deep-q-learning系列的创新点或者遇到什么问题，采用什么技术手法解决的？" class="headerlink" title="问：简要说一下deep-q-learning系列的创新点或者遇到什么问题，采用什么技术手法解决的？"></a>问：简要说一下deep-q-learning系列的创新点或者遇到什么问题，采用什么技术手法解决的？</h4><p>答：dqn的算法流程<img src="/2020/07/06/rl-questions/15.png" alt></p>
<ul>
<li><p>论文中使用神经网络拟合Q函数</p>
<ul>
<li>网络输入为4帧的游戏连续画面</li>
<li>输出为动作个数的q值</li>
</ul>
</li>
<li><p>强化学习中一直存在样本相关度过高的问题，即相邻的两个样本之间在时间上有一定的相关性，不服从独立同分布的假设</p>
<ul>
<li>解决办法：通过experience replay buffer的方法解决</li>
<li>将采集到的数据放到buffer中，每次训练从buffer中对数据进行采样，打破相关性</li>
</ul>
</li>
<li><p>deep-q-learning在13年的版本并没有使用额外固定一套网络来target值，会存在<strong>波动问题</strong></p>
<ul>
<li>问题描述，因为target的值参数为$w$，与要更新的参数相同，所以在每次更新过后都会导致Q函数（目标）发生一些变化，两者的相关性太强，不利于算法收敛<script type="math/tex; mode=display">
y_{j}=\left\{\begin{array}{ll}
R_{j} & \text {is_end}_{j} \text {is true} \\
R_{j}+\gamma \max _{a^{\prime}} Q\left(\phi\left(S_{j}^{\prime}\right), A_{j}^{\prime}, w\right) & \text {is_end}_{j} \text {is false}
\end{array}\right.</script></li>
</ul>
</li>
<li><p>解决方法（<strong>15年Nature</strong>）使用第二个网络（权重值在一段时间内不变，每隔一段时间复制正在学习的Q网络参数实现延时更新）来TD-target的目标值，从而增强训练的稳定性</p>
<script type="math/tex; mode=display">
  y_{j}=\left\{\begin{array}{ll}
  R_{j} & \text {is_end}_{j} \text {is true} \\
  R_{j}+\gamma \max _{a^{\prime}} Q\left(\phi\left(S_{j}^{\prime}\right), A_{j}^{\prime}, w'\right) & \text {is_end}_{j} \text {is false}
  \end{array}\right.</script></li>
<li><p>max导致的过度估计的问题</p>
<ul>
<li><p>问题描述：上面公式可以看出在计算目标值的时候使用的是max求极大值，但是一般情况下所谓的标答并不会达到极大值那么高，直接取max会导致过度估计的问题</p>
</li>
<li><p>解决方法：先在当前训练的网络（实时更新的网络$w$）中用$argmax$取得极大Q值所对应的动作，再将这个动作输入到参数为$w’$来计算$y_j$</p>
<script type="math/tex; mode=display">
y_{j}=R_{j}+\gamma Q^{\prime}\left(\phi\left(S_{j}^{\prime}\right), \arg \max _{a^{\prime}} Q\left(\phi\left(S_{j}^{\prime}\right), a, w\right), w^{\prime}\right)</script></li>
</ul>
</li>
<li><p>replay buffer在采样的时候对所有的样本一视同仁</p>
<ul>
<li><p>问题描述：buffer中的所有样本采出的概率相等，导致某些TD-error小的样本也会被多次采出来，对网络的更新影响较小，更希望让TD-error大的样本采出概率更大</p>
</li>
<li><p>解决方法：使用二叉树的结构存储样本（只有叶子节点存储样本，非叶子节点存的是其子节点的权重和）详情见<a href="https://www.cnblogs.com/pinard/p/9797695.html，要想在buffer中区的一个样本" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/9797695.html，要想在buffer中区的一个样本</a></p>
<ul>
<li><p>首先在所有权重值范围内（0，42）等概率取得一个数</p>
</li>
<li><p>该数字在哪个样本的区间范围内，就取得哪个样本（权重值根据TD-error计算得到，是正比于TD-error）</p>
</li>
<li><p>计算损失函数的时候需要乘以一个误差系数（系数越大，所占损失的权重越大）</p>
<script type="math/tex; mode=display">
\frac{1}{m} \sum_{j=1}^{m} w_{j}\left(y_{j}-Q\left(\phi\left(S_{j}\right), A_{j}, w\right)\right)^{2}</script></li>
<li><p>计算损失，通过神经网络梯度反向传播更新$w$参数后需要重新计算buffer中所有样本的TD-error:$\delta_j=y_j-Q(\phi(S_j),A_j,w)$，更新树结构所有节点的优先级$p_j=|\delta_j|$</p>
<ul>
<li>其中$w_j$是从$p_j$计算得来的<script type="math/tex; mode=display">
w_{j}=\frac{(N * P(j))^{-\beta}}{\max _{i}\left(w_{i}\right)}=\frac{(N * P(j))^{-\beta}}{\max _{i}\left((N * P(i))^{-\beta}\right)}=\frac{(P(j))^{-\beta}}{\max _{i}\left((P(i))^{-\beta}\right)}=\left(\frac{p_{j}}{\min _{i} P(i)}\right)^{-\beta}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>dueling DQN主要是从优化神经网络的结构来优化算法</p>
<ul>
<li><p>首先将原本的Q值分为两部分加和的形式<img src="/2020/07/06/rl-questions/13.png" alt></p>
</li>
<li><p>公式如下（其中$w$是$A$和$V$的公共部分，$\alpha$是$V$特有部分，$\beta$是$A$值特有部分）</p>
<script type="math/tex; mode=display">
Q(S, A, w, \alpha, \beta)=V(S, w, \alpha)+\left(A(S, A, w, \beta)-\frac{1}{\mathcal{A}} \sum_{a^{\prime} \in \mathcal{A}} A\left(S, a^{\prime}, w, \beta\right)\right)</script></li>
<li><p>通过这样分解不仅可以得到给定状态的q值，还可以得到具体的$V$值和$A$值，分开后有如下优势：”强化学习精要第八章-210“</p>
<ul>
<li>原本的输出值为|A|个取值为$(0~+∞)$的值，分开后变为训练一个$(0~+∞)$的Q值和$|A|$个均值为0，实际取值为$[-C,C]$的数值，对网络训练来说后者更<strong>容易</strong>，缩小值的范围后，去除多余的自由度，可以提高算法稳定性。</li>
<li>$V$值是所有Q值的平均值，所以在更新网络的时候会更新这个$V$值，相当于将所有动作的价值都进行调整，可以在更少的次数让更多的值进行更新<a href="https://zhuanlan.zhihu.com/p/110807201" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110807201</a></li>
<li>$V$函数可以很好的区分不同状态的影响，$A$函数可以很好的区分不同动作的影响</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="问：policy-based和value-based算法的区别是什么？"><a href="#问：policy-based和value-based算法的区别是什么？" class="headerlink" title="问：policy-based和value-based算法的区别是什么？"></a>问：policy-based和value-based算法的区别是什么？</h4><p>答：</p>
<ul>
<li>神经网络输出（或者说学的东西）不一样<ul>
<li>value-based算法输出的是各个动作的Q值（学的是价值函数）</li>
<li>policy-based输出的是动作的概率分布（不学价值函数，直接学策略）</li>
</ul>
</li>
<li>输出的策略不一致<ul>
<li>value-based算法最后得到的策略是确定性策略，给定一个状态后对Q值取极大，肯定输出确定性的动作</li>
<li>policy-based得到的是随机策略（以一定的概率采取某个动作，以另外的概率采取别的动作）</li>
</ul>
</li>
</ul>
<h4 id="问：policy-based的优缺点？"><a href="#问：policy-based的优缺点？" class="headerlink" title="问：policy-based的优缺点？"></a>问：policy-based的优缺点？</h4><p>答：</p>
<p>优点为</p>
<ul>
<li><p>无论数据的多少，我们都会得到一个策略函数（数据的多少决定是全局最优还是局部最优）</p>
</li>
<li><p>在高维的动作空间更有效（相比于value-based方法不需要输出非常多的动作个数的神经元，只是输出概率分布即可）</p>
</li>
<li><p>学得的是随机策略在某些情况下会比确定性策略更有效（可以举迷宫的例子）</p>
</li>
<li><p>从表达形式可以看出策略梯度的核心思想是求得目标函数对于策略$\theta$的导数，而目标函数可以写为关于状态的价值函数，一个episode的总奖励值等，通过梯度上升的方法迭代求出目标函数的极大值。（那如果目标函数不可导怎么办？）</p>
<ul>
<li><p>可以用cross entropy method或者finite difference的方法求解</p>
<ul>
<li><p><img src="/2020/07/06/rl-questions/14.png" alt></p>
</li>
<li><p>简而言之意思就是先从最原始的策略的概率分布$\theta$采样出100组参数，用100组参数与环境交互得到100个目标函数，根据这100个目标函数排序得到前10组参数，利用10组参数对原先的策略参数$\theta$进行更新，后面就反复循环</p>
</li>
<li><p>finite difference的目的是使用如下方法去求得梯度的近似解</p>
<script type="math/tex; mode=display">
\frac{\partial J(\theta)}{\partial \theta_{k}} \approx \frac{J\left(\theta+\epsilon u_{k}\right)-J(\theta)}{\epsilon}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>收敛到局部最优解</li>
<li>方差较大（reward波动非常大）</li>
</ul>
<h4 id="问：策略梯度的score-function是怎么来的？有什么作用和几何意义？"><a href="#问：策略梯度的score-function是怎么来的？有什么作用和几何意义？" class="headerlink" title="问：策略梯度的score function是怎么来的？有什么作用和几何意义？"></a>问：策略梯度的score function是怎么来的？有什么作用和几何意义？</h4><p>答：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} \pi_{\theta}(s, a) &=\pi_{\theta}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(s, a)}{\pi_{\theta}(s, a)} \\
&=\pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)
\end{aligned}</script><p>其中$\nabla_{\theta} \log \pi_{\theta}(s, a)$是score function</p>
<p>几何意义后面补上：</p>
<h4 id="问：怎么推出来最基本的策略梯度算法？"><a href="#问：怎么推出来最基本的策略梯度算法？" class="headerlink" title="问：怎么推出来最基本的策略梯度算法？"></a>问：怎么推出来最基本的策略梯度算法？</h4><p>答：<a href="https://www.bilibili.com/video/BV1fZ4y1x7mp?from=search&amp;seid=18213355905709582432在下面公式推导中，记$d(s)$为状态$s$的初始出现概率" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1fZ4y1x7mp?from=search&amp;seid=18213355905709582432在下面公式推导中，记$d(s)$为状态$s$的初始出现概率</a></p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\theta) &=\mathbb{E}_{\pi_{\theta}}[r] \\
&=\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(s, a) r \\
\nabla_{\theta} J(\theta) &=\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a) r \\
&=\mathbb{E}_{\pi_{\theta}}\left[r \nabla_{\theta} \log \pi_{\theta}(s, a)\right]
\end{aligned}</script><p>上式最后一行可以得到单步马尔可夫过程的期望奖励对策略$\theta$的梯度值</p>
<p>类似的对多步马尔可夫过程进行如下表达</p>
<script type="math/tex; mode=display">
J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T} R\left(s_{t}, a_{t}\right)\right]=\sum_{\tau} P(\tau ; \theta) R(\tau)</script><p>其中$\tau$是在$\pi_\theta$策略下与环境交互得到的一系列交互轨迹，而$R(\tau)$是该条轨迹得到的总回报值$R(\tau)=\sum_{t=0}^{T}R(s_t,a_t)$,且$P(\tau ; \theta)$是该轨迹的累乘概率值</p>
<script type="math/tex; mode=display">
P(\tau ; \theta)=\mu\left(s_{0}\right) \prod_{t=0}^{T-1} \pi_{\theta}\left(a_{t} \mid s_{t}\right) p\left(s_{t+1} \mid s_{t}, a_{t}\right)</script><p>推导如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} J(\theta) &=\nabla_{\theta} \sum_{\tau} P(\tau ; \theta) R(\tau) \\
&=\sum_{\tau} \nabla_{\theta} P(\tau ; \theta) R(\tau) \\
&=\sum_{\tau} \frac{P(\tau ; \theta)}{P(\tau ; \theta)} \nabla_{\theta} P(\tau ; \theta) R(\tau) \\
&=\sum_{\tau} P(\tau ; \theta) R(\tau) \frac{\nabla_{\theta} P(\tau ; \theta)}{P(\tau ; \theta)} \\
&=\sum_{\tau} P(\tau ; \theta) R(\tau) \nabla_{\theta} \log P(\tau ; \theta)
\end{aligned}</script><p>而关于上面最后一行还有两个操作才能得到PPT上的公式推导：</p>
<ul>
<li>第一项$P(\tau ; \theta)$可以使用MC的方法进行求解</li>
</ul>
<script type="math/tex; mode=display">
\nabla_{\theta} J(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} R\left(\tau_{i}\right) \nabla_{\theta} \log P\left(\tau_{i} ; \theta\right)</script><ul>
<li>对$\nabla_{\theta} \log P(\tau ; \theta)$进行分解<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} \log P(\tau ; \theta)=& \nabla_{\theta} \log \left[\mu\left(s_{0}\right) \prod_{t=0}^{T-1} \pi_{\theta}\left(a_{t} \mid s_{t}\right) p\left(s_{t+1} \mid s_{t}, a_{t}\right)\right] \\
&=\nabla_{\theta}\left[\log \mu\left(s_{0}\right)+\sum_{t=0}^{T-1} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)+\log p\left(s_{t+1} \mid s_{t}, a_{t}\right)\right] \\
&=\sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)
\end{aligned}</script>最后得到如下公式与第十页形式统一<script type="math/tex; mode=display">
\nabla_{\theta} J(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)
R\left(\tau_{i}\right)</script></li>
</ul>
<h4 id="问：AC算法有哪些不同形式？为什么critic那里也需要计算TD-error？"><a href="#问：AC算法有哪些不同形式？为什么critic那里也需要计算TD-error？" class="headerlink" title="问：AC算法有哪些不同形式？为什么critic那里也需要计算TD-error？"></a>问：AC算法有哪些不同形式？为什么critic那里也需要计算TD-error？</h4><p>答：</p>
<script type="math/tex; mode=display">
\begin{aligned}\nabla_{\theta} J(\theta) &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) G_{t}\right]-\text { REINFORCE } \\&=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q^{w}(s, a)\right]-\text { Q Actor-Critic } \\&=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) A^{\mathrm{w}}(s, a)\right]-\text { Advantage Actor-Critic } \\&=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \delta\right]-\text { TD Actor-Critic }\end{aligned}</script><p><img src="/2020/07/06/rl-questions/D:/rockblog/source/rockblog/source/_posts/Atari-a3c/pic7.png" alt></p>
<script type="math/tex; mode=display">
\mathbf{w} \leftarrow \mathbf{w}+\alpha^{\mathbf{w}} \delta \nabla \hat{v}(S, \mathbf{w})</script><p>答：该公式最早是从sutton书中201页的Stochastic-gradient and Semi-gradient Methods 讲到的，在这里w是critic的网络参数，那么其目的是使得下式中的$\hat{v}(S_{t},w_t)$越接近$v_\pi(S_t)$越好（在这里$v_\pi(S_t)$是所谓的“标答”），也就是$v_{\pi}(S_{t})-\hat{v}(S_{t})$越小越好，所以将$v_{\pi}(S_{t})-\hat{v}(S_{t})$对w求梯度，在更新w时采用梯度下降的方法如下式所示，而与sutton书中不同的地方有两点，其一是学习率用指数次方的形式去表示这样可以达到学习率递减的效果，其二是将标答换为<script type="math/tex">R+\gamma \hat{v}\left(S^{\prime}, \mathbf{w}\right)</script> 即以实际走出一步得到reward奖励+一步之后的折扣评估值作为标答，即可推出w的更新公式，</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1} & \doteq \mathbf{w}_{t}-\frac{1}{2} \alpha \nabla\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right]^{2} \\
&=\mathbf{w}_{t}+\alpha\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
\end{aligned}</script><p>且在DQN里面也有相关体现<img src="/2020/07/06/rl-questions/D:/rockblog/source/rockblog/source/_posts/Atari-a3c/pic12.png" alt></p>
<h4 id="问：A3C算法流程？"><a href="#问：A3C算法流程？" class="headerlink" title="问：A3C算法流程？"></a>问：A3C算法流程？</h4><p><img src="/2020/07/06/rl-questions/17.png" alt></p>
<p>A3C算法相比于之前的DQN算法的新的技术点为</p>
<ul>
<li>并行的交互采样和训练，同时启用N个线程，Agent将在N个进程中同时进行交互,收集到样本</li>
<li>每一个进程独立完成训练并得到参数更新量，异步更新到全局模型中</li>
<li>下一次训练时，多线程的模型参数将和全局参数完成同步</li>
<li>计算所得结果不同<ul>
<li>DQN只会得到Q价值函数</li>
<li>而A3C会得到critic价值函数和代表策略policy的actor网络</li>
</ul>
</li>
<li></li>
</ul>
<p><img src="/2020/07/06/rl-questions/16.png" alt></p>
<h4 id="问：A2C算法流程？"><a href="#问：A2C算法流程？" class="headerlink" title="问：A2C算法流程？"></a>问：A2C算法流程？</h4><p>工作流程</p>
<ul>
<li>开启多个线程(Worker)，从Global Network同步最新的网络参数；</li>
<li>每个Worker独立地进行采样；</li>
<li>当数据总量达到mini-batch size时，全部停止采样；</li>
<li>Global Network根据mini-batch的数据统一训练学习；</li>
<li>每个Worker更新Global Network的参数<ul>
<li>重复2~5。</li>
</ul>
</li>
</ul>
<p>A2C相比于A3C的区别点？</p>
<ul>
<li>首先是更新方式不同<ul>
<li>A3C是异步更新，主网络接收子网络传来的梯度信息，利用该梯度信息更新参数再将新参数传给自网络，</li>
<li>A2C是收集多个环境传来的s,a,r,s’ 在主网络中进行状态的价值评估，在主网络中利用得到的梯度更新参数</li>
</ul>
</li>
<li>其次是使用硬件资源的不同<ul>
<li>A3C是为没有GPU环境设计的算法，摆脱对显卡的依赖还能获得不错的效果</li>
<li>A2C可以使用GPU进行训练，因此加快了训练的速度，因为有了显卡的加持，更适合于batch较大或者网络结构较为复杂的情况（<strong>感觉</strong>脱离硬件资源，单说收敛速度是不合适的）</li>
</ul>
</li>
</ul>
<h4 id="问：PPO算法有什么改进点？PPO算法流程？"><a href="#问：PPO算法有什么改进点？PPO算法流程？" class="headerlink" title="问：PPO算法有什么改进点？PPO算法流程？"></a>问：PPO算法有什么改进点？PPO算法流程？</h4><p>答：PPO算法相当于在TRPO的基础上改进过来的，相比于之前的算法有如下改进点</p>
<ul>
<li>从on-policy转换为off-policy，目的是增加数据采样效率（提高数据利用率）</li>
<li>使用重要性采样的方法为得到的值乘上一个新策略与旧策略的概率比值，使用这个修正项的目的是减小在off-policy前提下所得值的偏差(提高稳定性)</li>
<li>增加KL散度的约束，KL散度越大，两个概率分布越相似（PPO1）</li>
<li>PPPO2:<ul>
<li><img src="/2020/07/06/rl-questions/18.png" alt></li>
</ul>
</li>
</ul>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1415500736@qq.com </span>
    </div>
</article>


<p>
    <a href="javascript:void(0)" class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>RL_questions</p>
    
    <p><span class="copy-title">本文作者:</span><a href="javascript:void(0)" title="Rock">Rock</a></p>
    <p><span class="copy-title">发布时间:</span>2020-07-06, 21:25:01</p>
    <p><span class="copy-title">最后更新:</span>2020-10-27, 21:00:15</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2020/07/06/rl-questions/" title="RL_questions">http://rock-blog.top/2020/07/06/rl-questions/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: 'e78b4c19bc08850d88df',
            clientSecret: '308b55a6d580ee7a819af0f950b3188be697ae29',
            repo: 'guobaoyo.github.io',
            owner: 'guobaoyo',
            admin: ['guobaoyo'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js" value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">

    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2016-2019 Rock&#39;s  blog</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class id="rocket" href="javascript:void(0)"></a>
    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close" href="javascript:void(0)" onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1"></script>

<script src="/js/script.js?v=1.0.1"></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#三省吾身','#AI','#数学','#深度学习','#CV','#python','#编程','#强化学习','#技术小节','#leetcode','#技术小结','#go','#组会报告','#考研','#NLP',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #c1bfc1;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.5;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
