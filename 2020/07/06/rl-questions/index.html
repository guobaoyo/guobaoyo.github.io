<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <title>RL_questions | Rock-Blog</title>
  <meta name="keywords" content=" AI , 深度学习 , 强化学习 ">
  <meta name="description" content="RL_questions | Rock-Blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="剑指 Offer 06. 从尾到头打印链表">
<meta name="keywords" content="python,技术小结,leetcode">
<meta property="og:type" content="article">
<meta property="og:title" content="剑指offer06">
<meta property="og:url" content="http://rock-blog.top/2020/07/07/sword-offer06/index.html">
<meta property="og:site_name" content="Rock-Blog">
<meta property="og:description" content="剑指 Offer 06. 从尾到头打印链表">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://rock-blog.top/2020/07/07/sword-offer06/image-20200708065457507.png">
<meta property="og:updated_time" content="2020-07-07T23:27:56.976Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="剑指offer06">
<meta name="twitter:description" content="剑指 Offer 06. 从尾到头打印链表">
<meta name="twitter:image" content="http://rock-blog.top/2020/07/07/sword-offer06/image-20200708065457507.png">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>

<script src="/js/iconfont.js?v=1.0.1"></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="true">
  <input class="theme_blog_path" value>
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg">
</a>
<div class="author">
    <span>Rock</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/guobaoyo" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"/>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:1415500736@qq.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"/>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=1415500736&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"/>
                </svg>
            
        </a>
        
    
        
        <a title="neteasemusic" href="https://music.163.com/#/user/home?id=280020740" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-neteasemusic"/>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(33)</small></div></li>
    
        
            
            <li><div data-rel="强化学习">强化学习<small>(6)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="三省吾身">三省吾身<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="AI">AI<small>(7)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="编程">编程<small>(11)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数学">数学<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="组会报告">组会报告<small>(1)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    
    
    </div>
    <div><a class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="33">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
            <li><a target="_blank" href="http://zivblog.top">王金锋</a></li>
            
            <li><a target="_blank" href="http://yearing1017.cn/">进哥</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off" id="local-search-input">
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none">
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">编程</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">AI</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">数学</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">三省吾身</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">深度学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">强化学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">CV</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">python</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">go</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">技术小结</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">leetcode</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">考研</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">NLP</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">组会报告</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a class="强化学习 " href="/2020/06/08/A-algorithm/" data-tag="编程,AI,数学" data-author>
            <span class="post-title" title="A*_algorithm">A*_algorithm</span>
            <span class="post-date" title="2020-06-08 21:25:01">2020/06/08</span>
        </a>
        
        <a class="三省吾身 " href="/2019/07/08/20190708日记/" data-tag="AI,数学,三省吾身" data-author>
            <span class="post-title" title="20190708日记">20190708日记</span>
            <span class="post-date" title="2019-07-08 19:43:26">2019/07/08</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/06/Atari-a2c/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A2c&amp;ppo">Aatri_A2c&amp;ppo</span>
            <span class="post-date" title="2020-07-06 10:16:15">2020/07/06</span>
        </a>
        
        <a class="AI " href="/2019/05/08/DLwithPython/" data-tag="深度学习,CV,python" data-author>
            <span class="post-title" title="DeepLearning with Python">DeepLearning with Python</span>
            <span class="post-date" title="2019-05-08 17:54:01">2019/05/08</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/29/DLwords/" data-tag="AI,深度学习" data-author>
            <span class="post-title" title="DLwords">DLwords</span>
            <span class="post-date" title="2019-04-29 19:17:39">2019/04/29</span>
        </a>
        
        <a class="强化学习 " href="/2020/01/26/RLTF/" data-tag="AI,数学,强化学习" data-author>
            <span class="post-title" title="RLTF">RLTF</span>
            <span class="post-date" title="2020-01-26 19:21:06">2020/01/26</span>
        </a>
        
        <a class="AI " href="/2020/07/12/atari-env-note/" data-tag="编程,深度学习,强化学习" data-author>
            <span class="post-title" title="Atari游戏环境笔记">Atari游戏环境笔记</span>
            <span class="post-date" title="2020-07-12 10:00:12">2020/07/12</span>
        </a>
        
        <a class="AI " href="/2019/09/14/deep-reinforcement-learning/" data-tag="数学,深度学习,强化学习" data-author>
            <span class="post-title" title="deep-reinforcement-learning">deep-reinforcement-learning</span>
            <span class="post-date" title="2019-09-14 10:00:12">2019/09/14</span>
        </a>
        
        <a class="编程 " href="/2020/01/31/leetcode/" data-tag="编程,数学,python" data-author>
            <span class="post-title" title="leetcode">leetcode</span>
            <span class="post-date" title="2020-01-31 10:14:45">2020/01/31</span>
        </a>
        
        <a class="强化学习 " href="/2019/06/22/Atari-a3c/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A3c">Aatri_A3c</span>
            <span class="post-date" title="2019-06-22 21:25:01">2019/06/22</span>
        </a>
        
        <a class href="/2020/06/05/lgb-note/" data-tag="编程,AI,python" data-author>
            <span class="post-title" title="lgb_note">lgb_note</span>
            <span class="post-date" title="2020-06-05 21:25:01">2020/06/05</span>
        </a>
        
        <a class="编程 " href="/2019/07/23/go-http搭建服务器知识点儿/" data-tag="编程,go" data-author>
            <span class="post-title" title="go-http搭建服务器知识点儿">go-http搭建服务器知识点儿</span>
            <span class="post-date" title="2019-07-23 11:29:08">2019/07/23</span>
        </a>
        
        <a class="数学 " href="/2019/04/25/math/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="线性代数补充笔记">线性代数补充笔记</span>
            <span class="post-date" title="2019-04-25 21:25:01">2019/04/25</span>
        </a>
        
        <a class="强化学习 " href="/2020/07/06/rl-questions/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="RL_questions">RL_questions</span>
            <span class="post-date" title="2020-07-06 21:25:01">2020/07/06</span>
        </a>
        
        <a class="AI " href="/2020/06/02/pytorch-note/" data-tag="AI,python,技术小结" data-author>
            <span class="post-title" title="pytorch_note">pytorch_note</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="编程 " href="/2020/07/04/sword-offer03/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer03">剑指offer03</span>
            <span class="post-date" title="2020-07-04 20:19:39">2020/07/04</span>
        </a>
        
        <a class="编程 " href="/2020/07/06/sword-offer05/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer05">剑指offer05</span>
            <span class="post-date" title="2020-07-06 20:19:39">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/06/sword-offer04/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer04">剑指offer04</span>
            <span class="post-date" title="2020-07-06 20:19:39">2020/07/06</span>
        </a>
        
        <a class="编程 " href="/2020/07/10/sword-offer10/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer10">剑指offer10</span>
            <span class="post-date" title="2020-07-10 20:19:39">2020/07/10</span>
        </a>
        
        <a class href="/2020/07/13/sword-offer11/" data-tag data-author>
            <span class="post-title" title="sword_offer11">sword_offer11</span>
            <span class="post-date" title="2020-07-13 07:22:46">2020/07/13</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/11/tianshou-a2c-note/" data-tag="编程,AI,强化学习" data-author>
            <span class="post-title" title="tianshou平台源码阅读笔记">tianshou平台源码阅读笔记</span>
            <span class="post-date" title="2020-06-11 21:25:01">2020/06/11</span>
        </a>
        
        <a class="编程 " href="/2019/04/29/使用Python实现excel中固定数据排序且改名至另一个文件夹/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="基于python实现excel中读取文件目录+相应数据排序+将文件重命名">基于python实现excel中读取文件目录+相应数据排序+将文件重命名</span>
            <span class="post-date" title="2019-04-29 19:30:06">2019/04/29</span>
        </a>
        
        <a class="编程 " href="/2020/07/09/sword-offer09/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer09">剑指offer09</span>
            <span class="post-date" title="2020-07-09 20:19:39">2020/07/09</span>
        </a>
        
        <a class="编程 " href="/2020/07/08/sword-offer07/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer07">剑指offer07</span>
            <span class="post-date" title="2020-07-08 20:19:39">2020/07/08</span>
        </a>
        
        <a class="数学 " href="/2019/10/24/数值计算与凸优化补充笔记/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="数值计算与凸优化补充笔记">数值计算与凸优化补充笔记</span>
            <span class="post-date" title="2019-10-24 16:14:54">2019/10/24</span>
        </a>
        
        <a class="三省吾身 " href="/2019/11/03/智源大会听报告笔记/" data-tag="AI,三省吾身" data-author>
            <span class="post-title" title="智源大会听学术报告笔记">智源大会听学术报告笔记</span>
            <span class="post-date" title="2019-11-03 13:24:12">2019/11/03</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/15/考研历程反思总结/" data-tag="三省吾身,考研" data-author>
            <span class="post-title" title="考研历程反思总结">考研历程反思总结</span>
            <span class="post-date" title="2019-04-15 21:25:01">2019/04/15</span>
        </a>
        
        <a class="AI " href="/2019/07/21/计算机视觉存疑解答记录/" data-tag="AI,数学,CV" data-author>
            <span class="post-title" title="计算机视觉存疑解答记录">计算机视觉存疑解答记录</span>
            <span class="post-date" title="2019-07-21 13:04:25">2019/07/21</span>
        </a>
        
        <a class="编程 " href="/2019/09/05/物体检测打标小脚本-获取当前图片中鼠标位置/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="物体检测打标小脚本-获取当前图片中鼠标位置">物体检测打标小脚本-获取当前图片中鼠标位置</span>
            <span class="post-date" title="2019-09-05 15:30:36">2019/09/05</span>
        </a>
        
        <a class="AI " href="/2019/07/29/达观杯比赛记录/" data-tag="AI,三省吾身,NLP" data-author>
            <span class="post-title" title="达观杯比赛记录">达观杯比赛记录</span>
            <span class="post-date" title="2019-07-29 20:19:39">2019/07/29</span>
        </a>
        
        <a class="组会报告 " href="/2019/09/26/组会报告-0930-QLearning/" data-tag="深度学习,强化学习,组会报告" data-author>
            <span class="post-title" title="组会报告-0930-QLearning">组会报告-0930-QLearning</span>
            <span class="post-date" title="2019-09-26 15:15:04">2019/09/26</span>
        </a>
        
        <a class="编程 " href="/2020/07/07/sword-offer06/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer06">剑指offer06</span>
            <span class="post-date" title="2020-07-07 20:19:39">2020/07/07</span>
        </a>
        
        <a class="AI " href="/2020/06/02/强化学习在滴滴网约车的应用记录/" data-tag="AI,强化学习,组会报告" data-author>
            <span class="post-title" title="强化学习在滴滴网约车的应用笔记">强化学习在滴滴网约车的应用笔记</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-rl-questions" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">RL_questions</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a href="javascript:" data-rel="强化学习">强化学习</a>
            
        </span>
        
        
        <span class="tag">
            
            <a href="javascript:" class="color3">AI</a>
            
            <a href="javascript:" class="color5">深度学习</a>
            
            <a href="javascript:" class="color5">强化学习</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title="更新时间: 2020-07-12 20:26:58">2020-07-06 21:25</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RL概括与基础"><span class="toc-text">RL概括与基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是强化学习？"><span class="toc-text">问：什么是强化学习？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：那你再详细说说强化学习和监督学习的区别点？"><span class="toc-text">问：那你再详细说说强化学习和监督学习的区别点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：为什么研究强化学习？"><span class="toc-text">问：为什么研究强化学习？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：强化学习的组成成分？"><span class="toc-text">问：强化学习的组成成分？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：强化学习分为哪些种类？"><span class="toc-text">问：强化学习分为哪些种类？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是马尔可夫过程（马尔可夫链）？"><span class="toc-text">问：什么是马尔可夫过程（马尔可夫链）？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是马尔可夫奖励过程？"><span class="toc-text">问：什么是马尔可夫奖励过程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：折扣因子的存在意义是什么？"><span class="toc-text">问：折扣因子的存在意义是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？"><span class="toc-text">问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？"><span class="toc-text">问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：简要说一下MDP里面的价值函数及其推导关系？"><span class="toc-text">问：简要说一下MDP里面的价值函数及其推导关系？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？"><span class="toc-text">问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？"><span class="toc-text">问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：什么是动态规划？（动态规划适合解决什么样的问题？）"><span class="toc-text">问：什么是动态规划？（动态规划适合解决什么样的问题？）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么解决policy-evaluation问题？"><span class="toc-text">问：怎么解决policy evaluation问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？"><span class="toc-text">问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：怎么通过policy-iteration来优化策略？为什么优化后的策略一定比之前的策略要好？"><span class="toc-text">问：怎么通过policy iteration来优化策略？为什么优化后的策略一定比之前的策略要好？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：value-iteration与policy-iteration的区别？"><span class="toc-text">问：value iteration与policy iteration的区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：MC和TD方法的好处是什么？-现实情况下没有状态转移矩阵应该怎么办？"><span class="toc-text">问：MC和TD方法的好处是什么？/现实情况下没有状态转移矩阵应该怎么办？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：DP和MC之间的区别是什么？"><span class="toc-text">问：DP和MC之间的区别是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：简要介绍一下TD-你觉得TD的优点是什么-（TD于MC的区别是什么？）"><span class="toc-text">问：简要介绍一下TD,你觉得TD的优点是什么?（TD于MC的区别是什么？）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：-epsilon-greedy-有什么作用？-为什么要用-epsilon-greedy-呢？"><span class="toc-text">问：$\epsilon-greedy $ 有什么作用？(为什么要用$\epsilon-greedy $ 呢？)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：Sarsa和Q-learning的异同点？"><span class="toc-text">问：Sarsa和Q-learning的异同点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：强化学习训练十分不稳定的原因有哪些？"><span class="toc-text">问：强化学习训练十分不稳定的原因有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#问：蒙特卡罗，sarsa，q-learning的收敛问题？"><span class="toc-text">问：蒙特卡罗，sarsa，q-learning的收敛问题？</span></a></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>主要是看周博磊老师的强化学习视频<a href="https://space.bilibili.com/511221970/channel/detail?cid=105354" target="_blank" rel="noopener">https://space.bilibili.com/511221970/channel/detail?cid=105354</a></p>
<p>再次总结的值得思考的问题（或许也是<strong>面试</strong>经常会问的问题）</p>
<a id="more"></a>
<h2 id="RL概括与基础"><a href="#RL概括与基础" class="headerlink" title="RL概括与基础"></a>RL概括与基础</h2><h4 id="问：什么是强化学习？"><a href="#问：什么是强化学习？" class="headerlink" title="问：什么是强化学习？"></a>问：什么是强化学习？</h4><p>答：这个问题需要与其他同级概念相互比较才会好理解，我理解的强化学习是一机器学习中的一部分，机器学习的学习范式有监督学习，无监督学习，强化学习，其中监督学习有明确的监督信号也就是所谓的标答，无监督学习没有标答主要是想从内部挖掘数据的关联性，而强化学习与监督学习与无监督学习的不同点是强化学习中<strong>没有所谓的标准答案</strong>，有的是智能体与环境这两大组成部分，智能体根据环境的观测状态值做出动作选择，动作输入到环境后环境会改变状态，随着时间往后推移，智能体与环境是在一直交互的，其核心目的是使得强化学习中的智能体在一个<strong>不确定环境</strong>下能够学习到<strong>最大化累计奖励</strong>的<strong>策略</strong>。</p>
<h4 id="问：那你再详细说说强化学习和监督学习的区别点？"><a href="#问：那你再详细说说强化学习和监督学习的区别点？" class="headerlink" title="问：那你再详细说说强化学习和监督学习的区别点？"></a>问：那你再详细说说强化学习和监督学习的区别点？</h4><p>答：在监督学习中，每个数据样本都有一个标准答案，且把这些样本输入到网络时有两个特点</p>
<ul>
<li>这些数据都是符合独立同分布条件</li>
<li>标答也给到网络去计算loss</li>
</ul>
<p>而训练强化学习智能体时，训练数据是一个交互过程（一个时间序列）</p>
<ul>
<li>智能体所观测到的状态不符合独立同分布条件，上一帧和下一帧是十分相关的</li>
<li>在训练的时候不会告诉智能体去做哪个动作，智能体需要自己去与环境进行交互，在最大化累计奖励的过程中去寻找最优策略</li>
<li>奖励（或者说反馈）具有延迟性，也就是智能体当前时刻做出了个动作并不会马上获得奖励，有可能在多步之后再获得奖励，因此强化学习的训练也会更加困难</li>
<li>基于第二点，在强化学习中也不能单凭奖励值的大小去判断这个动作决策的好坏，因为在强化学习中还有个比较重要的概念就是探索，智能体有可能会舍弃掉即时奖励比较大的动作，而去选择即时奖励较小的动作，其根本原因还是第二点虽然当前这个动作的即时奖励较小，但是可能在后面所带来的延迟奖励会很大</li>
</ul>
<h4 id="问：为什么研究强化学习？"><a href="#问：为什么研究强化学习？" class="headerlink" title="问：为什么研究强化学习？"></a>问：为什么研究强化学习？</h4><p>答：从相互比较的角度来看，监督学习中图像分类任务中的标答都是人给定的标准答案，所获得效果最好就是让训练出来的模型无限逼近于人类的图像分类能力。 而强化学习的这种学习范式是让智能体在环境中自己探索，所以在某些环境下有可能会达到超越人类的结果。</p>
<h4 id="问：强化学习的组成成分？"><a href="#问：强化学习的组成成分？" class="headerlink" title="问：强化学习的组成成分？"></a>问：强化学习的组成成分？</h4><p>答：感觉可以答分为两部分，从环境加智能体出发（也就是1+2为智能体，3为环境）</p>
<ul>
<li>决策函数-policy，也就是智能体<ul>
<li><strong>随机</strong>策略函数$\pi(a|s)=P[A_t = a|S_t=s]$ </li>
<li><strong>确定</strong>性策略函数$a^*=argmax_a\pi(a|s)$ </li>
</ul>
</li>
<li>价值函数，是对当前状态或者（状态+动作）的估值</li>
</ul>
<script type="math/tex; mode=display">
v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right] \  \ all \ of \ \ s∈S</script><script type="math/tex; mode=display">
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right]</script><ul>
<li>model-也就是模型，这里虽然叫做模型但是可以理解为环境，该部分主要包含两个方面，其一是状态转移概率是状态s到状态s’的概率<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{P}_{s s^{\prime}}^{a} &=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right] \\
\end{aligned}</script>以及奖励函数（当前状态采取某一动作后会得到多少奖励）<script type="math/tex; mode=display">
\mathcal{R}_{s}^{a} =\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]</script></li>
</ul>
<h4 id="问：强化学习分为哪些种类？"><a href="#问：强化学习分为哪些种类？" class="headerlink" title="问：强化学习分为哪些种类？"></a>问：强化学习分为哪些种类？</h4><p>答：可以从两个角度来分类，</p>
<ul>
<li>第一个角度：从智能体的学习情况分为三种，分别为<ul>
<li>value-based    <ul>
<li>学习值函数</li>
<li>通过比较值函数来生成策略</li>
</ul>
</li>
<li>policy-based<ul>
<li>不学习值函数</li>
<li>输出的是一个策略$\pi$，代表各个动作的概率值</li>
</ul>
</li>
<li>Actor-Critic<ul>
<li>输出两部分为值函数与策略</li>
</ul>
</li>
</ul>
</li>
<li>第二个角度：是否学习环境来分<ul>
<li>model-based：学习了环境的模型，知道状态转移概率</li>
<li>model-free：不知道状态转移概率，通过学习value-function或者policy-function来生成策略</li>
</ul>
</li>
</ul>
<h4 id="问：什么是马尔可夫过程（马尔可夫链）？"><a href="#问：什么是马尔可夫过程（马尔可夫链）？" class="headerlink" title="问：什么是马尔可夫过程（马尔可夫链）？"></a>问：什么是马尔可夫过程（马尔可夫链）？</h4><p>答：马尔可夫过程是一个具有马尔可夫性质的过程，主要是用于研究离散事件在动态系统状态空间的方法。而马尔可夫性(未来状态只与当前状态和当前动作有关而与之前状态无关)可以用如下公式表达</p>
<script type="math/tex; mode=display">
p(s_{t+1}|s_t) = p(s_{t+1}|h_t)其中h_t = {s_1,s_2,s_3,...,s_t}</script><p>而马尔可夫链的主要元素有两部分，分别为各个状态和状态转移矩阵。</p>
<p>用公式表达为</p>
<script type="math/tex; mode=display">
P=\left[\begin{array}{cccc}
P\left(s_{1} \mid s_{1}\right) & P\left(s_{2} \mid s_{1}\right) & \dots & P\left(s_{N} \mid s_{1}\right) \\
P\left(s_{1} \mid s_{2}\right) & P\left(s_{2} \mid s_{2}\right) & \dots & P\left(s_{N} \mid s_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
P\left(s_{1} \mid s_{N}\right) & P\left(s_{2} \mid s_{N}\right) & \dots & P\left(s_{N} \mid s_{N}\right)
\end{array}\right]</script><h4 id="问：什么是马尔可夫奖励过程？"><a href="#问：什么是马尔可夫奖励过程？" class="headerlink" title="问：什么是马尔可夫奖励过程？"></a>问：什么是马尔可夫奖励过程？</h4><p>答：马尔可夫奖励过程是在马尔可夫过程基础上加上奖励函数的过程，主要由三部分组成，分别为状态state，状态转移矩阵，<strong>奖励函数+折扣因子</strong>$(S,P,R,\gamma)$。其中奖励函数和折扣因子用来计算总奖励值Return，用公式表达为$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma ^3 R_{t+4}+…+\gamma^{T-t-1}R_T$</p>
<p>相比于马尔可夫过程，马尔可夫奖励过程中还要多出来的一个概念是价值函数，价值函数是总奖励值的期望值。</p>
<h4 id="问：折扣因子的存在意义是什么？"><a href="#问：折扣因子的存在意义是什么？" class="headerlink" title="问：折扣因子的存在意义是什么？"></a>问：折扣因子的存在意义是什么？</h4><p>答：可以从两个角度来描述其意义</p>
<ul>
<li>有的游戏或者实例是无限马尔可夫过程，如果不带折扣因子，表示某一状态的价值函数就是无穷大，这样是无法通过价值函数的比较产生决策，所以添加折扣因子使得价值函数存在上届</li>
<li>对于同样数值大小的奖励，我们可能更希望在下一时刻就得到，而不是很多时间步之后再得到，所以添加折扣因子可以从一定程度上减小来未来奖励值的权重</li>
</ul>
<h4 id="问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？"><a href="#问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？" class="headerlink" title="问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？"></a>问：什么是贝尔曼方程？怎么理解贝尔曼方程？怎么求解该方程？</h4><p>答：</p>
<blockquote>
<p>贝尔曼方程主要描述了当前状态和未来状态的迭代关系</p>
</blockquote>
<p>有条弹幕说的很贴切，可以把贝尔曼方程理解为套娃</p>
<script type="math/tex; mode=display">
V(s)=\underbrace{R(s)}_{\text {Immediate reward }}+\underbrace{\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {Discounted sum of future reward }}</script><p>$V(s)$是由s状态的奖励和其后续状态$s’$的价值函数决定的</p>
<p>那么$s’$也是这样，由s’的奖励值和$s’$的后续状态$s’’$的价值函数决定……</p>
<script type="math/tex; mode=display">
\begin{array}{c}
{\left[\begin{array}{c}
V\left(s_{1}\right) \\
V\left(s_{2}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right]=\left[\begin{array}{c}
R\left(s_{1}\right) \\
R\left(s_{2}\right) \\
\vdots \\
R\left(s_{N}\right)
\end{array}\right]+\gamma\left[\begin{array}{cccc}
P\left(s_{1} \mid s_{1}\right) & P\left(s_{2} \mid s_{1}\right) & \dots & P\left(s_{N} \mid s_{1}\right) \\
P\left(s_{1} \mid s_{2}\right) & P\left(s_{2} \mid s_{2}\right) & \dots & P\left(s_{N} \mid s_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
P\left(s_{1} \mid s_{N}\right) & P\left(s_{2} \mid s_{N}\right) & \dots & P\left(s_{N} \mid s_{N}\right)
\end{array}\right]\left[\begin{array}{c}
V\left(s_{1}\right) \\
V\left(s_{2}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right]}
\end{array}</script><p>对于上面的式子可以由如下方法计算$V$值</p>
<p>直接计算：（缺点为涉及矩阵求逆，计算量巨大）</p>
<script type="math/tex; mode=display">
V=R+\gamma PV\\
V=(I-\gamma P)^{-1}R</script><p>迭代计算：（后面会有具体的问答）</p>
<ul>
<li>DP</li>
<li>MC</li>
<li>TD</li>
</ul>
<h4 id="问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？"><a href="#问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？" class="headerlink" title="问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？"></a>问：什么是马尔可夫决策过程？与马尔可夫奖励过程的区别和联系是什么？</h4><p>答：马尔可夫决策过程是在马尔科夫奖励过程的基础上加上动作决策的过程，主要由状态，动作，状态转移矩阵，奖励函数+折扣因子组成:$(S,A,P,R,\gamma)$</p>
<p>主要区别为相比于MRP,MDP增加动作这个概念。而MRP与MDP之间的联系是可以将MDP对动作进行”marginization”我更倾向于将”边缘化“理解为”积分”，推导出MRP</p>
<script type="math/tex; mode=display">
\begin{aligned}
P^{\pi}\left(s^{\prime} \mid s\right) &=\sum_{a \in A} \pi(a \mid s) P\left(s^{\prime} \mid s, a\right) \\
R^{\pi}(s) &=\sum_{a \in A} \pi(a \mid s) R(s, a)
\end{aligned}</script><p>从图的角度来看如下（从周博磊老师的PPT上拷过来的）</p>
<p><img src="/2020/07/06/rl-questions/1.png" alt></p>
<h4 id="问：简要说一下MDP里面的价值函数及其推导关系？"><a href="#问：简要说一下MDP里面的价值函数及其推导关系？" class="headerlink" title="问：简要说一下MDP里面的价值函数及其推导关系？"></a>问：简要说一下MDP里面的价值函数及其推导关系？</h4><p>MDP里面的价值函数一共有两种分别是$V$和$Q$</p>
<p>其中$V$函数与$MRP$中的价值函数相同，</p>
<script type="math/tex; mode=display">
\begin{array}{l}
v^{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s\right] \\
q^{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s, A_{t}=a\right] \\
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a)
\end{array}</script><p>而第三个公式为对状态动作价值q函数的动作进行“积分”会得到v价值函数</p>
<h4 id="问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？"><a href="#问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？" class="headerlink" title="问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？"></a>问：什么是贝尔曼期望方程？与贝尔曼方程的区别是什么？贝尔曼期望方程中的价值函数如何相互推导？</h4><script type="math/tex; mode=display">
\begin{array}{c}
v^{\pi}(s)=E_{\pi}\left[R_{t+1}+\gamma v^{\pi}\left(s_{t+1}\right) \mid s_{t}=s\right] \\
q^{\pi}(s, a)=E_{\pi}\left[R_{t+1}+\gamma q^{\pi}\left(s_{t+1}, A_{t+1}\right) \mid s_{t}=s, A_{t}=a\right]
\end{array}</script><p>与贝尔曼方程的区别是增加了“期望”，这个期望指的是对policy中的动作进行求期望</p>
<script type="math/tex; mode=display">
\begin{array}{c}
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s) q^{\pi}(s, a) \\
q^{\pi}(s, a)=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right) \\

\end{array}</script><p>$V$与$Q$的关系如上所示，将上面第一行代入第二行或者第二行代入第一行如下所示(可以看出自举的意思)</p>
<p><img src="/2020/07/06/rl-questions/2.png" alt></p>
<script type="math/tex; mode=display">
v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\right) \\</script><p><img src="/2020/07/06/rl-questions/3.png" alt></p>
<script type="math/tex; mode=display">
q^{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q^{\pi}\left(s^{\prime}, a^{\prime}\right)</script><h4 id="问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？"><a href="#问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？" class="headerlink" title="问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？"></a>问：怎么理解马尔可夫过程中的预测和控制？怎么解决这两个问题？</h4><p>预测和控制的输入和输出是不一样的，预测功能的输入是$MDP+\pi$或者是$MRP$，而其输出是价值函数，目的是判断某一状态的价值是多少</p>
<p>控制功能的输入是MDP的5元组，其输出是最优价值函数或者最优策略，这部分实现的是功能为输出最优策略，为智能体做出动作决策。</p>
<p>解决：</p>
<p>可以通过动态规划和蒙特卡罗方法或者时序差分的方法来解决问题</p>
<h4 id="问：什么是动态规划？（动态规划适合解决什么样的问题？）"><a href="#问：什么是动态规划？（动态规划适合解决什么样的问题？）" class="headerlink" title="问：什么是动态规划？（动态规划适合解决什么样的问题？）"></a>问：什么是动态规划？（动态规划适合解决什么样的问题？）</h4><p>答：动态规划是一种求解决策过程最优化的方法，该问题具有如下两种性质</p>
<ul>
<li>问题具有最优子结构<ul>
<li>问题可以被分为多个子问题</li>
<li>一个最优策略的子策略也是最优的</li>
</ul>
</li>
<li>子问题具有重叠性<ul>
<li>子问题都重复多遍<ul>
<li>以斐波那契数列为例，fib(5)和fib(6)里都需要计算fib(4)</li>
</ul>
</li>
<li>子问题的解可以被存储或者重用<ul>
<li>还是以斐波那契数列为例，通过一个列表存储子问题的解，属于用空间换时间的想法</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>而之前的马尔可夫决策过程正好满足上述条件，贝尔曼等式可以将问题分为多个子问题，而价值函数就相当于“列表”的作用存储并重用子问题的解，下面的策略评估，策略迭代，价值迭代都算是动态规划大类里面，且这三种方法都知道状态转移矩阵，所以是model-based方法</p>
<h4 id="问：怎么解决policy-evaluation问题？"><a href="#问：怎么解决policy-evaluation问题？" class="headerlink" title="问：怎么解决policy evaluation问题？"></a>问：怎么解决policy evaluation问题？</h4><p>答：policy evaluation问题可以通过贝尔曼期望方程来解决，贝尔曼期望方程是通过$V(s)$和$V(s’)$来表达当前时刻状态和下一时刻状态之间的关系,在奖励函数被完全告知的前提下，可以通过迭代计算使得该公式收敛，当公式收敛时会得到一个比较合适的价值函数，这个价值函数就是对策略$\pi$的估值，MDP或者MRP之间的关系如下所示</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{t+1}(s)=& \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{t}\left(s^{\prime}\right)\right) \\
& v_{t+1}(s)=R^{\pi}(s)+\gamma P^{\pi}\left(s^{\prime} \mid s\right) v_{t}\left(s^{\prime}\right)
\end{aligned}</script><h4 id="问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？"><a href="#问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？" class="headerlink" title="问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？"></a>问：最优价值函数和价值函数之间的关系是什么？什么是最优策略？怎么求得最优策略？</h4><p>最优价值函数是对于某个状态s，在所有的策略所对应价值函数$v^{\pi}(s)$中取得最大的价值函数</p>
<p>而最优策略是使得价值函数最大所对应的策略</p>
<script type="math/tex; mode=display">
\begin{array}{c}
v^{*}(s)=\max _{\pi} v^{\pi}(s) \\
\pi^{*}(s)=\arg \max _{\pi} v^{\pi}(s)
\end{array}</script><p>对于价值函数为q的表达式，可以通过极大化$q$函数来得到最优策略</p>
<script type="math/tex; mode=display">
\pi^{*}(a \mid s)=\left\{\begin{array}{ll}
1, & \text { if } a=\arg \max _{a \in A} q^{*}(s, a) \\
0, & \text { otherwise }
\end{array}\right.</script><p>且一般情况下，在有限马尔可夫过程里最优策略具有如下性质</p>
<ul>
<li>确定性</li>
<li>平稳性（无论是在t时刻还是t+n时刻，最优策略不会随着时间而改变）</li>
<li>不具有唯一性（有可能有两个动作的价值函数相等，那么最优策略不唯一）</li>
</ul>
<h4 id="问：怎么通过policy-iteration来优化策略？为什么优化后的策略一定比之前的策略要好？"><a href="#问：怎么通过policy-iteration来优化策略？为什么优化后的策略一定比之前的策略要好？" class="headerlink" title="问：怎么通过policy iteration来优化策略？为什么优化后的策略一定比之前的策略要好？"></a>问：怎么通过policy iteration来优化策略？为什么优化后的策略一定比之前的策略要好？</h4><p>答：policy iteration = policy evaluation+policy improvement</p>
<ul>
<li>首先对于给定的策略$\pi$和价值函数使用贝尔曼期望方程使得价值函数收敛<ul>
<li>$v_{i}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{i-1}\left(s^{\prime}\right)\right)$</li>
</ul>
</li>
<li>通过贪婪的方式，对价值函数取极大化来改进其策略<ul>
<li>$q^{\pi_{i}}(s, a) =R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi_{i}}\left(s^{\prime}\right)$</li>
<li>$pi_{i+1}(s) =\underset{a}{\arg \max } q^{\pi_{i}}(s, a)$</li>
</ul>
</li>
<li>得到新的策略$\pi’$重复1，2两步</li>
</ul>
<p><img src="/2020/07/06/rl-questions/rl-questions\4.png" alt></p>
<p>为什么优化后的策略一定比之前的策略要好？可通过如下公式推导得出</p>
<script type="math/tex; mode=display">
q^{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in \mathcal{A}} q^{\pi}(s, a) \geq q^{\pi}(s, \pi(s))=v^{\pi}(s) \\</script><script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s) & \leq q_{\pi}\left(s, \pi^{\prime}(s)\right) \\
&=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=\pi^{\prime}(s)\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1}, A_{t+1}=\pi^{\prime}\left(S_{t+1}\right)\right] \mid S_{t}=s]\right.\\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} v_{\pi}\left(S_{t+2}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} v_{\pi}\left(S_{t+3}\right) \mid S_{t}=s\right] \\
& \vdots \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots \mid S_{t}=s\right] \\
&=v_{\pi^{\prime}}(s)
\end{aligned}</script><p>而当策略收敛不再改变时，就已经达到最优策略（第一行），推导可得出满足<strong>贝尔曼最优方程</strong>（最后一行）</p>
<script type="math/tex; mode=display">
\begin{array}{c}
v^{*}(s)=\max _{a} q^{*}(s, a) \\
q^{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{*}\left(s^{\prime}\right) \\
v^{*}(s)=\max _{a} R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{*}\left(s^{\prime}\right) \\
q^{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} q^{*}\left(s^{\prime}, a^{\prime}\right)
\end{array}</script><h4 id="问：value-iteration与policy-iteration的区别？"><a href="#问：value-iteration与policy-iteration的区别？" class="headerlink" title="问：value iteration与policy iteration的区别？"></a>问：value iteration与policy iteration的区别？</h4><ul>
<li>value iteration 是<strong>直接</strong>使用贝尔曼最优方程来进行迭代<ul>
<li>$v_{i+1}(s) \leftarrow \max _{a \in \mathcal{A}} R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) v_{i}\left(s^{\prime}\right)$</li>
</ul>
</li>
<li>当价值函数收敛时可以推导得到最佳策略<ul>
<li>$\pi^{*}(s) \leftarrow \underset{a}{\arg \max } R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v_{e n d}\left(s^{\prime}\right)$</li>
</ul>
</li>
</ul>
<p><img src="/2020/07/06/rl-questions/6.png" alt></p>
<p>可以看出价值迭代方法没有像底下策略迭代一样需要分为两步，只是一个两侧的循环即可，在David silver课程中讲过价值迭代也可以看作单步的策略迭代</p>
<p><img src="/2020/07/06/rl-questions/rl-questions\5.png" alt></p>
<p>这两种实现“控制”方法的区别点在于</p>
<ul>
<li><p>直观来看，策略迭代分为两步，价值迭代只有一步</p>
</li>
<li><p>策略迭代中的策略评估部分需要收敛后才能根据收敛的价值函数进行策略提升，而价值迭代不需要等价值函数收敛，马上对价值函数取极大值，没有policy的参与</p>
</li>
<li>策略迭代使用的更新公式为<strong>贝尔曼期望方程</strong>，价值迭代使用的更新公式为<strong>贝尔曼最优方程</strong></li>
</ul>
<h4 id="问：MC和TD方法的好处是什么？-现实情况下没有状态转移矩阵应该怎么办？"><a href="#问：MC和TD方法的好处是什么？-现实情况下没有状态转移矩阵应该怎么办？" class="headerlink" title="问：MC和TD方法的好处是什么？/现实情况下没有状态转移矩阵应该怎么办？"></a>问：MC和TD方法的好处是什么？/现实情况下没有状态转移矩阵应该怎么办？</h4><p>答：动态规划中的价值估计和MC,TD之间的一个比较重要的区别是动态规划中并没有让智能体与环境进行交互，而MC,TD是智能体与环境交互才会得到真实交互序列，才会进一步得到对状态的估值。现实情况中，没有状态转移矩阵和奖励函数R还要得到状态估计值（可能是因为环境过于复杂，不能得到状态转移矩阵和所有的准确的奖励值）就不能用model-based的动态规划方法求解，可以使用蒙特卡罗方法或者是时序差分方法来计算价值函数。</p>
<h4 id="问：DP和MC之间的区别是什么？"><a href="#问：DP和MC之间的区别是什么？" class="headerlink" title="问：DP和MC之间的区别是什么？"></a>问：DP和MC之间的区别是什么？</h4><ul>
<li>价值函数的求法不同<ul>
<li>动态规划方法是使用自举的方式估计价值函数（最优子结构的性质）</li>
<li>MC采用均值法估计价值函数（蒙特卡罗方法的核心思想是大数定理，让智能体与环境进行多轮交互，对每轮游戏的每种状态都取其return值的平均值。而与环境交互次数越多，得到return的平均值越接近于真实值。）</li>
</ul>
</li>
<li>所需条件不同<ul>
<li>DP为model-based方法，需要知道状态转移矩阵和奖励函数</li>
<li>优点为model-free，不用状态转移矩阵和奖励函数</li>
</ul>
</li>
<li><p>游戏类型也有区分</p>
<ul>
<li>DP类游戏对终止状态的有无没有明确的规定</li>
<li>MC规定该类游戏必须是有<strong>终止状态</strong>的游戏。</li>
</ul>
</li>
<li><p>更新状态不同</p>
<ul>
<li>DP是更新所有状态（如果状态数量过多会导致更新速度缓慢）</li>
<li>MC更新的只有实际产生序列中的状态（不需要考虑其他未出现的状态，有利有弊，利为更新速度快，弊为还存在一部分没有更新到的状态）</li>
</ul>
</li>
</ul>
<h4 id="问：简要介绍一下TD-你觉得TD的优点是什么-（TD于MC的区别是什么？）"><a href="#问：简要介绍一下TD-你觉得TD的优点是什么-（TD于MC的区别是什么？）" class="headerlink" title="问：简要介绍一下TD,你觉得TD的优点是什么?（TD于MC的区别是什么？）"></a>问：简要介绍一下TD,你觉得TD的优点是什么?（TD于MC的区别是什么？）</h4><p>TD可以看作是蒙特卡洛法和动态规划方法的一个折中，其优点为</p>
<ul>
<li>计算价值函数的时间点不同<ul>
<li>MC需要等序列结束才能计算价值函数（偏差小，方差大）</li>
<li>TD不需要等序列结束只是向后走一步，得到一个target的估计值（偏差大，方差小）</li>
</ul>
</li>
<li>TD与MC对于终止状态的要求不一样<ul>
<li>TD不要求序列有终止状态</li>
<li>MC要求序列有终止状态</li>
</ul>
</li>
<li>对马尔可夫性的要求程度也不同<ul>
<li>MC对马尔可夫性质要求不高，即使该序列不符合MDP，也可以使用MC方法求价值函数</li>
<li>TD方法在马尔可夫环境中计算效率更高</li>
</ul>
</li>
<li>TD与MC一样，不需要知道状态转移矩阵和奖励函数就可以计算出价值函数</li>
<li>相比于MC中的target值为序列的奖励总return值，TD的target值不一样（是向后走一步的即时奖励+$\gamma*$后继状态的估计值），TD也是直接于环境进行交互，吸取DP的自举思想，通过下一时刻状态的价值函数求得当前状态的价值函数<ul>
<li>TD:$\begin{array}{c}<br>v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma v\left(S_{t+1}\right)-v\left(S_{t}\right)\right) \\<br>\end{array}$</li>
<li>MC:$v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{i, t}-v\left(S_{t}\right)\right)$</li>
</ul>
</li>
</ul>
<p>图示区别如下</p>
<p><img src="/2020/07/06/rl-questions/7.png" alt></p>
<p><img src="/2020/07/06/rl-questions/rl-questions\8.png" alt></p>
<p><img src="/2020/07/06/rl-questions/rl-questions\9.png" alt></p>
<h4 id="问：-epsilon-greedy-有什么作用？-为什么要用-epsilon-greedy-呢？"><a href="#问：-epsilon-greedy-有什么作用？-为什么要用-epsilon-greedy-呢？" class="headerlink" title="问：$\epsilon-greedy $ 有什么作用？(为什么要用$\epsilon-greedy $ 呢？)"></a>问：$\epsilon-greedy $ 有什么作用？(为什么要用$\epsilon-greedy $ 呢？)</h4><ul>
<li>$\epsilon-greedy $ 相比于直接greedy更好的体现了”探索“的思想，以1-$\epsilon$的概率随机选择动作，以$1-\epsilon$的概率选择贪婪策略，且探索因子$\epsilon $随着迭代次数的增加而减小</li>
</ul>
<script type="math/tex; mode=display">
\pi(a \mid s)=\left\{\begin{array}{ll}
\epsilon /|\mathcal{A}|+1-\epsilon & \text { if } a^{*}=\arg \max _{a \in \mathcal{A}} Q(s, a) \\
\epsilon /|\mathcal{A}| & \text { otherwise }
\end{array}\right.</script><p>如下推导确定，以$\epsilon -greedy$的方法进行探索出来的策略$\pi’$一定优于迭代之前的$\pi$</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a \in \mathcal{A}} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \\
&=\frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\epsilon}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a \mid s)-\frac{\epsilon}{|\mathcal{A}|}}{1-\epsilon} q_{\pi}(s, a) \\
&=\sum_{a \in \mathcal{A}} \pi(a \mid s) q_{\pi}(s, a)=v_{\pi}(s)
\end{aligned}</script><p>我更倾向于如下理解（这样就可以与目标对应上）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi’}(s)=q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a \in \mathcal{A}} \pi^{\prime}(a | s) q_{\pi}(s, a) \\
&=\epsilon / m \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \max _{a \in \mathcal{A}} q_{\pi}(s, a) \\
& \geq \epsilon / m \sum_{a \in \mathcal{A}} q_{\pi}(s, a)+(1-\epsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a | s)-\epsilon / m}{1-\epsilon} q_{\pi}(s, a) \\
&=\sum_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a)=q_{\pi}\left(s, \pi(s)\right)=v_{\pi}(s)
\end{aligned}</script><h4 id="问：Sarsa和Q-learning的异同点？"><a href="#问：Sarsa和Q-learning的异同点？" class="headerlink" title="问：Sarsa和Q-learning的异同点？"></a>问：Sarsa和Q-learning的异同点？</h4><p>答：</p>
<p><img src="/2020/07/06/rl-questions/10.png" alt></p>
<p><img src="/2020/07/06/rl-questions/rl-questions\11.png" alt>sarsa和q-learning的算法流程图如上所示</p>
<p>两算法的主要区别点是一个为<strong>on-policy</strong>算法，一个为<strong>off-policy</strong>算法。</p>
<ul>
<li>关于on-policy还是off-policy最重要的区别是算法<strong>所更新的策略和跟环境交互的策略是否相同</strong>，同为on，异为off。</li>
<li>且网上还有部分资料说区分on还是off是看是否使用了<strong>重要性采样</strong>（我在这里并不是十分认同这种说法，个人理解为增加重要性采样的权重值只是为了使得通过两个不同的策略计算的值差别缩小，换句话说，重要性采样是方法而非原因）</li>
<li>周老师的讲解是sarsa采取动作A和A’用的都是$\epsilon -greedy$所以是on-policy而q-learning算法分别采用$\epsilon-greedy$和$max$操作，所以q-learning是off-policy（没有十分理解这种说法，在此提出的疑问是如果将q-learning的选择动作$A$换成$greedy$的算法，难道q-learning就变成on-policy的了？或者将sarsa算法的第一步改为$greedy$就变成off-policy了？-&gt;这种说法跟别说不通啊，或者说暂时存疑~）</li>
<li>我比较认同的说法是，对于sarsa算法，在更新Q值需要计算的TD-target值的动作A’是从$\epsilon-greedy$方式得来的，而在q-learning算法中计算TD-target值是直接对所有的Q值取最大值。该层loop结束后，sarsa算法说到做到一定会执行<strong>刚才用于计算TD-target值</strong>的动作，而q-learning算法只是将”$a$”用于计算TD-target值，在新一轮loop会根据新的状态重新选择动作。</li>
</ul>
<h4 id="问：强化学习训练十分不稳定的原因有哪些？"><a href="#问：强化学习训练十分不稳定的原因有哪些？" class="headerlink" title="问：强化学习训练十分不稳定的原因有哪些？"></a>问：强化学习训练十分不稳定的原因有哪些？</h4><ul>
<li>首先问题出在函数拟合上，我们将状态输入到神经网络，输出状态的价值函数会产生很多误差</li>
<li>自举的模式中的TD-target值包含$v(s’)$，是含有一部分偏差的，会导致训练不稳定</li>
<li>在off-policy模式下训练智能体，可能会导致两个策略差异过大导致效果变差</li>
</ul>
<h4 id="问：蒙特卡罗，sarsa，q-learning的收敛问题？"><a href="#问：蒙特卡罗，sarsa，q-learning的收敛问题？" class="headerlink" title="问：蒙特卡罗，sarsa，q-learning的收敛问题？"></a>问：蒙特卡罗，sarsa，q-learning的收敛问题？</h4><p><img src="/2020/07/06/rl-questions/12.png" alt></p>
<p>备注：对号为可以收敛到全局最优解，（对号）是接近于全据最优解，叉号为不保证</p>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1415500736@qq.com </span>
    </div>
</article>


<p>
    <a href="javascript:void(0)" class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>RL_questions</p>
    
    <p><span class="copy-title">本文作者:</span><a href="javascript:void(0)" title="Rock">Rock</a></p>
    <p><span class="copy-title">发布时间:</span>2020-07-06, 21:25:01</p>
    <p><span class="copy-title">最后更新:</span>2020-07-12, 20:26:58</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2020/07/06/rl-questions/" title="RL_questions">http://rock-blog.top/2020/07/06/rl-questions/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: 'e78b4c19bc08850d88df',
            clientSecret: '308b55a6d580ee7a819af0f950b3188be697ae29',
            repo: 'guobaoyo.github.io',
            owner: 'guobaoyo',
            admin: ['guobaoyo'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js" value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">

    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2016-2019 Rock&#39;s  blog</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class id="rocket" href="javascript:void(0)"></a>
    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close" href="javascript:void(0)" onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1"></script>

<script src="/js/script.js?v=1.0.1"></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#编程','#AI','#数学','#三省吾身','#深度学习','#强化学习','#CV','#python','#go','#技术小结','#leetcode','#考研','#NLP','#组会报告',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #c1bfc1;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.5;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
