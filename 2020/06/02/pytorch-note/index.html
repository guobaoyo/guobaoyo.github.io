<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <title>pytorch_note | Rock-Blog</title>
  <meta name="keywords" content=" AI , python , 技术小结 ">
  <meta name="description" content="pytorch_note | Rock-Blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="剑指offer_第三题&amp;amp;a,b = b,a备注">
<meta name="keywords" content="python,技术小结,leetcode">
<meta property="og:type" content="article">
<meta property="og:title" content="剑指offer03">
<meta property="og:url" content="http://rock-blog.top/2020/07/04/sword-offer03/index.html">
<meta property="og:site_name" content="Rock-Blog">
<meta property="og:description" content="剑指offer_第三题&amp;amp;a,b = b,a备注">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2020-07-05T00:24:47.960Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="剑指offer03">
<meta name="twitter:description" content="剑指offer_第三题&amp;amp;a,b = b,a备注">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.0.1" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.0.1" rel="stylesheet">

<link href="//cdn.bootcss.com/animate.css/3.5.2/animate.min.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="/js/jquery.autocomplete.min.js?v=1.0.1"></script>

<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>



<script src="//cdn.bootcss.com/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>

<script src="/js/iconfont.js?v=1.0.1"></script>

</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="true">
  <input class="theme_blog_path" value>
</div>

<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/" class="avatar_target">
    <img class="avatar" src="/img/avatar.jpg">
</a>
<div class="author">
    <span>Rock</span>
</div>

<div class="icon">
    
        
        <a title="github" href="https://github.com/guobaoyo" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-github"/>
                </svg>
            
        </a>
        
    
        
        <a title="email" href="mailto:1415500736@qq.com" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-email"/>
                </svg>
            
        </a>
        
    
        
        <a title="qq" href="http://wpa.qq.com/msgrd?v=3&uin=1415500736&site=qq&menu=yes" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-qq"/>
                </svg>
            
        </a>
        
    
        
        <a title="neteasemusic" href="https://music.163.com/#/user/home?id=280020740" target="_blank">
            
                <svg class="iconfont-svg" aria-hidden="true">
                    <use xlink:href="#icon-neteasemusic"/>
                </svg>
            
        </a>
        
    
</div>




<ul>
    <li><div class="all active">全部文章<small>(23)</small></div></li>
    
        
            
            <li><div data-rel="三省吾身">三省吾身<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="强化学习">强化学习<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="AI">AI<small>(7)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="编程">编程<small>(4)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="数学">数学<small>(2)</small></div>
                
            </li>
            
        
    
        
            
            <li><div data-rel="组会报告">组会报告<small>(1)</small></div>
                
            </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
    
    
    
    
    </div>
    <div><a class="friends">友链</a></div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="23">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="back-title-list"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
            <li><a target="_blank" href="http://zivblog.top">王金锋</a></li>
            
            <li><a target="_blank" href="http://yearing1017.cn/">进哥</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <form onkeydown="if(event.keyCode==13){return false;}">
        <input class="search" type="text" placeholder="以 in: 开头进行全文搜索" autocomplete="off" id="local-search-input">
        <i class="cross"></i>
        <span>
            <label for="tagswitch">Tags:</label>
            <input id="tagswitch" type="checkbox" style="display: none">
            <i id="tagsWitchIcon"></i>
        </span>
    </form>
    <div class="tags-list">
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">三省吾身</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">AI</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">数学</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">编程</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">深度学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">强化学习</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">CV</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color2">python</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">go</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">技术小结</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">leetcode</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color5">组会报告</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color3">考研</a>
    </li>
    
    <li class="article-tag-list-item">
        <a href="javascript:" class="color4">NLP</a>
    </li>
    
    <div class="clearfix"></div>
</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        <a class="三省吾身 " href="/2019/07/08/20190708日记/" data-tag="三省吾身,AI,数学" data-author>
            <span class="post-title" title="20190708日记">20190708日记</span>
            <span class="post-date" title="2019-07-08 19:43:26">2019/07/08</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/08/A-algorithm/" data-tag="AI,数学,编程" data-author>
            <span class="post-title" title="A*_algorithm">A*_algorithm</span>
            <span class="post-date" title="2020-06-08 21:25:01">2020/06/08</span>
        </a>
        
        <a class="强化学习 " href="/2019/06/22/Atari-a3c/" data-tag="AI,深度学习,强化学习" data-author>
            <span class="post-title" title="Aatri_A3c">Aatri_A3c</span>
            <span class="post-date" title="2019-06-22 21:25:01">2019/06/22</span>
        </a>
        
        <a class="AI " href="/2019/05/08/DLwithPython/" data-tag="深度学习,CV,python" data-author>
            <span class="post-title" title="DeepLearning with Python">DeepLearning with Python</span>
            <span class="post-date" title="2019-05-08 17:54:01">2019/05/08</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/29/DLwords/" data-tag="AI,深度学习" data-author>
            <span class="post-title" title="DLwords">DLwords</span>
            <span class="post-date" title="2019-04-29 19:17:39">2019/04/29</span>
        </a>
        
        <a class="强化学习 " href="/2020/01/26/RLTF/" data-tag="AI,数学,强化学习" data-author>
            <span class="post-title" title="RLTF">RLTF</span>
            <span class="post-date" title="2020-01-26 19:21:06">2020/01/26</span>
        </a>
        
        <a class="AI " href="/2019/09/14/deep-reinforcement-learning/" data-tag="数学,深度学习,强化学习" data-author>
            <span class="post-title" title="deep-reinforcement-learning">deep-reinforcement-learning</span>
            <span class="post-date" title="2019-09-14 10:00:12">2019/09/14</span>
        </a>
        
        <a class="编程 " href="/2019/07/23/go-http搭建服务器知识点儿/" data-tag="编程,go" data-author>
            <span class="post-title" title="go-http搭建服务器知识点儿">go-http搭建服务器知识点儿</span>
            <span class="post-date" title="2019-07-23 11:29:08">2019/07/23</span>
        </a>
        
        <a class="编程 " href="/2020/01/31/leetcode/" data-tag="数学,编程,python" data-author>
            <span class="post-title" title="leetcode">leetcode</span>
            <span class="post-date" title="2020-01-31 10:14:45">2020/01/31</span>
        </a>
        
        <a class href="/2020/06/05/lgb-note/" data-tag="AI,编程,python" data-author>
            <span class="post-title" title="lgb_note">lgb_note</span>
            <span class="post-date" title="2020-06-05 21:25:01">2020/06/05</span>
        </a>
        
        <a class="数学 " href="/2019/04/25/math/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="线性代数补充笔记">线性代数补充笔记</span>
            <span class="post-date" title="2019-04-25 21:25:01">2019/04/25</span>
        </a>
        
        <a class="AI " href="/2020/06/02/pytorch-note/" data-tag="AI,python,技术小结" data-author>
            <span class="post-title" title="pytorch_note">pytorch_note</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="AI " href="/2020/07/04/sword-offer03/" data-tag="python,技术小结,leetcode" data-author>
            <span class="post-title" title="剑指offer03">剑指offer03</span>
            <span class="post-date" title="2020-07-04 20:19:39">2020/07/04</span>
        </a>
        
        <a class="强化学习 " href="/2020/06/11/tianshou-a2c-note/" data-tag="AI,编程,强化学习" data-author>
            <span class="post-title" title="tianshou平台源码阅读笔记">tianshou平台源码阅读笔记</span>
            <span class="post-date" title="2020-06-11 21:25:01">2020/06/11</span>
        </a>
        
        <a class="编程 " href="/2019/04/29/使用Python实现excel中固定数据排序且改名至另一个文件夹/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="基于python实现excel中读取文件目录+相应数据排序+将文件重命名">基于python实现excel中读取文件目录+相应数据排序+将文件重命名</span>
            <span class="post-date" title="2019-04-29 19:30:06">2019/04/29</span>
        </a>
        
        <a class="AI " href="/2020/06/02/强化学习在滴滴网约车的应用记录/" data-tag="AI,强化学习,组会报告" data-author>
            <span class="post-title" title="强化学习在滴滴网约车的应用笔记">强化学习在滴滴网约车的应用笔记</span>
            <span class="post-date" title="2020-06-02 20:19:39">2020/06/02</span>
        </a>
        
        <a class="数学 " href="/2019/10/24/数值计算与凸优化补充笔记/" data-tag="AI,数学,深度学习" data-author>
            <span class="post-title" title="数值计算与凸优化补充笔记">数值计算与凸优化补充笔记</span>
            <span class="post-date" title="2019-10-24 16:14:54">2019/10/24</span>
        </a>
        
        <a class="三省吾身 " href="/2019/11/03/智源大会听报告笔记/" data-tag="三省吾身,AI" data-author>
            <span class="post-title" title="智源大会听学术报告笔记">智源大会听学术报告笔记</span>
            <span class="post-date" title="2019-11-03 13:24:12">2019/11/03</span>
        </a>
        
        <a class="编程 " href="/2019/09/05/物体检测打标小脚本-获取当前图片中鼠标位置/" data-tag="编程,python,技术小结" data-author>
            <span class="post-title" title="物体检测打标小脚本-获取当前图片中鼠标位置">物体检测打标小脚本-获取当前图片中鼠标位置</span>
            <span class="post-date" title="2019-09-05 15:30:36">2019/09/05</span>
        </a>
        
        <a class="组会报告 " href="/2019/09/26/组会报告-0930-QLearning/" data-tag="深度学习,强化学习,组会报告" data-author>
            <span class="post-title" title="组会报告-0930-QLearning">组会报告-0930-QLearning</span>
            <span class="post-date" title="2019-09-26 15:15:04">2019/09/26</span>
        </a>
        
        <a class="三省吾身 " href="/2019/04/15/考研历程反思总结/" data-tag="三省吾身,考研" data-author>
            <span class="post-title" title="考研历程反思总结">考研历程反思总结</span>
            <span class="post-date" title="2019-04-15 21:25:01">2019/04/15</span>
        </a>
        
        <a class="AI " href="/2019/07/21/计算机视觉存疑解答记录/" data-tag="AI,数学,CV" data-author>
            <span class="post-title" title="计算机视觉存疑解答记录">计算机视觉存疑解答记录</span>
            <span class="post-date" title="2019-07-21 13:04:25">2019/07/21</span>
        </a>
        
        <a class="AI " href="/2019/07/29/达观杯比赛记录/" data-tag="三省吾身,AI,NLP" data-author>
            <span class="post-title" title="达观杯比赛记录">达观杯比赛记录</span>
            <span class="post-date" title="2019-07-29 20:19:39">2019/07/29</span>
        </a>
        
    </nav>
</div>
    </div>
    <div class="hide-list">
        <div class="semicircle">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div class="post">
    <div class="pjax">
        <article id="post-pytorch-note" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">pytorch_note</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            
                <a href="javascript:" data-rel="AI">AI</a>
            
        </span>
        
        
        <span class="tag">
            
            <a href="javascript:" class="color3">AI</a>
            
            <a href="javascript:" class="color2">python</a>
            
            <a href="javascript:" class="color5">技术小结</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
        创建时间:<time class="date" title="更新时间: 2020-06-21 22:21:01">2020-06-02 20:19</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读:<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-handbook-易错点记录"><span class="toc-text">pytorch-handbook 易错点记录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#chapter1"><span class="toc-text">chapter1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-tensor-tutorial"><span class="toc-text">1_tensor_tutorial</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#跟着notebook安装好pytorch"><span class="toc-text">跟着notebook安装好pytorch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-randn-torch-rand和torch-empty的区别"><span class="toc-text">torch.randn torch.rand和torch.empty的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#震惊add-copy-和-t-这种函数会有以下效果"><span class="toc-text">震惊add_ copy_ 和 t_ 这种函数会有以下效果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#item-只适用于看标量"><span class="toc-text">item()只适用于看标量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-autograd-tutorial"><span class="toc-text">2_autograd_tutorial</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-neural-networks-tutorial"><span class="toc-text">3_neural_networks_tutorial</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#在查看卷积层的size-时需要注意的问题"><span class="toc-text">在查看卷积层的size()时需要注意的问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch接收小批量的样本"><span class="toc-text">torch接收小批量的样本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pytorch的unsequeeze"><span class="toc-text">pytorch的unsequeeze</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#chapter2"><span class="toc-text">chapter2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-pytorch-basics-tensor"><span class="toc-text">2.1.1.pytorch-basics-tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#注意："><span class="toc-text">注意：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-pytorch-basics-autograd"><span class="toc-text">2.1.2-pytorch-basics-autograd</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-pytorch-basics-nerual-network"><span class="toc-text">2.1.3-pytorch-basics-nerual-network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2线性回归"><span class="toc-text">2.2.2线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3损失函数"><span class="toc-text">2.2.3损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-cnn"><span class="toc-text">2.4-cnn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-RNN"><span class="toc-text">2.5 RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-logistic-regression"><span class="toc-text">3-1-logistic-regression</span></a></li></ol></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>pytorch_note <a href="https://github.com/zergtant/pytorch-handbook" target="_blank" rel="noopener">https://github.com/zergtant/pytorch-handbook</a></p>
<a id="more"></a>
<h1 id="pytorch-handbook-易错点记录"><a href="#pytorch-handbook-易错点记录" class="headerlink" title="pytorch-handbook 易错点记录"></a>pytorch-handbook 易错点记录</h1><h2 id="chapter1"><a href="#chapter1" class="headerlink" title="chapter1"></a>chapter1</h2><h3 id="1-tensor-tutorial"><a href="#1-tensor-tutorial" class="headerlink" title="1_tensor_tutorial"></a>1_tensor_tutorial</h3><h4 id="跟着notebook安装好pytorch"><a href="#跟着notebook安装好pytorch" class="headerlink" title="跟着notebook安装好pytorch"></a>跟着notebook安装好pytorch</h4><h4 id="torch-randn-torch-rand和torch-empty的区别"><a href="#torch-randn-torch-rand和torch-empty的区别" class="headerlink" title="torch.randn torch.rand和torch.empty的区别"></a>torch.randn torch.rand和torch.empty的区别</h4><ul>
<li>rand是0-1的均匀分布</li>
<li>randn是均值为0，方差是1的正态分布（理解为n是normal的意思）</li>
<li>empty是创建出来一个指定维度的tensor张量，其中有个stride的属性意思是<strong>在张量的第k维从一个元素跳转到下一个元素所需的内存</strong><a href="https://blog.csdn.net/qq_32806793/article/details/102950838" target="_blank" rel="noopener">https://blog.csdn.net/qq_32806793/article/details/102950838</a></li>
<li>可以从他的例子中看出虽然tensor转置了，但是在转置前从0.0834到0.0364需要跨过5个单位的内存，从0.0834到0.8865只需要一个单位的内存。转置之后，虽然变为5行2列的tensor，但是从0.0834跳到0.0364还是需要跳过1个单位的内存，从0.0834到0.0364表面上看是在一起了，实际上还是需要跨过5个单位的内存（2020_0602的粗浅理解，可继续改）</li>
</ul>
<h4 id="震惊add-copy-和-t-这种函数会有以下效果"><a href="#震惊add-copy-和-t-这种函数会有以下效果" class="headerlink" title="震惊add_ copy_ 和 t_ 这种函数会有以下效果"></a>震惊add_ copy_ 和 t_ 这种函数会有以下效果</h4><p>任何 以<code>_</code> 结尾的操作都会用结果替换原变量. 例如: <code>x.copy_(y)</code>, <code>x.t_()</code>, 都会改变 <code>x</code>.</p>
<h4 id="item-只适用于看标量"><a href="#item-只适用于看标量" class="headerlink" title="item()只适用于看标量"></a>item()只适用于看标量</h4><h3 id="2-autograd-tutorial"><a href="#2-autograd-tutorial" class="headerlink" title="2_autograd_tutorial"></a>2_autograd_tutorial</h3><h3 id="3-neural-networks-tutorial"><a href="#3-neural-networks-tutorial" class="headerlink" title="3_neural_networks_tutorial"></a>3_neural_networks_tutorial</h3><h4 id="在查看卷积层的size-时需要注意的问题"><a href="#在查看卷积层的size-时需要注意的问题" class="headerlink" title="在查看卷积层的size()时需要注意的问题"></a>在查看卷积层的size()时需要注意的问题</h4><p><img src="/2020/06/02/pytorch-note/图例_1.png" alt></p>
<pre><code class="lang-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
def __init__(self):
    super(Net, self).__init__()
    # 1 input image channel, 6 output channels, 5x5 square convolution,指的是输入的图片是1通道，6个输出通道，卷积核是5*5的
    # kernel
    self.conv1 = nn.Conv2d(1, 6, 5)#一开始是(32-5+2*0)/1+1 = 28 28*28经过2*2的最大池化后变成14*14*6
    self.conv2 = nn.Conv2d(6, 16, 5)#14*14的图像经过卷积 （14-5+2*0）/1+1 = 10 经过2*2的最大池化后 变为5*5*16
    # an affine operation: y = Wx + b
    self.fc1 = nn.Linear(16 * 5 * 5, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

def forward(self, x):
    # Max pooling over a (2, 2) window
    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
    # If the size is a square you can only specify a single number 
    x = F.max_pool2d(F.relu(self.conv2(x)), 2)
    print(self.num_flat_features(x))
    x = x.view(-1, self.num_flat_features(x))
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return x

def num_flat_features(self, x):
    size = x.size()[1:]  # all dimensions except the batch dimension
    num_features = 1
    for s in size:
        num_features *= s
    return num_features
x = torch.empty(1,1,32,32)
net = Net()
print(net.forward(x))
print(net)
</code></pre>
<p><img src="/2020/06/02/pytorch-note/print_1.png" alt></p>
<p>可以看到net的parameters一共有10层，2020_0603的理解是每个conv卷积运算算一层，每个激活函数relu和sofmax算一层（这么理解是错误的，没法与10对应上）</p>
<p>在后面多的三层[120],[84],[10]是bias参数，而将前面卷积层的池化，激活函数都去掉后发现torch.Size([6])，torch.Size([16])依然存在，输出发现是形如</p>
<pre><code class="lang-python">tensor([ 0.0424,  0.0156,  0.0110, -0.0343,  0.0380,  0.0437],
       requires_grad=True)
</code></pre>
<p>的张量</p>
<p><font color="red"><strong>存疑：</strong> </font>具体里面的6个数不知道是干啥用的~</p>
<p><font color="blue"><strong>答疑：</strong> </font>6个数是卷积层的偏置值，可以看<a href="https://cs231n.github.io/assets/conv-demo/index.html" target="_blank" rel="noopener">https://cs231n.github.io/assets/conv-demo/index.html</a> 链接可以看出偏置的维度=卷积核的个数=卷积后的通道数</p>
<h4 id="torch接收小批量的样本"><a href="#torch接收小批量的样本" class="headerlink" title="torch接收小批量的样本"></a>torch接收小批量的样本</h4><p>如果想输入单个样本需要定义</p>
<pre><code class="lang-python">input = torch.randn(1, 1, 32, 32)
</code></pre>
<p>参数分别是(样本数 <em> 通道数</em> 高*宽)</p>
<pre><code class="lang-python">input.unsqueeze(0)
</code></pre>
<p>指的是增加一个维度，且增加的位置是第一个</p>
<h4 id="pytorch的unsequeeze"><a href="#pytorch的unsequeeze" class="headerlink" title="pytorch的unsequeeze"></a>pytorch的unsequeeze</h4><pre><code class="lang-python">import torch
a = torch.tensor([[1,2],[3,4]])
print(a.size())
print(a)
b = a.unsqueeze(0)
print(b.size())
print(b)
c = a.unsqueeze(1)
print(c.size())
print(c)
d = a.unsqueeze(2)
print(d.size())
print(d)
</code></pre>
<p>输出为</p>
<p><img src="/2020/06/02/pytorch-note/print_2.png" alt></p>
<p>squeeze与unsqueeze函数相对应，squeeze的意思是将指定维度为1的维度去掉，或者将全部维度为1的维度去掉</p>
<h2 id="chapter2"><a href="#chapter2" class="headerlink" title="chapter2"></a>chapter2</h2><h3 id="2-1-1-pytorch-basics-tensor"><a href="#2-1-1-pytorch-basics-tensor" class="headerlink" title="2.1.1.pytorch-basics-tensor"></a>2.1.1.pytorch-basics-tensor</h3><p>tensor里面有7种数据类型：</p>
<ul>
<li>32位浮点型：torch.FloatTensor。 (默认)</li>
<li>64位整型：torch.LongTensor。</li>
<li>32位整型：torch.IntTensor。</li>
<li>16位整型：torch.ShortTensor。</li>
<li>64位浮点型：torch.DoubleTensor。</li>
</ul>
<h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><ul>
<li><p>如果张量中只有一个元素的tensor也可以调用<code>tensor.item()</code>方法</p>
</li>
<li><p>Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。</p>
</li>
</ul>
<h3 id="2-1-2-pytorch-basics-autograd"><a href="#2-1-2-pytorch-basics-autograd" class="headerlink" title="2.1.2-pytorch-basics-autograd"></a>2.1.2-pytorch-basics-autograd</h3><p>看到这里有几个不明白的点：</p>
<p>问：在简单的自动求导部分为什么运行一遍，导数显示增加1？</p>
<p>​    <img src="/2020/06/02/pytorch-note/pic_3.png" alt></p>
<p>答：目前的理解是如果不对缓存进行清空，调用backward()函数会自动在其上面进行叠加。</p>
<p>问：复杂的自动求导部分，为什么必须在backward里面加上torch.ones_like参数？</p>
<p><img src="/2020/06/02/pytorch-note/pic4.png" alt></p>
<p>答：其传入的参数应该与output输出的维度一样，可以理解为每个output分量对输入张量求导时的权重。将下面的两篇文章和官方的教程结合理解，如果输出是一个标量，直接执行out.backward()默认参数是torch.tensor(1.0)，也就是说在输出的导数或梯度上乘以1，也就是其本身。输出如果不是一个标量而是一个向量的话，会输出<em>grad can be implicitly created only for scalar outputs</em> </p>
<p>上面的解答是从资料<a href="https://zhuanlan.zhihu.com/p/79801410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79801410</a> 和 <a href="https://www.cnblogs.com/JeasonIsCoding/p/10164948.html" target="_blank" rel="noopener">https://www.cnblogs.com/JeasonIsCoding/p/10164948.html</a> 推理出来的</p>
<p>注意点：</p>
<ul>
<li>浮点类型的数才可以自动求导</li>
<li>在调用<code>backward()</code>时，只计算<code>requires_grad</code>和<code>is_leaf</code>同时为真的节点的梯度。</li>
</ul>
<h3 id="2-1-3-pytorch-basics-nerual-network"><a href="#2-1-3-pytorch-basics-nerual-network" class="headerlink" title="2.1.3-pytorch-basics-nerual-network"></a>2.1.3-pytorch-basics-nerual-network</h3><p>注意在这里第一个参数的意思是这个batch的大小为1，第二个参数的意思是通道数，后两个是长和宽</p>
<pre><code class="lang-python">input = torch.randn(1, 1, 32, 32) # 这里的对应前面forward的输入是32
out = net(input)
out.size()
</code></pre>
<h3 id="2-2-2线性回归"><a href="#2-2-2线性回归" class="headerlink" title="2.2.2线性回归"></a>2.2.2线性回归</h3><pre><code class="lang-python">x = np.random.rand(256)#这里面的数字的意思是又256个点
</code></pre>
<p>这里的256的意思是一共输出256个点，且这些数值均服从“0~1”均匀分布</p>
<h3 id="2-2-3损失函数"><a href="#2-2-3损失函数" class="headerlink" title="2.2.3损失函数"></a>2.2.3损失函数</h3><p>注意：损失函数的计算出来的结果已经对batch取了平均</p>
<h3 id="2-4-cnn"><a href="#2-4-cnn" class="headerlink" title="2.4-cnn"></a>2.4-cnn</h3><p><a href="https://zhuanlan.zhihu.com/p/87117010" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/87117010</a></p>
<p><a href="https://www.cnblogs.com/CJT-blog/p/9314134.html" target="_blank" rel="noopener">https://www.cnblogs.com/CJT-blog/p/9314134.html</a></p>
<p>LRN的作用是在不同的通道之间增加相互竞争的关系，使用如下的计算公式使得相对大的值变的更大，且抑制其他数值较小的神经元,其想法来自神经生物学中的横向抑制，意思就是兴奋的神经细胞抑制周围神经细胞的能力</p>
<script type="math/tex; mode=display">
b_{x, y}^{i}=a_{x, y}^{i} /\left(k+\alpha \sum_{j=\max (0, i-n / 2)}^{\min (N-1, i+n / 2)}\left(a_{x, y}^{j}\right)^{2}\right)^{\beta}</script><p>从2变成0.1的计算：</p>
<script type="math/tex; mode=display">
b = 2/(0+1*(1^2 + 2^2 +4^2))^1 = 0.10</script><p><img src="/2020/06/02/pytorch-note/pic5.jpg" alt="img"></p>
<p>注意2：在alexnet中第六层的9216是摊平后的结果（6<em>6\</em>256 = 9216）</p>
<h3 id="2-5-RNN"><a href="#2-5-RNN" class="headerlink" title="2.5 RNN"></a>2.5 RNN</h3><p><a href="https://blog.csdn.net/qq_27825451/article/details/88988755" target="_blank" rel="noopener">https://blog.csdn.net/qq_27825451/article/details/88988755</a></p>
<p>在循环神经网络中需要注意的点是rnn的输入参数也就是input_size和hidden_size</p>
<pre><code class="lang-python">class RNN(object):
    def __init__(self,input_size,hidden_size):
        super().__init__()
        self.W_xh=torch.nn.Linear(input_size,hidden_size) #因为最后的操作是相加 所以hidden要和output 的shape一致
        self.W_hh=torch.nn.Linear(hidden_size,hidden_size)

    def __call__(self,x,hidden):
        return self.step(x,hidden)
    def step(self, x, hidden):
        #前向传播的一步
        h1=self.W_hh(hidden)
        w1=self.W_xh(x)
        out = torch.tanh( h1+w1)
        hidden=self.W_hh.weight
        return out,hidden

rnn = RNN(20,50)
input = torch.randn( 32 , 20)
h_0 =torch.randn(32 , 50) 
seq_len = input.shape[0]

for i in range(seq_len):
    output,hn= rnn(input[i, :], h_0)
print(output.size(),h_0.size())
</code></pre>
<p>从上述代码中可以看出输入大小是（32，20）也就是32个维度为20的具有时间属性的样本，每个样本的维度是20，且在官方的例子里面是将self.W_xh与self.W_hh相加，所以输出的是32个标签，每个标签的维度是50,所以最后的输出是（32，50）</p>
<p><img src="/2020/06/02/pytorch-note/gif1.gif" alt="img"></p>
<p>直接看pytorch的handbook描述lstm并不是特别细致，在这里做个简要的备忘笔记：</p>
<p>参考<a href="https://www.jianshu.com/p/95d5c461924c" target="_blank" rel="noopener">https://www.jianshu.com/p/95d5c461924c</a></p>
<p>如下图所示lstm的一个单元内有四个网络层</p>
<p><img src="/2020/06/02/pytorch-note/pic6.png" alt="img"></p>
<p>首先是遗忘门输出的$f_t$决定以多少程度丢弃哪些信息，其中决定$f_t$是多少的输入参数主要是$h_{t-1}$ 和$x_t$ 可以理解为根据当前的输入和前一状态的隐藏层信息决定前一状态$C_{t-1}$ 有多少被遗忘，而激活函数是sigmoid函数，输出值是0-1的向量，0表示全部遗忘，1表示全保留，将$C_{t-1}$ 与$f_t$结合可以理解为弱化上一轮的信息部分记忆</p>
<p><img src="/2020/06/02/pytorch-note/pic7.png" alt="img"></p>
<p>其次是输入门决定了给当前状态增加哪些新的信息，可以从公式中看出$i_t$的形式与上一步的$f_t$ 一致，激活函数都是sigmoid函数，输入也都是$h_{t-1}$ 和$x_t$ 是要更新信息的系数，而<script type="math/tex">\tilde{C}_{t}</script> 是当前的状态信息其主要输入是$h_{t-1}$ 和$x_t$ 将<script type="math/tex">\tilde{C}_{t}</script>与$i_t$结合即可得到哪些信息是要被加强记忆</p>
<p><img src="/2020/06/02/pytorch-note/pic8.png" alt="img"></p>
<p>下一步则是将$C_{t-1}$更新为$C_t$的过程，将上一状态的$C_{t-1}$与$f_t$相点乘确定忘记部分信息，再加上当前状态需要加强记忆的信息则得到$C_t$ </p>
<p><img src="/2020/06/02/pytorch-note/pic.png" alt="img"></p>
<p>最后一步则是输出当前细胞状态的隐藏层状态$h_t$ 该参数是由$o_t$和$C_t$点乘计算后得到的，其中$o_t$可以理解为根据当前输入$x_t$和前一状态隐藏层信息$h_{t-1}$得到的历史信息+现在信息的重要程度信息，而使用$o_t$与$tanh(C_t)$相乘表示从最原始状态到现在状态信息的重要程度信息</p>
<p><img src="/2020/06/02/pytorch-note/pic10.png" alt="img"></p>
<h3 id="3-1-logistic-regression"><a href="#3-1-logistic-regression" class="headerlink" title="3-1-logistic-regression"></a>3-1-logistic-regression</h3><pre><code class="lang-python">def test(pred,lab):
    t=pred.max(-1)[1]==lab
    return torch.mean(t.float())
</code></pre>
<p><a href="https://jianzhuwang.blog.csdn.net/article/details/103267516" target="_blank" rel="noopener">https://jianzhuwang.blog.csdn.net/article/details/103267516</a></p>
<p>参考上面的大佬做了个笔记：</p>
<p>在pred.max(-1)[1]中的(-1)指的是按照pred的最后的那个维度来取最大值，且[1]代表返回最大值的索引，进而于lab进行直接比较。下面是更具体的例子，方便理解torch的max函数</p>
<pre><code>a = torch.randn(2,3,4)
print(a)
print(a.max(-1))
print(a.max(0))
print(a.max(1))

#下方为输出

tensor([[[-0.8042, -1.9735,  0.3806, -1.1213],
         [-0.6722, -0.3348,  0.4051,  0.1693],
         [ 0.4199,  1.0681, -2.2424,  1.3049]],

        [[ 0.2313,  0.7321, -1.2748,  4.0471],
         [-1.2841, -0.3507, -1.5445,  1.6310],
         [-0.5556,  1.6177,  0.8500, -0.3895]]])
torch.return_types.max(
values=tensor([[0.3806, 0.4051, 1.3049],
        [4.0471, 1.6310, 1.6177]]),
indices=tensor([[2, 2, 3],
        [3, 3, 1]]))
torch.return_types.max(
values=tensor([[ 0.2313,  0.7321,  0.3806,  4.0471],
        [-0.6722, -0.3348,  0.4051,  1.6310],
        [ 0.4199,  1.6177,  0.8500,  1.3049]]),
indices=tensor([[1, 1, 0, 1],
        [0, 0, 0, 1],
        [0, 1, 1, 0]]))
torch.return_types.max(
values=tensor([[0.4199, 1.0681, 0.4051, 1.3049],
        [0.2313, 1.6177, 0.8500, 4.0471]]),
indices=tensor([[2, 2, 1, 2],
        [0, 2, 2, 0]]))
</code></pre><p>注意点2：<br>区分模型中 net.train()和net.eval()，这两个函数并没起到之前理解的训练和预测作用，而是起到指示作用，跟BatchNormalization 和 Dropout的启用与关闭有关，train后面将启用 BatchNormalization 和 Dropout，eval()后面将不启用 BatchNormalization 和 Dropout</p>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1415500736@qq.com </span>
    </div>
</article>


<p>
    <a href="javascript:void(0)" class="dashang" onclick="dashangToggle()">赏</a>
</p>


<div class="article_copyright">
    <p><span class="copy-title">文章标题:</span>pytorch_note</p>
    
    <p><span class="copy-title">本文作者:</span><a href="javascript:void(0)" title="Rock">Rock</a></p>
    <p><span class="copy-title">发布时间:</span>2020-06-02, 20:19:39</p>
    <p><span class="copy-title">最后更新:</span>2020-06-21, 22:21:01</p>
    <span class="copy-title">原始链接:</span><a class="post-url" href="/2020/06/02/pytorch-note/" title="pytorch_note">http://rock-blog.top/2020/06/02/pytorch-note/</a>
    <p>
        <span class="copy-title">版权声明:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
    </p>
</div>



    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: 'e78b4c19bc08850d88df',
            clientSecret: '308b55a6d580ee7a819af0f950b3188be697ae29',
            repo: 'guobaoyo.github.io',
            owner: 'guobaoyo',
            admin: ['guobaoyo'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js" value="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">

    




    </div>
    <div class="copyright">
        <p class="footer-entry">©2016-2019 Rock&#39;s  blog</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full"><span class="min "></span></button>
<button class="post-toc-menu"><span class="post-toc-menu-icons"></span></button>
<div class="post-toc"><span class="post-toc-title">目录</span>
    <div class="post-toc-content">

    </div>
</div>
<a class id="rocket" href="javascript:void(0)"></a>
    </div>
</div>
<div class="acParent"></div>

<div class="hide_box" onclick="dashangToggle()"></div>
<div class="shang_box">
    <a class="shang_close" href="javascript:void(0)" onclick="dashangToggle()">×</a>
    <div class="shang_tit">
        <p>喜欢就点赞,疼爱就打赏</p>
    </div>
    <div class="shang_payimg">
        <div class="pay_img">
            <img src="/img/alipay.jpg" class="alipay" title="扫码支持">
            <img src="/img/weixin.jpg" class="weixin" title="扫码支持">
        </div>
    </div>
    <div class="shang_payselect">
        <span><label><input type="radio" name="pay" checked value="alipay">支付宝</label></span><span><label><input type="radio" name="pay" value="weixin">微信</label></span>
    </div>
</div>


</body>
<script src="/js/jquery.pjax.js?v=1.0.1"></script>

<script src="/js/script.js?v=1.0.1"></script>
<script>
    var img_resize = 'default';
    /*作者、标签的自动补全*/
    $(function () {
        $('.search').AutoComplete({
            'data': ['#三省吾身','#AI','#数学','#编程','#深度学习','#强化学习','#CV','#python','#go','#技术小结','#leetcode','#组会报告','#考研','#NLP',],
            'itemHeight': 20,
            'width': 418
        }).AutoComplete('show');
    })
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $(".post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        
        $("input[name=pay]").on("click", function () {
            if($("input[name=pay]:checked").val()=="weixin"){
                $(".shang_box .shang_payimg .pay_img").addClass("weixin_img");
            } else {
                $(".shang_box .shang_payimg .pay_img").removeClass("weixin_img");
            }
        })
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().split('\n').length - 1, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
    }

    /*打赏页面隐藏与展示*/
    
    function dashangToggle() {
        $(".shang_box").fadeToggle();
        $(".hide_box").fadeToggle();
    }
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0px;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: #fafafa;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
        .nav .hide-list.fullscreen {
            left: 492px
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #c1bfc1;
    }
    
    

    /*列表样式*/
    
    .post .pjax article .article-entry>ol, .post .pjax article .article-entry>ul, .post .pjax article>ol, .post .pjax article>ul{
        border: #e2dede solid 1px;
        border-radius: 10px;
        padding: 10px 32px 10px 56px;
    }
    .post .pjax article .article-entry li>ol, .post .pjax article .article-entry li>ul,.post .pjax article li>ol, .post .pjax article li>ul{
        padding-top: 5px;
        padding-bottom: 5px;
    }
    .post .pjax article .article-entry>ol>li, .post .pjax article .article-entry>ul>li,.post .pjax article>ol>li, .post .pjax article>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    .post .pjax article .article-entry li>ol>li, .post .pjax article .article-entry li>ul>li,.post .pjax article li>ol>li, .post .pjax article li>ul>li{
        margin-bottom: auto;
        margin-left: auto;
    }
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.5;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
</style>







</html>
